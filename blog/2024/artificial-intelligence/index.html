<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Artificial Intelligence | Trey Wilkinson </title> <meta name="author" content="Trey Wilkinson"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://t-wilkinson.github.io/blog/2024/artificial-intelligence/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Trey</span> Wilkinson </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/resume/">resume </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Artificial Intelligence</h1> <p class="post-meta"> Created in July 23, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Threats of AI models? What happens if threat actors develop first AGI? There isn’t much we can do. But the people who should be in charge are.</p> <p>Missing memory and sufficient generalization</p> <p>Process-supervised Reward Models Overall better at training LLMs as they give more nuanced details.</p> <p>Perhaps memory would be more effective if model computes and stores future inputs into the model. It could then use the memory to regenerate complex information that can be used to modify the input to the transformer, change the queries, etc.</p> <p>AI and LLM in particular clearly have huge potential in todays society but what exactly that is isn’t exactly clear. The world of AI is changing quickly and its not clear how AI will exactly fit in our lives. They are clearly useful tools for coding, but their role with existing programmers is not well known. Will they replace programmers and when? Will they only ever be a useful tool? Currently, models like ChatGPT are useful as compressed corpus of information, with an intelligent interaction including understanding queries, and usefully packaging and organizing recalled information.</p> <p>Everyone likes to say that deep learning models are “just numbers” but its an oversimplification. Human brains are “just neurons”, and neurons function like small feed foward networks, with some potential for dynamically changing their “weights”. Both are universal function approximators. Both take inputs, human brain takes a ton of sensory inputs. Both learn to extract information in a way that is useful for how the intelligent entity interacts with its environment. The few things missing from LLMs, in particular, is a reinforcement model, where the machine learns to set goals. The more general goals become, the more one dives into philosophy, needing to understand how goals affect the entities in its surrounding environment, which is essentially morality. Also, in order to be computationally efficient, they need to “abstract” their philosophy; compress their philosophy and goals to fit a large number of interactions and environments. This is where meaning comes in.</p> <p>Artificial intelligence models could have some kind of “autism” or another mental disorder like anti-social personality disorder. Just like in humans, it can be difficult to predict and the only way to diagnose it is to look at past events, in which case it is too late.</p> <p>Consciousness is measurable, but hard to define.</p> <ul> <li>AI vs. Human <ul> <li>Human attributes successful AI must replicate <ul> <li>Heuristics</li> <li>Generalizations from little information</li> <li>Relating</li> <li>Memory</li> <li>Competition</li> <li>Brains need very little to learn from and generalize. How much knowledge is encoded in DNA? The brain is very dynamic, so too should nueral networks.</li> <li>The brain is very messy, many parts communicate with others</li> <li>Internal representation of the world</li> <li>Construct thoughts to test algorithms to add to policy network</li> <li>Consciousness seems to be a recursive thought</li> <li>Construct multiple representations/interpretations of the environment.</li> <li>What are the critical parts of the human brain? Neocortex, inner, outer, stem, etc.</li> <li>Dynamic rewriting (in order to do this, would you have to have an intuition on what does what in the neural architecture?)</li> <li>Lazy architecture? (Don’t compute until necessary, allow neurons on same level to read from eachother ‘before’ the fire)</li> <li>What role does the neocortex play in the thought process</li> <li>Morals, conscience, parents, school, culture, religion is the human value network.</li> <li>Human value network is modifiable.</li> <li>Look at everything as a learning experience</li> <li>What is the human value function? value(t) := (current state(t), current goal(t))</li> <li>Exploiters at multiple levels of abstraction.</li> <li>Personality. <ul> <li>Ideal for mutualism. Facilitates specialization.</li> <li>Like a puzzle piece.</li> <li>Marriage are two opposite exploiters with a value function maximizing cooperation.</li> </ul> </li> <li>Selection function for competition</li> <li>Multiple methods for learning (reinforcement, supervision, and unsupervised)</li> <li>Identify symmetries</li> <li>Evolution (Exploitain of current knowledge and exploration of unkown)</li> <li>Learn from others mistakes. Adapt in real time.</li> <li>Dopamine (and similar) are amazing rewards. They are very general, dynamic, and can be overwritten.</li> <li>There needs both high and low levels of supervised learning. Low levels produce people that rethink everything, very necessary.</li> <li>Neurons are individual units</li> <li>Core ‘brain’, and dynamic ‘functionalities’ tacked on the end.</li> <li>Humans reward ourselves from the observation of the action</li> <li>Meta-learning (learn to learn)</li> <li>Abstract when learning small task</li> <li>DNA encodes reward function learnt from millions of years.</li> <li>Learn in multiple simulations</li> <li>The reward function maximizes existance. Anything that does not, will simply fail to exist.</li> <li>Some (complex) variation of NEAT is necessary to reach and match the complexity of evolution of human life.</li> <li>Abstract thought between ‘thinking to move’ and ‘moving in that direction’</li> <li>Q-Learning acts like memory which enables one to learn from the past. Only store the worst / best memories.</li> <li>Human brain only lets a couple bits of information into the brain.</li> <li>Break ultimate reward (existance) into multiple sub-goals.</li> <li>The human brain is very dynamic, as such, there is little hard-coded information</li> <li>Understand symmetry. Understand general layout of game controller but sometimes buttons roles can be reversed. We can then ‘transfer learn’ without having to relearn.</li> <li>Neurons encode multiple representations of a concept</li> </ul> </li> <li>Computers vs. Brains <ul> <li>Bad memory has its benefit. Extract essence from concept.</li> <li>Computers suffer from the same effect that humans with amazing memory do. They are able to see in amazing clarity the detail, but not what that detail forms. Unable to see the bigger picture.</li> <li>The digital aspect of computers is what prevents them most from operating similar to humans.</li> <li>Computers are so powerful for humans because they operate on our greatest weakness. The same thing that allows us to create and innovate is what prevents us from holding a couple random numbers in our head, or remembering a picture color by color.</li> <li>In the future, there will be a union with self-aware computers and humans, with those being seperate from classical computers we use today.</li> </ul> </li> </ul> </li> <li>Research <ul> <li>Research ideas <ul> <li>Have an internal neural net and when faced with a new challenge, init a new neural net with the internal one. Update the internal net very slightly compared to new net ? How to encode linguistic information in ai model as a sort of supervised learning ? What is the right level of abstraction for knowledge base <ul> <li>Something to approximate core knowledge (elizabeth spelke)</li> </ul> </li> </ul> </li> <li>Look for applications <ul> <li>Have point of view of PhD student <ul> <li>You need to publish papers</li> <li>You need new innovative ideas ? How can it be implemented in code ? What problems does it solve ? What does it relate to</li> </ul> </li> <li>To general AI</li> <li>Visual applications</li> <li>Real-world problems</li> <li>Skip things that are too difficult or mind-bending</li> <li>Really chew on core concepts</li> <li>Think about innovations that you can make ? Why is it done like this ? Why not this way ? What if we add this</li> </ul> </li> </ul> </li> <li>Companies <ul> <li>Future AI <ul> <li>https://futureai.guru/</li> </ul> </li> </ul> </li> </ul> <h1 id="generalization">Generalization</h1> <p>Generalization of AI algorithms are abysmal. Human brains are apt at forming abstractions and generalizations.</p> <p>Perhaps one path to generalization is creating multiple models based on very limited data. This allows offloading a lot of computation beforehand. Then during the next iteration, you identify which model or collection of models are the best. The connection and association between models as well is a very powerful tool to compose models in complex ways. These cross connections can also be tuned and honed to develop multiple complex abstract models and then select the optimal one for the task at hand.</p> <h1 id="architectures">Architectures</h1> <p>Architectures: Neural Turing machine, Differentiable neural computer, Transformer, Recurrent neural network (RNN), Long short-term memory (LSTM), Gated recurrent unit (GRU), Echo state network, Multilayer perceptron (MLP), Convolutional neural network, Residual neural network, Mamba, Autoencoder, Variational autoencoder (VAE), Generative adversarial network (GAN), Graph neural network Organizations: Anthropic, EleutherAI, Google, DeepMind, Hugging, Face, OpenAI, Meta, AI, Mila, MIT, CSAIL, Huawei People: Yoshua Bengio, Alex Graves, Ian Goodfellow, Stephen Grossberg, Demis Hassabis, Geoffrey Hinton, Yann LeCun, Fei-Fei Li, Andrew Ng, Jürgen Schmidhuber, David Silver, Ilya Sutskever Concepts: Gradient descent, SGD, Clustering, Regression, Overfitting, Hallucination, Adversary, Attention, Convolution, Loss functions, Backpropagation, Batchnorm, Activation, Softmax, Sigmoid, Rectifier, Regularization, Datasets, Augmentation, Diffusion, Autoregression</p> <p>Convolutional Neural Networks (CNNs): LeNet, AlexNet, VGGNet, GoogLeNet (Inception), ResNet (Residual Networks), DenseNet, MobileNet, EfficientNet, Capsule Networks Recurrent Neural Networks (RNNs): Elman Networks, Jordan Networks, Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), Bidirectional RNNs, Echo State Networks (ESNs), Neural Turing Machines (NTMs), Transformer (though primarily used in sequence-to-sequence tasks) Graph Neural Networks (Graph Nets): Graph Convolutional Networks (GCNs), GraphSAGE (Graph Sample and Aggregated), Gated Graph Neural Networks (GGNNs), Graph Isomorphism Networks (GIN), Message Passing Neural Networks (MPNNs), Graph Attention Networks (GAT), Graph Neural Networks for Molecular Graphs Autoencoders: Vanilla Autoencoders, Variational Autoencoders (VAEs), Denoising Autoencoders, Sparse Autoencoders, Contractive Autoencoders, Adversarial Autoencoders Generative Adversarial Networks (GANs): Vanilla GANs, Deep Convolutional GANs (DCGANs), Wasserstein GANs (WGANs), Conditional GANs (cGANs), InfoGAN, CycleGAN, StyleGAN, BigGAN, ProGAN State space model: Mamba</p> <p>Memory-Augmented Networks:</p> <ul> <li>Neural Turing Machines (NTMs)</li> <li>Differentiable Neural Computers (DNCs)</li> <li>Transformer-XL Attention Mechanisms:</li> <li>Bahdanau Attention</li> <li>Luong Attention</li> <li>Transformer (self-attention)</li> <li>BERT (Bidirectional Encoder Representations from Transformers)</li> </ul> <p>Few-Shot Learning:</p> <ul> <li>Matching Networks</li> <li>Prototypical Networks</li> <li>Relation Networks Continual Learning:</li> <li>EWC (Elastic Weight Consolidation)</li> <li>LwF (Learning without Forgetting)</li> <li>iCaRL (Incremental Classifier and Representation Learning) Federated Learning:</li> <li>FedAvg (Federated Averaging)</li> <li>FedProx (Federated Proximal)</li> <li>FedGAN (Federated GAN) Pretrained Language Models:</li> <li>OpenAI GPT (Generative Pre-trained Transformer)</li> <li>BERT (Bidirectional Encoder Representations from Transformers)</li> <li>XLNet</li> <li>RoBERTa</li> <li>T5 (Text-To-Text Transfer Transformer)</li> </ul> <p>Siamese Networks:</p> <ul> <li>Siamese Neural Networks</li> <li>Triplet Networks</li> <li>Contrastive Divergence Networks Capsule Networks:</li> <li>Dynamic Routing Between Capsules (CapsNets)</li> <li>Capsule Networks with Attention</li> <li>Graph Capsule Networks Neural Architecture Search (NAS):</li> <li>DARTS (Differentiable Architecture Search)</li> <li>ENAS (Efficient Neural Architecture Search)</li> <li>MNAS (Mobile Neural Architecture Search)</li> <li>NASNet</li> <li>AmoebaNet Speech Recognition:</li> <li>DeepSpeech</li> <li>Listen, Attend, and Spell (LAS)</li> <li>Wav2Vec Video Analysis:</li> <li>C(Convolutional 3D)</li> <li>I(Inflated ConvNets)</li> <li>TSN (Temporal Segment Networks) Recommendation Systems:</li> <li>Collaborative Filtering using Neural Networks</li> <li>DeepFM (Factorization Machines)</li> <li>Neural Collaborative Filtering Anomaly Detection:</li> <li>Autoencoder-based Methods</li> <li>Variational Autoencoders (VAEs) for Anomaly Detection</li> <li>One-Class SVM with Neural Networks Health and Medical Imaging:</li> <li>CheXNet for Chest X-ray Analysis</li> <li>U-Net for Medical Image Segmentation</li> <li>CNNs for Brain Tumor Detection Time Series Analysis:</li> <li>WaveNet for Audio Generation</li> <li>DeepAR for Time Series Forecasting</li> <li>TCN (Temporal Convolutional Network)</li> </ul> <h1 id="research">Research</h1> <p>Efficient Training of Neural Networks:</p> <ul> <li>Pruning and Quantization: Techniques to reduce the size of models and the computational resources needed for training and inference.</li> <li>Federated Learning: Distributed training that allows for privacy preservation and efficiency by training models across multiple devices.</li> <li>Transfer Learning: Leveraging pre-trained models on new tasks to reduce the amount of data and computation required for training.</li> </ul> <p>Mixture of Experts (MoE):</p> <ul> <li>MoE (sparse model) models aim to improve scalability and efficiency by having different ‘expert’ sub-models that specialize in different aspects of a task.</li> </ul> <p>Language Models (LMs) and Large Language Models (LLMs):</p> <ul> <li>Research into improving the performance and training efficiency of large-scale language models, like GPT-3 and BERT.</li> <li>Exploration of new training techniques, regularization methods, and architectures to improve the capabilities of LLMs.</li> </ul> <p>Foundational models:</p> <ul> <li>Ongoing improvements to the transformer architecture to increase efficiency, such as the Linformer, Performer, and Reformer.</li> <li>Research into making transformers more parameter and data-efficient.</li> <li>Mamba</li> <li>Sparse Transformers: Reducing the computational complexity by using sparsity in the attention mechanism.</li> <li>Vision Transformers (ViTs): Adapting the transformer architecture for computer vision tasks.</li> </ul> <p>Generalization Capabilities:</p> <ul> <li>Meta-Learning: Teaching AI to learn new tasks with minimal data by generalizing from previous experiences.</li> <li>Few-Shot Learning: Improving the ability of models to perform tasks with very few examples.</li> <li>Generalizing models through general interfaces (like languages) where the model learns tasks based on input</li> </ul> <p>Cognitive Neuroscience-Inspired Architectures:</p> <ul> <li>Research into architectures that mimic certain aspects of human cognition or brain structure, such as Hierarchical Temporal Memory (HTM) and models inspired by the Thousand Brains Theory of Intelligence.</li> </ul> <p>Model Distillation:</p> <ul> <li>Techniques for transferring knowledge from large, complex models to smaller, more efficient ones without a significant loss in performance.</li> </ul> <p>Self-Supervised Learning:</p> <ul> <li>Methods for training models using unlabeled data, where the learning algorithm generates its own labels from the data.</li> </ul> <p>Robustness and Adversarial Machine Learning:</p> <ul> <li>Developing models that are resistant to adversarial attacks and can generalize well in the face of distribution shifts or noisy data.</li> </ul> <p>Explainability and Interpretability:</p> <ul> <li>Techniques to understand and explain the decisions made by AI models, which is crucial for deployment in sensitive areas.</li> </ul> <p>Fairness, Accountability, and Transparency:</p> <ul> <li>Ensuring that AI systems are fair and do not perpetuate or amplify biases. This includes research into ethical AI and algorithmic accountability.</li> </ul> <p>Multi-Modal Learning:</p> <ul> <li>Multi modal input/output</li> <li>Combining information from various data sources (e.g., text, images, and audio) to improve learning and prediction.</li> <li>Multi modal transformers: q-transformer</li> </ul> <p>Neuro-Symbolic AI:</p> <ul> <li>Combining neural networks with symbolic reasoning to create systems that can reason with the abstract, structured knowledge.</li> </ul> <p>Energy-Efficient AI:</p> <ul> <li>Research aimed at reducing the carbon footprint of training and deploying AI models.</li> </ul> <p>Alignment:</p> <ul> <li>RLAiF, RLHF, Self-rewarding language models</li> </ul> <p>Cognitive capabilities:</p> <ul> <li>Logical reasoning and other modals</li> </ul> <p>Limited data:</p> <ul> <li>Training with limited data: synthetic, more efficiently generalizing, GPT or CLIP style models</li> <li>Sample efficiency</li> </ul> <p>Natural Language Processing (NLP):</p> <ul> <li>Text Processing</li> <li>Speech Recognition</li> <li>Machine Translation</li> <li>Question Answering</li> <li>Sentiment Analysis Word Embeddings: Word2Vec, GloVe. Seq2Seq Models: Encoder-Decoder architectures. Attention Mechanisms: Vital for handling context in sequences.</li> <li>BERT (Bidirectional Encoder Representations from Transformers)</li> <li>ELMO (Embeddings from Language Models)</li> <li>Transformer-XL</li> <li>GPT-3</li> </ul> <p>Computer Vision:</p> <ul> <li>Object Recognition</li> <li>Image Classification</li> <li>Image Generation</li> <li>Image Segmentation</li> <li>Visual Question Answering Image Processing Techniques: Filters, edge detection. Object Detection and Recognition: Techniques like YOLO (You Only Look Once), R-CNNs. Object Detection and Segmentation: <ul> <li>YOLO (You Only Look Once)</li> <li>SSD (Single Shot Multibox Detector)</li> <li>Mask R-CNN</li> <li>FCN (Fully Convolutional Networks)</li> <li>U-Net Image Generation:</li> <li>DCGAN (Deep Convolutional Generative Adversarial Network)</li> <li>ProGAN (Progressive GAN)</li> <li>StyleGAN2</li> <li>CycleGAN</li> <li>Pix2Pix Image Super-Resolution:</li> <li>SRCNN (Super-Resolution Convolutional Neural Network)</li> <li>VDSR (Very Deep Super-Resolution)</li> </ul> </li> </ul> <p>Robotics:</p> <ul> <li>Robotic Perception</li> <li>Motion Planning</li> <li>Robot Control</li> <li>Human-Robot Interaction</li> <li>Swarm Robotics</li> </ul> <p>Knowledge Representation and Reasoning:</p> <ul> <li>Ontologies</li> <li>Knowledge Graphs</li> <li>Logic Programming</li> <li>Inference Systems</li> </ul> <p>Expert Systems:</p> <ul> <li>Rule-Based Systems</li> <li>Decision Support Systems</li> <li>Diagnostic Systems</li> </ul> <p>Planning and Scheduling:</p> <ul> <li>Automated Planning</li> <li>Resource Allocation</li> <li>Task Scheduling</li> </ul> <p>Evolutionary Computing:</p> <ul> <li>Genetic Algorithms</li> <li>Genetic Programming</li> <li>Evolutionary Strategies</li> </ul> <p>Neural Networks and Deep Learning:</p> <ul> <li>Feedforward Networks</li> <li>Recurrent Networks</li> <li>Convolutional Networks</li> <li>Generative Adversarial Networks (GANs)</li> <li>Transformer Models</li> </ul> <p>Cognitive Computing:</p> <ul> <li>Emulating Human Thought Processes</li> <li>Mimicking Human Perception</li> <li>Cognitive Models</li> </ul> <p>Interdisciplinary: AI Ethics: Bias and Fairness, Explainability, Accountability, Privacy and Security AI for Healthcare: Medical Imaging, Disease Prediction, Drug Discovery, Personalized Medicine AI in Finance: Algorithmic Trading, Fraud Detection, Credit Scoring, Risk Management AI in Gaming: Game Playing Agents, Procedural Content Generation, Player Behavior Modeling AI in Education: Intelligent Tutoring Systems, Adaptive Learning Platforms, Educational Data Mining AI in Cybersecurity: Threat Detection, Anomaly Detection, Security Analytics Quantum AI: Quantum Machine Learning, Quantum Algorithms Swarm Intelligence: Ant Colony Optimization, Particle Swarm Optimization AI and Creativity: Computational Creativity, AI in Art and Music AI Hardware: Neuromorphic Computing, Quantum Computing</p> <h1 id="model-deployment">Model deployment</h1> <p>Web server: Flax, Django, Express.js, FastAPI Cloud systems: AWS Lambda, AWS SageMaker, Azure Functions, Azure ML Service, Google Cloud Functions, Google Cloud AI Platform Model deployment: TorchServe, TensorFlow Serving, ONNX Runtime (PyTorch models can be converted to ONNX format)</p> <h1 id="fundamental-concepts">Fundamental concepts</h1> <p>Mathematics: Linear Algebra: matrices, vectors, eigenvalues, eigenvectors. Calculus: derivatives and integrals, lagrange multiplier, hessian matrix, jacobian Probability and Statistics: Probability distributions, statistical inference, Bayesian reasoning, bayesian inference, markov random fields, bayesian networks Bayesian Inference: Probabilistic reasoning and updating beliefs. Linear algebra: vectors, matrices, tensors, multiplying matrices, SVD, eigendecomposition, PCA Probability: random variables, probability distributions, conditional probability, independence, variance, bayesian statistics Information theory: information Numerical computation: overflow and underflow, gradient based optimization, constrained optimization Machine learning: cpacity, overfitting, underfitting, MLE, SGD Discrete mathematics: concrete mathematics, recurrence, sums, number theory, binomial coefficient, special numbers, generating functions, discrete probability, asymptotics</p> <p>Machine Learning: Supervised Learning: classification - Linear Regression - Support Vector Machines - Decision Trees - Random Forests - Neural Networks Unsupervised Learning: Clustering, dimensionality reduction. - k-Means Clustering - Hierarchical Clustering - Principal Component Analysis (PCA) - Generative Adversarial Networks (GANs) - Autoencoders Neural Networks: Architectures, activation functions, training algorithms.</p> <ul> <li>Reinforcement Learning: multi-agent systems</li> <li>Transfer Learning</li> <li>Semi-Supervised Learning</li> <li>Self-Supervised Learning</li> </ul> <p>Reinforcement Learning:</p> <ul> <li>Exploration-Exploitation Strategies</li> <li>Actor-Critic Models Policy and Value Iteration: on policy/off policy, value iteration, policy iteration Exploration vs. Exploitation Trade-off: Balancing the need to explore new actions and exploit known ones. Algorithms: Q-learning, DQN, policy gradient methods, PPO, DDPG, TRPO Markov decision process: reward functions, observability, reinforcement learning, continuous</li> </ul> <p>Deep Learning: Neural Network Architectures: Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Transformers. Transfer Learning: Leveraging pre-trained models for new tasks. Regularization Techniques: Dropout, batch normalization. Deep networks: example of XOR, gradient-based learning, architecture design, back propagation, regularization, multitask learning, sparse representations, dropout, adversarial traning, basic algorithms</p> <p>Optimization: Gradient Descent: Variants like Stochastic Gradient Descent (SGD), Adam, RMSProp. Metaheuristic Algorithms: Genetic algorithms, simulated annealing.</p> <ul> <li>Gradient Descent Variants (e.g., Adam, RMSProp)</li> <li>Evolutionary Algorithms</li> <li>Simulated Annealing</li> </ul> <p>Algorithmic Techniques: Dynamic Programming: Solving problems by breaking them into overlapping subproblems. Monte Carlo Methods: Leveraging random sampling for estimation.</p> <ul> <li>Dynamic Programming for Optimization</li> <li>Monte Carlo Methods</li> <li>Markov Chain Monte Carlo (MCMC) Techniques</li> </ul> <p>Probabilistic Graphical Models:</p> <ul> <li>Bayesian Networks</li> <li>Markov Random Fields</li> </ul> <p>Data Structures for Efficient Computation:</p> <ul> <li>Tensor Data Structures for Neural Networks</li> <li>Sparse Matrix Representations</li> <li>Graph Data Structures (for representation and traversal)</li> </ul> <p>Ethical and Social Implications: Fairness and Bias: Understanding and mitigating biases in AI systems. Interdisciplinary Understanding: Recognizing the societal impact of AI and ethical considerations.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/model-architectures/">Model Architectures</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/cloud-technologies/">Cloud Technologies</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/category-theory/">Category Theory</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/neocortex/">Neocortex</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/futurology/">Futurology</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Trey Wilkinson. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"projects",description:"A growing collection of my projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-resume",title:"resume",description:"This is a description of the page. You can modify it in '_pages/cv.md'. You can also change or remove the top pdf download button.",section:"Navigation",handler:()=>{window.location.href="/resume/"}},{id:"post-model-architectures",title:"Model Architectures",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/model-architectures/"}},{id:"post-cloud-technologies",title:"Cloud Technologies",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/cloud-technologies/"}},{id:"post-category-theory",title:"Category Theory",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/category-theory/"}},{id:"post-neocortex",title:"Neocortex",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/neocortex/"}},{id:"post-futurology",title:"Futurology",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/futurology/"}},{id:"post-artificial-intelligence",title:"Artificial Intelligence",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/artificial-intelligence/"}},{id:"post-music-theory",title:"Music Theory",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/music-theory/"}},{id:"post-neuroscience",title:"Neuroscience",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/neuroscience/"}},{id:"post-homotopy-type-theory",title:"Homotopy Type Theory",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/homotopy-type-theory/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%77%69%6E%73%74%6F%6E.%74%72%65%79.%77%69%6C%6B%69%6E%73%6F%6E@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=qc6CJjYAAAAJ","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>