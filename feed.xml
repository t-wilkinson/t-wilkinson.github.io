<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://t-wilkinson.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://t-wilkinson.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-07-23T20:45:55+00:00</updated><id>https://t-wilkinson.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Model Architectures</title><link href="https://t-wilkinson.github.io/blog/2024/model-architectures/" rel="alternate" type="text/html" title="Model Architectures"/><published>2024-07-23T16:43:39+00:00</published><updated>2024-07-23T16:43:39+00:00</updated><id>https://t-wilkinson.github.io/blog/2024/model-architectures</id><content type="html" xml:base="https://t-wilkinson.github.io/blog/2024/model-architectures/"><![CDATA[<ul> <li>rl := reinforcement learning</li> <li>llm := large language model</li> <li> <p>gpt := generative pretrained transformer</p> </li> <li>openai</li> </ul> <p>Language-augmented vision models utilize language to supplement image data and further understanding of visual semantics, in order to specialize in tasks like object detection, object classification, segmentation, captioning, visual generation, and visual editing. Where the task is implicit in the model design. However, language is a useful general interface for a variety of tasks, and a visual model can interface through language to select tasks, provisioning the road for a more general visual model.</p> <p>LLM: DeepSeek, Starling GPT MiniGPT-4 BERT Gemini LLaVA Multimodal: LLaVA MegaByte</p> <ul> <li>#machine-learning#icml# [MegaByte: Predicting Million-byte Sequences with Multiscale Transformers]{author=Lili Yu; author=Dániel Simig; author=Colin Flaherty; author=Armen Aghajanyan; author=Luke Zettlemoyer; author=Mike Lewis; ref=./resources/reading/2305.07185.pdf} [z:1125.23023.45031] #cognitive-science#clip#openai# [(CLIP) Learning Transferable Visual Models From Natural Language Supervision]{subject=Proceedings of the International Conference on Machine Learning 2020; author=Alec Radford; author=Jong Wook Kim; author=Chris Hallacy; author=Aditya Ramesh; author=Gabriel Goh; author=Sandhini Agarwal; author=Girish Sastry; author=Amanda Askell; author=Pamela Mishkin; author=Jack Clark; author=Gretchen Krueger; author=Ilya Sutskever; ref=./resources/reading/2103.00020.pdf} [z:0056.73123.99603] #openai#clip# [CLIP: Learning Transferable Visual Models From Natural Language Supervision]{}</li> </ul> <h1 id="llms">LLMs</h1> <p>Have an understanding of words, sentence and grammer structure, and paragraph structure. But are limited to a large degree by their context size, as it has a O(n²) growth in complexity with respect to its context size.</p> <p>Transformers, then, require in addition to their context, a additional, more abstract concept of “goals”. Something more long term to attend to, to keep everything consistent and with a higher up view of what it is attempting.</p> <h2 id="gpt-gpt">GPT #gpt#</h2> <p>GPT2: https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf</p> <h2 id="minigpt-4-gpt">MiniGPT-4 #gpt#</h2> <p><a href="https://huggingface.co/docs/transformers/main/model_doc/blip-2">BLIP2</a> The model architecture of MiniGPT-4 follows BLIP-2. Don’t forget to check this great open-source work if you don’t know it before! <a href="https://github.com/salesforce/LAVIS">Lavis</a> This repository is built upon Lavis! <a href="https://github.com/lm-sys/FastChat">Vicuna</a> The fantastic language ability of Vicuna with only 13B parameters is just amazing. And it is open-source!</p> <ul> <li>[MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models]{ref=./resources/reading/2304.10592.pdf}</li> </ul> <h2 id="bert">BERT</h2> <p>Best practices are established by Liu et al. in 2019.</p> <h1 id="transformer">Transformer</h1> <p>Allows the model to contextualize responses. Develop attention at multiple levels of abstraction.</p> <p>Composite transformer</p> <ul> <li>#cognitive-science# [Efficient Transformers: A Survey]{ref=./resources/reading/2009.06732.pdf}</li> <li>#cognitive-science# [Fast Transformer Decoding: One Write-Head is All You Need]{ref=./resources/reading/1911.02150.pdf}</li> <li>#cognitive-science# [Transformer Architecture: The Positional Encoding]{author=Amirhossein Kazemnejad’s Blog; ref=https://kazemnejad.com/blog/transformer_architecture_positional_encoding/; resource=website;}</li> <li>#cognitive-science# [Training Tips for the Transformer Model]{ref=./resources/reading/1804.00247.pdf}</li> </ul> <h2 id="q-transformer">Q-transformer</h2> <p>Will have a short term memory if only looking at most recent actions and states. Want to potentially do something like the DeepMind Atari reinforcement AI.</p> <h2 id="architecture">Architecture</h2> <h3 id="encoder">Encoder</h3> <p>Takes input as a whole. Trained by putting noise on input or filling input with holes. Useful model for forming world views from imperfect information.</p> <h3 id="decoder">Decoder</h3> <p>Takes input in sequences. Autoregressive. Trained by predicting next sequence.</p> <h2 id="issues">Issues</h2> <h2 id="improvements">Improvements</h2> <h2 id="resources">Resources</h2> <ul> <li>https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf</li> <li>Improving Language Understanding by Generative Pre-Training</li> <li>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</li> <li>Fast Transformer Decoding: One Write-Head is All You Need</li> <li><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&amp;t=1198s&amp;ab_channel=StanfordOnline">Introduction to Transformers</a></li> <li><a href="https://www.youtube.com/watch?v=kCc8FmEb1nY&amp;ab_channel=AndrejKarpathy">Lets build GPT: from scratch, in code, spelled out</a></li> <li>#transformer# [Stanford CS 25]{subtitle=Transformers United; ref=https://web.stanford.edu/class/cs25/; resource=website;} <h1 id="clip">CLIP</h1> </li> <li><a href="./resources/reading/2111.03133.pdf">StyleCLIPDraw: Coupling Content and Style in Text-to-Drawing Synthesis</a></li> <li>#cognitive-science#CLIP# [(CLIP) Learning Transferable Visual Models From Natural Language Supervision]{subject=Proceedings of the International Conference on Machine Learning 2020; author=Alec Radford; author=Jong Wook Kim; author=Chris Hallacy; author=Aditya Ramesh; author=Gabriel Goh; author=Sandhini Agarwal; author=Girish Sastry; author=Amanda Askell; author=Pamela Mishkin; author=Jack Clark; author=Gretchen Krueger; author=Ilya Sutskever; ref=./resources/reading/2103.00020.pdf}</li> </ul> <h1 id="reinforcement-learning">Reinforcement learning</h1> <ul> <li><a href="./resources/reading/dqnnaturepaper.pdf">DQNN: Human-level Control Through Deep Reinforcement Learning - DeepMind</a></li> <li><a href="./resources/reading/Watkins-Dayan1992_Article_Q-learning.pdf">Q-learning - Watkins Dayan</a></li> </ul> <h1 id="gnn">GNN</h1> <h1 id="gan">GAN</h1> <p>\(\min{G}\max{D} V(G,D) = \mathbb{E}_{X\sim\hat{p}_{data}}[log D(x)] + \mathbb{E}_{Z\sim\hat{p}_{data}}[1-log D(G(Z))]\)</p> <ul> <li>#gan# [GAN]{subtitle=Generative Adversarial Networks; author=Ian J. Goodfellow; author=Jean Pouget-Abadie∗; author=Mehdi Mirza; author=Bing Xu; author=David Warde-Farley; author=Sherjil Ozair†; author=Aaron Courville; author=Yoshua Bengio; ref=./resources/reading/1406.2661.pdf; resource=pdf;}</li> <li>#gan# [PGAN]{}</li> <li>#gan# [BEGAN]{subtitle=Boundary Equilibrium Generative Adversarial Networks; ref=./resources/reading/1703.10717.pdf; resource=pdf;}</li> <li>#gan# [DCGAN]{subtitle=Unsupervised Representation Learning WITH Deep Convolutional Generative Adversarial Networks; ref=./resources/reading/1511.06434.pdf; resource=pdf;}</li> <li>#gan# [CGAN]{subtitle=Conditional Generative Adversarial Nets; ref=./resources/reading/1411.1784.pdf; resource=pdf;}</li> <li>#gan# [Improved Techniques for Training GANs]{author=OpenAI; ref=./resources/reading/1606.03498.pdf; resource=pdf;}</li> <li>#gan# [Multimodal Conditional Image Synthesis with Product-of-Experts GANs]{author=NVIDIA; ref=./resources/reading/2112.05130.pdf; resource=pdf;}</li> <li>#gan# [WaveNet]{subtitle=A Generative Model For Raw Audio; author=DeepMind; ref=./resources/reading/1609.03499.pdf; resource=pdf;}</li> <li>#gan# [Instance-Conditioned GAN]{author=Facebook; ref=./resources/reading/2109.05070.pdf; resource=pdf;}</li> <li>#gan# [GANSpace]{subtitle=Discovering Interpretable GAN Controls; ref=./resources/reading/2004.02546.pdf; resource=pdf;}</li> <li>#gan# [Image-to-Image Translation with Conditional Adversarial Networks]{author=Phillip Isola; author=Jun-Yan Zhu; author=Tinghui Zhou; author=Alexei A. Efros; ref=./resources/reading/1611.07004.pdf; resource=pdf;}</li> <li>#gan# [StyleFlow]{subtitle=Attribute conditioned Exploration of StyleGAN Generated Images using Conditional Continuous Normalizing Flows; author=Rameen Abdal; author=Peihao Zhu; author=Niloy Mitra; author=Peter Wonka; ref=./resources/reading/StyleFlow:-Attribute-conditioned-Exploration-of-StyleGAN-Generated-Images-using-Conditional-Continuous-Normalizing-Flows-by-Rameen-Abdal,Peihao-Zhu,Niloy-Mitra,Peter-Wonka.pdf; resource=pdf;}</li> <li>#gan# [Training Generative Adversarial Networks with Limited Data]{author=NVIDIA; ref=./resources/reading/2006.06676.pdf; resource=pdf;}</li> <li>#gan# [High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs]{author=Ting-Chun Wang; author=Ming-Yu Liu; author=Jun-Yan Zhu; author=Andrew Tao; author=Jan Kautz; author=Bryan Catanzaro; ref=./resources/reading/1711.11585.pdf; resource=pdf;}</li> <li>#gan# [InsetGAN for Full-Body Image Generation]{ref=./resources/reading/2203.07293.pdf; resource=pdf;}</li> <li>#gan# [JoJoGAN]{subtitle=One Shot Face Stylization; author=Min Chong; author=D.A. Forsyth; ref=./resources/reading/2112.11641.pdf; resource=pdf;}</li> </ul> <h1 id="cnn">CNN</h1> <ul> <li>Each layer is a collection of image filters or feature extractors. Layers progressively form higher-level understanding of the image. Initial layers focus on low-level pixel based information like detecting edges. Later layers form high-level representations of the image.</li> <li>One can visually examine the information at each layer by viewing the feature maps</li> <li> <p>Dramatically reduce complexity of neural network (as opposed to densly connected) and these constraints provide a built in idea of space into image processing</p> </li> <li><a href="./resources/reading/lecun-99.pdf">Object Recognition with Gradient-Based Learning - Yann LeCun, Patrick Haffner, Léon Bottou, Yoshua Bengio</a></li> <li>VGG <ul> <li><a href="./resources/reading/Very-Deep-Convolutional-Networks-for-Large-Scale-Image-Recognition-by-Karen-Simonyan,Andrew-Zisserman.pdf">Very Deep Convolutional Networks for Large Scale Image Recognition - Karen Simonyan,Andrew Zisserman</a></li> </ul> </li> <li>DenseNet</li> <li>AlexNet</li> <li> <p>ConvNet</p> </li> <li>DenseNet</li> <li>ResNet <ul> <li>HighwayNet</li> <li>DenseNet</li> </ul> </li> <li>DeCNN <ul> <li><a href="./resources/reading/deconvolutionalnetworks.pdf">Deconvolutional Networks</a></li> </ul> </li> <li>RegNet <ul> <li><a href="./resources/reading/2101.00590.pdf">RegNet: Self-Regulated Network for Image Classification - Jing Xu, Yu Pan, Xinglin Pan, Steven Hoi, Fellow, IEEE, Zhang Yi, Fellow, IEEE, and Zenglin Xu</a></li> <li><a href="./resources/reading/1512.03385.pdf">Deep Residual Learning for Image Recognition - Microsoft Research</a></li> </ul> </li> <li>FPN <ul> <li>BiFPN</li> <li><a href="./resources/reading/1612.03144.pdf">Feature Pyramid Networks for Object Detection - Facebook AI Research</a></li> </ul> </li> </ul> <h1 id="resources-1">Resources</h1> <ul> <li><a href="./resources/reading/2205.06175.pdf">GATO: A Generalist Agent</a> <ul> <li>https://openreview.net/pdf?id=1ikK0kHjvj [z:1125.23023.88829] #cognitive-science#transformer#BERT# [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding]{ref=./resources/reading/1810.04805.pdf} Bidirectiional Encoder Representations from Transformers [z:1125.23023.70763] #cognitive-science# [Photorealistic Text-to-Image Diffision Models with Deep Language Understanding]{ref=./resources/reading/2205.11487.pdf} [z:1136.43023.31142] #cognitive-science# [DistilBERT, a distilled version of BERT: smaller, faster, cheaperr and lighter]{ref=./resources/reading/1910.01108.pdf} Based on a technique known as knowledge distilation.</li> </ul> </li> </ul> <p>Authors achieved a reduced model size by 40%, retaining 97% of its language understanding, while being 60% faster.</p> <p>An important factor of large language models is their ability to learn inductive biases. In other words, they store knowledge in their many parameters.</p> <p>Models are trained to minimize cross-entropy loss between one-hot empirical distribution of training labels and the models predicted distribution. During training, the model uses a temperature parameter on softmax function, but during inference, the temperature is set to 1 to recover the standard softmax function.</p> <p>The final training object is a linear combination of supervised learning loss, cross entropy loss between the teacher and student, and a cosine embedding loss aimed to align the directions of the teacher and students hidden states vectors.</p> <p>Transformer architecture (linear layer followed by normalization) is highly optimized in machine learning frameworks so the authors found reducing the number of layers a more effective reduction in computation. To initialize the student and taking advantage of same dimensionality, one out of two layers were taken.</p> <p>DistillBERT on 8 16GB V100 GPUs for approximately 90 hours. In comparison, RoBERTa model Liu et al. 2019 required 1 day of training on 1024 32GB v100. [z:1125.23023.64564] #cognitive-science#machine-learning#image-generation#image-editing#image-inpainting#diffusion-models#guided-diffusion#clip#contrastive-learning# [GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models]{author=Alex Nichol; author=Prafulla Dhariwal; author=Aditya Ramesh; author=Pranav Shyam; author=Pamela Mishkin; author=Bob McGrew; author=Ilya Sutskever; author=Mark Chen; ref=./resources/reading/2112.10741.pdf} [z:1125.23023.23147] #cognitive-science#ai-model#bart# [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension]{ref=./resources/reading/1910.13461.pdf} [z:1125.23023.12728] #cognitive-science#3d-model#ai-model# [Point-E: A System for Generating 3D Point Clouds from Complex Prompts]{ref=./resources/reading/2212.08751.pdf} [z:1614.65222.55506] #cognitive-science#model-architecture# [Attention is All You Need]{author=Google Brain; ref=./resources/reading/Attention-is-All-You-Need-by-Google-Brain.pdf;}</p> <ul> <li>Most Seq2Seq implementations use recurrent neural networks or CNNs which are sequential and slow.</li> <li>Most architectures include an encoder and decoder.</li> <li>This paper proposes a new architecture relying purely on attention mechanisms.</li> <li>Attention mechanisms identify dependencies among items regardless of distance</li> <li>Most attention models have non-constant dependency mapping compared to distance.</li> <li>How does multi-head attention conteract the reduce effection resolution due to averaging attention-weighted positions</li> </ul> <h1 id="parallelism">Parallelism</h1> <p>One key feature the transformer promotes is parallelism. Inputs don’t depend on each other.</p> <h1 id="architecture-1">Architecture</h1> <h2 id="positional-embedding">Positional embedding</h2> <p>Embedding space places words in a space based on where they occur in sentences, how often, what context they appear in, and what words appear around them. Adding a positional embedding is like adding a vector to the word embedding, slightly moving it through its embedding space, changing its meaning.</p> <p>The paper uses sin and cosine functions to construct the positional embedding by encoding different resolutions at different positions of the word embedding. The initial indeces clearly distinguish immediate position at a loss of general position and the later indeces distinguish general areas with a loss of immediate positions. Don’t want to overweigh semantic embedding with positional embedding. sin and cosine have a fixed output range [-1,1] which means they won’t overinfluence the semantic embedding of words.</p> <p>Positional embeddings should be viewed as a pair of functions $sin(\omega_k \cdot t)$ and $cos(\omega_k \cdot t)$ where $\omega_k = \frac{1}{10000^{2k/d}}$. A nice property of these functions is it allows differentiating relative position differences with a linear function which allows the model to attend to relative word positions. Positional encodings represent a binary encoding of float values. They also generate embedings that differ symmetrically and nicely with time. Using both sine and cosine allow the model to form linear transformation that allows attending to relative positions, using a single one disallows this. The $\omega_k$ allows representing a sort of binary encoding.</p> <p>Think of sin and cos as expressing the x and y position on the unit circle, in a sense halfing the dimension of the embedding vectors.</p> <h2 id="multi-head-attention">Multi-head attention</h2> <p>Teaches the model to form multiple representation subspaces. Each subspace leads to isolating properties that each head wants to attend to, which also allows parallelization of attention.</p> <p>Multiplying the query and key matrices results in a matrix representing each word which attends to every other word.</p> <h2 id="decoder-1">Decoder</h2> <p>The decoder has a mask to prevent the self-attention layer from attending to future positions in the output sequence. This mask exists during training to allow parallel computation of a sequence, allowing getting the error of a mini-batch. In order to avoid the accumulation of errors of predicted outputs, we can use the real value to attend to. During inference, the model is run sequentially and even if it was parallelized, it would need a mask to avoid new input from the otherwise masked QK. Note that during inference, the output of decoders can be reused for future inferencing, improving performance.</p> <h1 id="limitations">Limitations</h1> <p>Large contexts. Maybe a “meta” context? Quickly summarize, then go back through, passing through the generated “summary”. The issues that big context attention has is similar to pitfalls of human attention. LongNet Another solution is memory. Instead of paying attention to more parts of a the input, store important parts of the context in memory. Include a goal module that selects what part of context to pay attention to first, store it in memory, then use it. Convlution attention.</p> <h1 id="other">Other</h1> <p>[z:1821.40624.10914] [Gemini v1.5 report]{ref=./resources/reading/gemini-v1-5-report.pdf} [z:0710.60624.63206] [V-JEPA: Revisiting Feature Prediction for Learning Visual Representation from Video]{ref=./resources/reading/427986745-768441298640104-1604906292521363076-n.pdf}</p> <ul> <li>https://openai.com/research/video-generation-models-as-world-simulators</li> <li>https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/?utm_source=substack&amp;utm_medium=email</li> <li> <p>https://github.com/facebookresearch/jepa/tree/main</p> </li> <li>https://github.com/eugeneyan/open-llms</li> <li>https://huggingface.co/docs/transformers/performance</li> <li>https://youtu.be/JXT4vfdhs2A</li> <li>https://github.com/Sentdex/Falcon-LLM</li> <li>[Let’s build GPT: from scratch, in code, spelled out]{ref=https://www.youtube.com/watch?v=kCc8FmEb1nY&amp;list=WL&amp;index=31&amp;t=21s&amp;pp=gAQBiAQB; resource=website;}</li> <li>[Tree of Thoughts: Deliberate problem solving with large language models]{ref=https://www.youtube.com/watch?v=ut5kp56wW_4; resource=website;}</li> </ul> <h1 id="modalities">Modalities</h1> <p>LLM Reinforcement learning</p> <h1 id="models">Models</h1> <ul> <li>#llm#llava# [LLaVA: Visual Instruction Tuning]{ref=./resources/reading/2304.08485.pdf}</li> <li>#alphageometry#deepmind# [Solving olympiad geometry without human demonstrations]{subject=Nature, doi:10.1038/s41586-023-06747-5; author=Trieu H. Trinh; ref=./resources/reading/s41586-023-06747-5.pdf}</li> <li>#agi#llm# [V*: Guided Search as a Core Mechanism in Multimodal LLMS]{ref=./resources/reading/2312.14135.pdf}</li> <li>#cognitive-science# [Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions]{ref=./resources/reading/qtransformer.pdf}</li> <li>#cognitive-science# [JARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models]{ref=./resources/reading/2311.05997.pdf}</li> <li> <p>#cognitive-science# [A* Search Without Expansions: Learning Heuristic Functions with Deep Q-Networks]{ref=./resources/reading/2102.04518.pdf} A* works to minimize the function $f(n) = g(n) + h(n)$ where $g(n)$ is the path from the start to n and h is the heuristic function. A Q-function can be used to guide search based which learns the transition costs and heuristic values. This essentially learns the shortest path from any node to the end.</p> <p>A potential issue is that this only encodes a single goal. It possibly has trouble encoding multiple goals, or temporarily switching goals, lack of metacognition, etc.</p> <p>Can be used as an abstract goal following AI. The nodes here only represent goal states, and not states in the surrounding environment. That means the optimal goal path needs to be research (might be efficient ways to do this).</p> </li> </ul> <h2 id="alphafamily">AlphaFamily</h2> <ul> <li>#cognitive-science#DeepMind# [AlphaGo]{author=DeepMind; ref=./resources/reading/AlphaGoNaturePaper.pdf; resource=pdf;} <ul> <li>Train on 100k of self-play</li> <li>About 17 iterations leads to world-championship (1.7M games)</li> <li>Play against old version. If new version beats old version, new version becomes the standard. <ul> <li>Also note positions that the previous model had trouble with. Ensure that the new model doesn’t have the same issues.</li> <li>This means that after 100k iterations, bad traits can propogate.</li> <li>Sometimes models can become adversarial and not find general solutions.</li> <li>Adversarial means essentially that the model focuses on beating individuals rather than the game. They are more short term focused. Not focused on the overall goal.</li> </ul> </li> <li>This method is like a mini-evolution</li> <li>Monte-carlo tree search <ul> <li>Values each move, then selects the most likely to succeed. Continues until the end of the game, accumulating the overall value of each position, in particular note that a win or loss will be weighted.</li> </ul> </li> </ul> </li> <li>#cognitive-science#DeepMind# [AlphaGo Zero]{author=DeepMind; ref=./resources/reading/AlphaGo-Zero—DeepMind;}</li> <li>#cognitive-science#DeepMind# [(AlphaZero) Mastering Chess and Shogi]{author=Self Play with a General Reinforcement Learning Algorithm - DeepMind; ref=./resources/reading/Mastering-Chess-and-Shogi-by-Self-Play-with-a-General-Reinforcement-Learning-Algorithm-by-DeepMind.pdf; resource=pdf;} The primary modules are a value network (fast-thinking), policy network, and MTCS (slow-thinking).</li> <li>#cognitive-science#DeepMind# [(MuZero) Mastering Atari,Go,Chess and Shogi]{author=Planning with a Learned Model - Deep Mind; ref=./resources/reading/Mastering-Atari,Go,Chess-and-Shogi-by-Planning-with-a-Learned-Model-by-Deep-Mind.pdf; resource=pdf;}</li> <li>#cognitive-science#DeepMind# [(AlphaStar) Grandmaster level in StarCraft II using multi agent reinforcement learning]{author=Deep Mind; ref=./resources/reading/Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning-by-Deep-Mind.pdf; resource=pdf;}</li> <li>#cognitive-science#DeepMind# [AlphaFold]{author=DeepMind; ref=./resources/reading/343019_3_art_0_py4t4l_convrt.pdf; resource=pdf;}</li> <li>Technical advances <ul> <li>Most complex system they built with 32 component algorithms and 60 pages of supplementary information</li> <li>There was no silver bullet, a complex system of interworking components. Emergence is the key here.</li> <li>Used attention</li> </ul> </li> </ul> <h2 id="llama">LLaMA</h2> <p>LLaMa 2: https://arxiv.org/pdf/2307.09288.pdf LLaMA code: fine-tuned for generating and discussing code[ LLaMA code: https://huggingface.co/TheBloke/CodeLlama-13B-fp16 LLaMA Python: Fine-tuned on 100B tokens of python code.</p> <ul> <li>[LLaMA: Open and Efficient Foundation Language Models]{author=Meta AI; ref=./resources/reading/2302.13971.pdf}</li> <li>[LLaMA 2: Open Foundation and Fine-Tuned Chat Models]{ref=./resources/reading/10000000-662098952474184-2584067087619170692-n.pdf}</li> <li>https://huggingface.co/nyanko7/LLaMA-7B/tree/main</li> <li>https://huggingface.co/huggyllama/llama-7b/tree/main</li> </ul> <h2 id="dall-e">DALL-E</h2> <ul> <li>#cognitive-science#machine-learning#DALL-E# [(DALL-E) Zero-Shot Text-to-Image Generation]{subject=Proceedings of the International Conference on Machine Learning 2020; author=Aditya Ramesh; author=Mikhail Pavlov; author=Gabriel Goh; author=Scott Gray; author=Chelsea Voss; author=Alec Radford; author=Mark Chen; author=Ilya Sutskever; ref=./resources/reading/2102.12092.pdf}</li> <li><a href="./resources/reading/2204.06125.pdf">DALL-E 2: Image Generation with CLIP Latents - OpenAI</a></li> <li>#cognitive-science#DALL-E# [DALL-E 2]{ref=./resources/reading/dall-e-2.pdf}</li> </ul> <h2 id="deepseek-coder">DeepSeek coder</h2> <ul> <li>https://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct</li> </ul> <h2 id="starling">Starling</h2> <ul> <li>https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha</li> </ul> <h1 id="column-transformer-graph">Column transformer graph</h1> <p>Gₙ: Generally refers to the below graphs. G_C: The cortex. Every layer except for III receive input from the thalamus. G₂₃: Concerns itself with concepts. Forms understanding G₅: Concerns itself with interacting with actions. In the brain, G₅ is in charge of physical movements. In AI, G₅ is the actions it can take (interacting with computer essentially) for things like text output, creating image, creating video, creating audio, moving mouse, etc. G₆: Feedback layer. G_T: Primary receiver of input both subthalamic and cortical areas. Each node represents a thalamic nuclei, each nuclei is in charge of attention of itself. Receives queries (inhibitory input) from G₅ or G₆? (also written Gₜ here).</p> <p>Context information perhaps comes from column local information, and long rang connections between corresponding “what” and “where” pathways. Parallel “what” and “where” pathways exist for all major sensory modalities. “what” pathways are invariant to rotation, rotation, and scale.”where” pathways form egocentric representations. Sparsity and few-hot encoding represents sparse activation of neurons. Real-valued vectors however represent computations, however with activation functions like ReLU, they perhaps approximate some notion of sparsity. G₂₃ does not touch G₆ but isn’t that necessary to provide context to sequences? Do we want other columns to change the attention of a g_T? How exactly do attention and neural circuits relate? Connections between any cortical layer should be considered a transformer block (attention and feed forward). Core computations are deciding which neurons should fire, feedback loops which compute sequences. The subthalamic functions should be considered evolutionarily old, but still replicatable by a column transformer. Some notion of space of concepts for each column. Does having context dependent sequences for concepts sufficiently provide notion of space?</p> <p>Try to get column transformer to represent sequences.</p> <p>Each pyramidal neuron and each micro column layer has an attention network and FFN</p> <p>Current generation of cortical transformer is not right. There should probably be a seperate computation from G₂₃ to G₅ and G₅ to G₂₃</p> <ul> <li>Inhibitory connections are like query paramemters.</li> <li>What is context?</li> <li>Reinforcement learning? RLHF, RLAIF, self-rewarding, etc. are perhaps the best options</li> <li>TRN?</li> <li>How to generate graph of column transformers</li> <li>What about displacement cells?</li> <li>Consciousness, emotions, subcortical structures?</li> <li>What are feedback connections?</li> <li>What kind of computation does G₂₃ and G₅ compute?</li> <li>How do we feed information into the CTG to approximate a LLM section, vision model, etc.</li> <li>Grid cells, place cells, path integration, etc. are fundamental and CTG needs to be designed with that in mind. To form a space, you need a frame of reference, points, and ways to move between those points in a consistent manner.</li> <li>How much of lower cortical structures need to be replicated?</li> <li>I need to go through, understand the types of computation being represented in the brain, abstract them and find the necessary computational structures to enable it.</li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[rl := reinforcement learning llm := large language model gpt := generative pretrained transformer]]></summary></entry><entry><title type="html">Cloud Technologies</title><link href="https://t-wilkinson.github.io/blog/2024/cloud-technologies/" rel="alternate" type="text/html" title="Cloud Technologies"/><published>2024-07-23T16:41:29+00:00</published><updated>2024-07-23T16:41:29+00:00</updated><id>https://t-wilkinson.github.io/blog/2024/cloud-technologies</id><content type="html" xml:base="https://t-wilkinson.github.io/blog/2024/cloud-technologies/"><![CDATA[<h1 id="service">Service</h1> <p>Cloud computing enables provisioning computational-based services through the internet. It is a form of shared resource interfaced through a service. Advantages cloud technologies provide include</p> <ul> <li>Cost</li> <li>Speed and agility</li> <li>Security</li> <li>Maintenance</li> <li>Scalability and elasticity</li> <li>Reliability</li> <li>Multitenancy</li> <li>Easy access to resources</li> </ul> <h1 id="virtualization">Virtualization</h1> <p>Hypervisors allow virtualizing computer hardware like CPU, RAM, storage, devices, etc. Emulators allows for any OS to run on any architecture. Hypervisors allow creating virtual machines which allow running operating systems on top of. Type-1 hypervisors run on bare metal while Type-2 hypervisors run on top of an existing operating system. Type-1 are more geared towards enterprises and Type-2 are for end-users.</p> <p>Type-1 hypervisor:</p> <ul> <li>AWS Nitro</li> <li>IBM z/VM</li> <li>Microsoft Hyper-V</li> <li>Nutanix AHV</li> <li>Oracle VM Server for SPARC</li> <li>Oracle VM Server for x86</li> <li>Red Hat Virtualization</li> <li>VMware ESXi</li> <li>Xen</li> </ul> <p>Type-2 hypervisor:</p> <ul> <li>Parallels Desktop for Mac</li> <li>VirtualBox</li> <li>VMware Player</li> <li>VMware Workstation.</li> </ul> <p>Type-1 and Type-2 hypervisor:</p> <ul> <li>KVM</li> <li>bhyve</li> </ul> <h2 id="kvm">KVM</h2> <p>Kernel-based Virtual Machine is a hardware-assisted virtualization software solution for Linux on x86 hardware (but it has been ported to other architectures). It allows for device abstraction for network interfaces, disks, but not the processor. It is a loadable kernel module which can insert itself between the hardware and KVMs guest operating system. Various hardware virtualization extensions must be supported by processors to work. It also supports overcommitting and hotpluggable devices.</p> <h2 id="vagrant">Vagrant</h2> <p>Vagrant is a solution to using virtual machines in a development environment, providing reproducibility, transparency, easy management of multiple projects, sharing development environments, and consistent environments across various hypervisors (even cloud provided ones). Vagrant primarily relies on a Vagrantfile which functions similar to a Dockerfile, but describing entire (multi) operating system environments. One can specify hardware, operating system, programs to install, persistent volumns and shared files, networking, and even specify multiple operating systems and how they interact, opposed to applications in Docker. Vagrant can switch hyperviser providers, allowing one to use type-1 or type-2 hypervisors, allowing development on local computers or just as easily on a cloud provider.</p> <h1 id="storage-options">Storage options</h1> <p>Boot disk: Persistent: high availability Local SSD: Cloud storage: huge capacity. cap read and write from multiple compute instances.</p> <h1 id="introduction-to-cloud-infrastructure-technologies-course-outline">Introduction to cloud infrastructure technologies course outline</h1> <ol> <li>Virtualization <ul> <li>Introduction and Learning Objectives</li> <li>KVM</li> <li>VirtualBox</li> <li>Vagrant</li> <li>Knowledge Check</li> </ul> </li> <li>Infrastructure as a Service (IaaS) <ul> <li>Introduction and Learning Objectives</li> <li>Amazon Elastic Compute Cloud</li> <li>Azure Virtual Machine</li> <li>DigitalOcean Droplet</li> <li>Google Compute Engine</li> <li>IBM Cloud Virtual Servers</li> <li>Oracle Cloud Compute Virtual Machines</li> <li>OpenStack</li> <li>Knowledge Check</li> </ul> </li> <li>Platform as a Service (PaaS) <ul> <li>Introduction and Learning Objectives</li> <li>Cloud Foundry</li> <li>Red Hat Openshift</li> <li>The Heroku Platform</li> <li>Knowledge Check</li> </ul> </li> <li>Containers <ul> <li>Introduction and Learning Objectives</li> <li>Containers</li> <li>Project Moby</li> <li>Knowledge Check</li> </ul> </li> <li>Containers: Micro OSes for Containers <ul> <li>Introduction and Learning Objectives</li> <li>Alpine Linux</li> <li>BusyBox</li> <li>Fedora CoreOS</li> <li>Flatcar Container Linux</li> <li>RancherOS and k3OS</li> <li>Ubuntu Core</li> <li>VMWare Photon OS</li> <li>Knowledge Check</li> </ul> </li> <li>Containers: Container Orchestration <ul> <li>Introduction and Learning Objectives</li> <li>Kubernetes</li> <li>Kubernetes Hosted Solutions and Platforms</li> <li>Docker Swarm</li> <li>Nomad by HashiCorp</li> <li>Cloud Container Orchestration Services</li> <li>Knowledge Check</li> </ul> </li> <li>Unikernels <ul> <li>Introduction and Learning Objectives</li> <li>Unikernels</li> <li>Knowledge Check</li> </ul> </li> <li>Microservices <ul> <li>Introduction and Learning Objectives</li> <li>Microservices</li> <li>Knowledge Check</li> </ul> </li> <li>Software-Defined Networking and Networking for Containers <ul> <li>Introduction and Learning Objectives</li> <li>Software-Defined Networking (SDN)</li> <li>Networking for Containers</li> <li>Docker Networking</li> <li>Podman Networking</li> <li>Kubernetes Networking</li> <li>Cloud Foundry: Container to Container Networking</li> <li>Knowledge Check</li> </ul> </li> <li>Software-Defined Storage and Storage Management for Containers <ul> <li>Introduction and Learning Objectives</li> <li>Ceph</li> <li>GlusterFS</li> <li>Docker Volumes</li> <li>Podman Volumes</li> <li>Kubernetes Volumes</li> <li>Cloud Foundry Volume Service</li> <li>Container Storage Interface (CSI)</li> <li>Knowledge Check</li> </ul> </li> <li>DevOps and CI/CD <ul> <li>Introduction and Learning Objectives</li> <li>CI/CD: Jenkins</li> <li>CI/CD: Travis CI</li> <li>CI/CD: Concourse</li> <li>Cloud Native CI/CD</li> <li>Knowledge Check</li> </ul> </li> <li>Tools for Cloud Infrastructure I (Configuration Management) <ul> <li>Introduction and Learning Objectives</li> <li>Ansible</li> <li>Puppet</li> <li>Chef</li> <li>Salt</li> <li>Knowledge Check</li> </ul> </li> <li>Tools for Cloud Infrastructure II (Build &amp; Release) <ul> <li>Introduction and Learning Objectives</li> <li>Terraform</li> <li>CloudFormation</li> <li>BOSH</li> <li>Knowledge Check</li> </ul> </li> <li>Tools for Cloud Infrastructure III (Key-Value Pair Store) <ul> <li>Introduction and Learning Objectives</li> <li>etcd</li> <li>Consul KV</li> <li>ZooKeeper</li> <li>Knowledge Check</li> </ul> </li> <li>Tools for Cloud Infrastructure IV (Image Building) <ul> <li>Introduction and Learning Objectives</li> <li>Building Images with Docker</li> <li>Building Container Images with Podman</li> <li>Building Container Images with Buildah</li> <li>Packer</li> <li>Image Registries</li> <li>Knowledge Check</li> </ul> </li> <li>Tools for Cloud Infrastructure V (Debugging, Logging, and Monitoring for Containerized Applications) <ul> <li>Introduction and Learning Objectives</li> <li>Sysdig</li> <li>cAdvisor</li> <li>Elasticsearch</li> <li>Fluentd</li> <li>Datadog</li> <li>Prometheus</li> <li>Splunk</li> <li>OpenTelemetry</li> <li>Dynatrace</li> <li>Knowledge Check</li> </ul> </li> <li>Service Mesh <ul> <li>Introduction and Learning Objectives</li> <li>Features and Implementation of Service Mesh</li> <li>Consul</li> <li>Envoy</li> <li>Istio</li> <li>Kuma</li> <li>Linkerd</li> <li>Traefik Mesh</li> <li>Tanzu Service Mesh</li> <li>Knowledge Check</li> </ul> </li> <li>Internet of Things (IoT) <ul> <li>Introduction and Learning Objectives</li> <li>Internet of Things</li> <li>Knowledge Check</li> </ul> </li> <li>Serverless Computing <ul> <li>Introduction and Learning Objectives</li> <li>Serverless Computing</li> <li>AWS Lambda</li> <li>Google Cloud Functions</li> <li>Azure Functions</li> <li>Serverless and Containers</li> <li>Knowledge Check</li> </ul> </li> <li>OpenTracing <ul> <li>Introduction and Learning Objectives</li> <li>OpenTracing</li> <li>Jaeger</li> <li>Knowledge Check</li> </ul> </li> <li>How to Be Successful in the Cloud <ul> <li>Introduction and Learning Objectives</li> <li>Developing Skills</li> <li>Challenges</li> <li>Knowledge Check</li> <li>Course Feedback</li> </ul> </li> </ol> <h1 id="cloud-services">Cloud services</h1> <p>Cloud services provide scalability, handling of hardware, abstractions, shared solutions, cost savings, and more. They may lack features or force applications to be built in certain ways. Cloud servers should be considered ephemeral and failable, but also cheap and scalable. Cloud services, scaling, containerization, and orchestration should be viewed as exposing common necessary functionality for an internet-based service: DNS, storage, compute, servers, database, load-balancer, log, data analytics, monitoring, CDN, etc.</p> <p>Providers: Azure is no-code focused AWS is complicated but complete GCP is a happy medium Digital ocean has good api and ease of use</p> <p>There are also private providers like OpenStack which offers software for custom cloud servers, or in building bespoke cloud server setups.</p> <p>IaaS: Infrastructure as a Service; everything above the hardware is managed through software like operating system, applications, databases, application runtime, virtual network, servers, storage PaaS: Platform as a Service; platform for running software is provided and managed and configured by user; user has access to applications, databases, application runtimes, application configuration. SaaS: Software as a Service; software is provided and user can interact through the software to manage and configure it</p> <p>Terms: Access to the cloud: CLI, website GUIs, SDK, API Regions and availability zones: making services more redundant, lower latency, higher availability; geographic DNS region Virtual private servers: pay for what you use per sec min or hour; easily scalable; can choose the images memory storage networking compute distro applications available; you can build your own images with Packer Serverless: charged for compute time used; functions-as-a-service; docker containers, or supported programming languages exposing functions available to the serverless function are activated in response to some predefined trigger; Networking: set address ranges of networks, define subnets, configure routes, set firewall rules, and construct VPNs to connect to external networks Storage: object stores (files) like AWS S3 or Google Cloud Storage storage large amounts of data supporting mostly read access, block storage can be attached to compute instances, ephemeral; they also offer relational databases like MySQL, AWS Redshift, GCP BigQuery and in-memory and NoSQL databases like Redis or memcached Identity and Access Management IAM: cloud services access should be provisioned accordingly Automation: more complex topologies require verbose and error prone scripts, AWS CloudFormation and other platform equivalents allow providing a file description; Terraform and similar applications abstract and handle the verbosity and are platform agnostic Cost control: compute time for load balancers servers databases etc., storage used, data transfer, up time; - pay as you go is cheapest for very limited use, but it is the most expensive rate; on AWS and DigitalOcean it is usually chaged per hour, GCP is charged per minute; AWS can be difficult to calculate exact price as you must decide before hand the compute you want and how long, GCP decreases per week of usage, down to 60%; on demand services may not be available when requested, but provisioned resources are; AWS provides spot pricing, where participants declare max bids of compute time rates for processing-heavy workloads; - network traffic can be hard to predict: websites reciving large file, image, video, pdf uploadss; replication intraregion iter-zone or inter-region for fault tolerance; storage that spans multiple regions</p> <h2 id="services">Services</h2> <p>Reviewing services not directly related to web servers could help with understanding futurology.</p> <p>Migrations: database, cloud</p> <p>Compute - Container-based compute that enables auto scaling through serverless or automatically managed infrastructure - Self-managed compute - Automated compute for solutions like web apps based on Wordpress, or providing other similar common solutions. - Heavy compute for heavy workloads (GPUs, TPUs, lots of RAM) for ML, scientific computing, 3D visualization - Providing runtimes - Serverless: App Engine, Cloud Run, AWS Lambda - EC2 (Elastic Compute Cloud) - Batch processing: GC Batch or AWS Batch - Build containers - Run containers Containers: a self-contained packaged application that can run on any supporting virtual machine, is scalable horizontally or vertically, modular, and replicatable. - Compute for containers Content delivery Database - Cloud managed SQL (PostgreSQL, MySQL, Sqlite) or custom SQL (Cloud SQL, AWS RDS) - Cloud managed NoSQL (Redis, Memcached, Memorystore) or custom NoSQL (DynamoDB, ElastiCache) Machine learning Networking - Common solutions like: CDN, DNS, Firewall, NAT, Load balancing, VPC, Domain management Operations Payment Security, Identity, &amp; Compliance Serverless Scalable: horizontally or vertically scaling; monitor the status of the applications to determine what necessary configuration to match the server load Storage - Object storage - File system: that can be mounted on other servers (userful for aggregating logs, extending storage, etc.) - Local SSD - Persistent storage - Backup - File cache - Block storage Analytics Monitoring</p> <p>Solutions: Machine learning: containers, compute GPU TPU, storage for model data Web development: compute, CDN, networking, storage, database, security, payment, analytics, monitoring Simulation: requires lots of compute 3D visualation: requires lot of compute Scientific computing: requires lots of data and compute</p> <p>Customer enablement Blockchain Satellite Quantum technologies Management and governance Frontend web and mobile XR: extended reality like mixed reality, AR and VR Application integration Cost management Cloud migration Cloud operations Database migrations Data lakes and analytics DevOps Archiving E-Commerce Edge computing High performance computing Hybrid cloud architectures IoT Modern application development Multicloud Remote work Resilience Scientific computing Websites</p> <p>AWS: Analytics Application Integration Blockchain Business Applications Cloud Financial Management Contact Center Developer Tools End User Computing Front-End Web &amp; Mobile Games Internet of Things Management &amp; Governance Media Services Migration &amp; Transfer Quantum Technologies Robotics Satellite Supply Chain</p> <p>GCloud: Data Analytics Developer Tools Distributed Cloud Hybrid and Multicloud Industry Specific Integration Services Management Tools Maps and Geospatial Media Services Migration Operations Productivity and Collaboration Web3</p> <p>Integration Services: Amazon MQ: Amazon MQ is a managed message broker service that makes it easy to run ActiveMQ and RabbitMQ on AWS infrastructure. It automatically handles provisioning for high availability, setup, software and security updates, and other maintenance tasks. Amazon SQS (Simple Queue Service): With SQS, you can send, store, and receive messages between microservices and serverless applications. It’s a fully managed service that has two types of message queues. Standard queues are best for maximum throughput and at-least-once delivery, while FIFO guarantees that messages are processed exactly once in the exact sending order. Amazon SNS (Simple Notification Service): Simple notification service provides high-throughput pub/sub fanout messaging to many subscribers, including Amazon SQS, AWS Lambda functions, HTTPS endpoints, and Amazon Kinesis Data Firehose. It can also send messages to users via email, mobile push notifications, and SMS. It’s a fully managed service, so you pay for use. Amazon SES (Simple Email Service): Amazon Simple Email Service is a flexible email service that supports mass email communication, marketing, and transactional emails at scale. It offers some analytics and calculates open and click-through rates to measure audience engagement in your email messages. AWS Secrets Manager: This service protects the secrets needed to access various APIs and resources required for your application. AWS Secrets Manager provides an API to retrieve secrets, so you don’t have to hardcode them in plain text configuration files. Access to these secrets controlled with fine-grained permissions.</p> <p>Management &amp; Governance: AWS CloudFormation: CloudFormation enables you to describe your desired resources and their dependencies with a code template as a single stack. You can provision, manage, update, and delete these stacks as single units without managing underlying resources individually. AWS CloudWatch: CloudWatch provides a unified view of AWS resources and services of your application. It’s helpful to optimize resources, detect abnormal behavior, set different alarms, monitor application health, and troubleshoot issues by viewing logs and various metrics.</p> <p>Solutions: Advertising and Marketing: Reimagine data-driven marketing Aerospace and Satellite: AWS provides secure, flexible, scalable, and cost-efficient cloud solutions to help commercial and government customers build satellites, conduct space and launch operations, and reimagine space exploration Automotive: Build intelligent connected experiences and accelerate time to market for every touchpoint of the customer journey Consumer Packaged Goods: Solutions to transform manufacturing, optimize end-to-end supply chain, and drive more profitable brand engagements and transformative interactions with brand consumers Education: Solutions to help facilitate teaching, learning, student engagement, and better learning outcomes as well as modernize enterprise wide IT operations Energy and Utilities: Revamp legacy operations and accelerate the development of innovative renewable energy business models Financial Services: Develop innovative and secure solutions across banking, capital markets, insurance, and payments. Games: Services to enable game development across all genres and platforms, from AAA games to small independent studios Government: Services designed to help government agencies modernize, meet mandates, reduce costs, drive efficiencies, and delivery mission outcomes Healthcare and Life Sciences: Solutions for increasing the pace of innovation, data lifecycle management, incorporating new technology into care delivery, and improving security and compliance Industrial: Services and Solutions for customers across Manufacturing, Automotive, Energy, Power &amp; Utilities, Transportation &amp; Logistics Manufacturing: Optimize production and speed time-to-market Media and Entertainment: Transform media &amp; entertainment with the most purpose-built capabilities and partner solutions of any cloud Nonprofit: Services enabling more mission for the money to minimize costs and optimize scale and donor engagement to further nonprofits and NGO’s serving their causes Power and Utilities: Solutions to extract deep insights from data to manage distributed energy networks and to deliver engaging new customer experiences Retail: Cloud solutions that help retailers accelerate innovation, optimize operations, and delight customers Semiconductor: Speed innovation, optimize production, and deliver cutting-edge products and services Sports: Fuel innovative fan, broadcast, and athlete experiences Sustainability: AWS provides knowledge and tools for organizations of all sizes across all sectors to build and implement solutions that meet their sustainability goals Telecommunications: Accelerate innovation, scale with confidence, and add agility with cloud-based telecom solutions Travel and Hospitality: Solutions to help travel and hospitality companies gain a competitive edge by enhancing customer experiences and operational efficiency</p> <p>AWS featured: Amazon EC2: Virtual servers in the cloud Amazon Simple Storage Service (S3): Scalable storage in the cloud Amazon Aurora: High performance managed relational database with full MySQL and PostgreSQL compatibility Amazon DynamoDB: Managed NoSQL database Amazon RDS: Managed relational database service for PostgreSQL, MySQL, MariaDB, SQL Server, Oracle, and Db2 AWS Lambda: Run code without thinking about servers Amazon VPC: Isolated cloud resources Amazon Lightsail: Launch and manage virtual private servers Amazon SageMaker: Build, train, and deploy machine learning models at scale</p> <p>Featured products: Compute Engine: Virtual machines running in Google’s data center. Cloud Storage: Object storage that’s secure, durable, and scalable. BigQuery: Data warehouse for business agility and insights. Cloud Run: Fully managed environment for running containerized apps. Google Kubernetes Engine: Managed environment for running containerized apps. Vertex AI platform: Unified platform for ML models and generative AI. Looker: Platform for BI, data applications, and embedded analytics. Apigee API Management: Manage the full life cycle of APIs anywhere with visibility and control. Cloud SQL: Relational database services for MySQL, PostgreSQL, and SQL Server. Cloud SDK: Command-line tools and libraries for Google Cloud. Cloud CDN: Content delivery network for delivering web and video. Cloud Functions: Event-driven compute platform for cloud services and apps.</p> <p>AI and Machine Learning: Vertex AI Platform: Unified platform for training, hosting, and managing ML models. Generative AI on Vertex AI: Build, tune, and deploy foundation models on Vertex AI. Vertex AI Search and Conversation: Generative AI apps for enterprise search and conversational AI. AutoML: Custom machine learning model training and development. Dialogflow: Conversation applications and systems development suite for virtual agents. Media Translation: Add dynamic audio translation directly to your content and applications. Natural Language AI: Sentiment analysis and classification of unstructured text. Recommendations AI: Deliver highly personalized product recommendations at scale. Speech-to-Text: Speech recognition and transcription supporting 125 languages. Text-to-Speech: Speech synthesis in 220+ voices and 40+ languages. Translation AI: Language detection, translation, and glossary support. Video AI: Video classification and recognition using machine learning. Vision AI: Custom and pre-trained models to detect emotion, text, more. Vertex AI Notebooks: A single interface for your data, analytics, and machine learning workflow. Vertex Explainable AI: Tools and frameworks to understand and interpret your machine learning models. Business Intelligence</p> <p>Analytics: Amazon Kinesis: With Kinesis, you can analyze real-time data streams with low-latency at any scale. It enables applications to collect, buffer, and process streaming data as it arrives and react to it instantly instead of waiting hours for data to be collected before processing begins. Amazon Redshift: Redshift provides a cost-effective way to build a data warehouse and run standard SQL queries against it. You can further analyze these results in various business intelligence tools to gain new insights from the underlying data. Amazon Athena: Athena is a serverless solution to analyze large datasets in Amazon S3 using standard SQL. It’s fast, easy to use, and doesn’t require complex ETL processes to prepare your data before analysis. You pay only for the amount of data scanned when running each query. AWS Glue: With serverless AWS Glue, data integration becomes much more comfortable. It helps discover and extract data from multiple sources, prepare this data for use, and organize it into databases, data warehouses, and data lakes for further analysis by specialized tools and custom applications.</p> <p>Compute: App Engine: Serverless application platform for apps and back ends. Bare Metal Solution: Infrastructure to run specialized workloads on Google Cloud. Batch: Fully-managed batch service to schedule, queue, and execute batch jobs at scale. Cloud GPUs: GPUs for ML, scientific computing, and 3D visualization. Cloud Run: Fully managed environment for running containerized apps. Compute Engine: Virtual machines running in Google’s data center. Deep Learning VM Image: Preconfigured VMs for deep learning applications. Migrate to Virtual Machines: Server and virtual machine migration to Compute Engine. Recommender: Proactive, easily actionable recommendations to keep your cloud optimized. Shielded VMs: Reinforced virtual machines on Google Cloud. Sole-tenant Nodes: Dedicated hardware for compliance, licensing, and management. Spot VMs: Compute instances for batch jobs and fault-tolerant workloads. SQL Server on Google Cloud: Options for running SQL Server virtual machines on Google Cloud. Tau VM: Compute Engine virtual machines optimized for scale-out workloads. VMware Engine: Migrate and run your VMware workloads natively on Google Cloud. Amazon EC2: Virtual servers in the cloud Amazon EC2: Auto Scaling Scale compute capacity to meet demand Amazon Lightsail: Launch and manage virtual private servers AWS App Runner: Build and run containerized web apps at scale AWS Batch: Run batch jobs at any scale AWS Elastic Beanstalk: Run and manage web apps AWS Lambda: Run code without thinking about servers AWS Local Zones: Run latency sensitive applications on a Local Zone AWS Outposts: Run AWS infrastructure on-premises AWS Serverless Application Repository: Discover, deploy, and publish serverless applications AWS Sim: Space Weaver: Build dynamic, large-scale spatial simulations on AWS managed infrastructure AWS Snow Family: Physical devices to aggregate and process data in edge locations, then transfer to AWS AWS Wavelength: Deliver ultra-low latency applications for 5G devices VMware Cloud on AWS: Build a hybrid cloud without custom hardware Amazon EC2 (Elastic Compute Cloud): This service lets you rent virtual servers in the cloud. Most of them are virtual machines, ranging from tiny two vCPU and 0.5Gb of RAM to beefy 96-core machines with 384 Gb of RAM. You can also rent physical dedicated servers with EC2. The service is designed for any workload, although you’ll still need to choose the best instances for your specific tasks. Besides generic ones, EC2 also offers cost-saving ARM instances and bare metal Mac minis. Amazon ECR (Elastic Container Registry): ECR is a highly available and high-performance container registry for easy storage, management, and deployment of your container images. Images can be private to your organization or shared worldwide. It works great with Amazon ECS, AWS Lambda, and AWS Fargate, enabling fast single-click deployments. It’s a fully managed service, and you pay only for the amount of data stored and data transferred over the internet. Amazon ECS (EC2 Container Service): ECS is a fully managed container orchestration service that enables you to run, scale, and secure Docker applications on Amazon EC2 and AWS Fargate. You can define multiple related containers and configure their various deployment parameters in task definitions. Being a foundational pillar for critical Amazon services, it can natively integrate with Route 53, Secrets Manager, IAM, CloudWatch, and other services. AWS Fargate: Fargate removes the need to manually provision and manage servers, freeing time to focus on building your application. It’s a serverless compute engine for containers that works both with ECS and EKS (Amazon Elastic Kubernetes Service). It automatically allocates the right amount of isolated compute resources for each container, so there is no additional cost for over-provisioning to handle more load. By design, running each application in isolation also improves its security. AWS Lambda: Lambda is a serverless compute service that enables you to run your code on the AWS platform without worrying about provisioning, maintenance, and scaling the servers. Your code gets automatically executed on incoming events or HTTP requests at any scale. Most popular languages like Javascript, C#, Java, Go, and Python are supported, and deployment is effortless. AWS Elastic Beanstack: This easy-to-use service automatically handles the deployment of web applications. It handles provisioning, load balancing, auto-scaling, and app health monitoring without losing full control over the underlying resources. Elastic Beanstack the fastest and most straightforward way to deploy your application to AWS, and it doesn’t require extensive knowledge of the platform and its services.</p> <p>Containers: Artifact Registry: Store, manage, and secure container images and language packages. Cloud Build: Solution for running build steps in a Docker container. Cloud Code: IDE support to write, run, and debug Kubernetes applications. Cloud Deploy: Fully managed continuous delivery to GKE and Cloud Run. Cloud Run: Fully managed environment for running containerized apps. Deep Learning Containers: Containers with data science frameworks, libraries, and tools. Google Kubernetes Engine (GKE): Managed environment for running containerized apps. Knative: Components to create Kubernetes-native cloud-based software. Kubernetes applications on Google Cloud Marketplace: Containerized apps with prebuilt deployment and unified billing. Migrate to Containers: Components for migrating VMs into system containers on GKE. Amazon Elastic Container Registry: Easily store, manage, and deploy container images Amazon Elastic Container Service (ECS): Highly secure, reliable, and scalable way to run containers Amazon ECS Anywhere: Run containers on customer-managed infrastructure Amazon Elastic Kubernetes Service (EKS): The most trusted way to run Kubernetes Amazon EKS Anywhere: Kubernetes on your infrastructure Amazon EKS Distro: Run consistent Kubernetes clusters AWS App2Container: Containerize and migrate existing applications AWS App Runner: Build and run containerized web apps at scale AWS Copilot: Easily deploy and operate containerized applications AWS Fargate: Serverless compute for containers Red Hat Open: Shift Service on AWSManaged Red Hat Open: Shift clusters</p> <p>Databases: AlloyDB for PostgreSQL: Fully managed, PostgreSQL-compatible database for demanding enterprise workloads. Bare Metal Solution: Infrastructure to run specialized workloads on Google Cloud. Cloud Bigtable: Cloud-native wide-column database for large-scale, low-latency workloads. Cloud Spanner: Cloud-native relational database with unlimited scale and 99.999% availability. Cloud SQL: Fully managed database for MySQL, PostgreSQL, and SQL Server. Database Migration Service: Serverless, minimal downtime migrations to the cloud. Firestore: Cloud-native document database for building rich mobile, web, and IoT apps. Memorystore: In-memory database for managed Redis and Memcached. Datastream: Serverless change data capture and replication service. Firebase Realtime Database (not official GCloud product): NoSQL database for storing and syncing data in real time. Amazon Aurora: High performance managed relational database with full My: SQL and Postgre: SQL compatibility Amazon Aurora Serverless V2: Instantly scale to &gt;100,000 transactions per second Amazon Document: DB (with Mongo: DB compatibility)Fully managed document database Amazon DynamoDB: Managed NoSQL database Amazon ElastiCache: In-memory caching service Amazon Keyspaces (for Apache Cassandra): Managed Cassandra-compatible database Amazon Memory DB for Redis: Redis-compatible, durable, in-memory database that delivers ultra-fast performance Amazon Neptune: Fully managed graph database service Amazon Quantum Ledger Database (QLDB): Fully managed ledger database Amazon RDS: Managed relational database service for PostgreSQL, MySQL, MariaDB, SQL Server, Oracle, and Db2 Amazon RDS on Outposts: Automate on-premises database management Amazon Redshift: Fast, simple, cost-effective data warehousing Amazon Timestream: Fully managed time series database AWS Database Migration Service: Migrate databases with minimal downtime Amazon DynamoDB: DynamoDB is a high-performance managed NoSQL database that supports both key-value and document store. It can handle more than 10 trillion requests per day, with peaks of more than 20 million requests per second. This durable and secure database with built-in in-memory caching, backups, and restore is the number one choice for many fastest-growing startups that demand low-latency data storage at any scale. Amazon ElastiCache: This service offers fully managed Redis and Memcached as high-throughput and low-latency in-memory data stores for your cloud applications. ElastiCache’s primary purpose is to boost web applications’ performance by caching mission-critical data on top of slower databases. It is also suitable for session storage, real-time analytics, and other tasks. Amazon Aurora: Aurora is a MySQL and PostgreSQL-compatible high-performance distributed relational database. Out of the box, it’s much faster than both MySQL and PostgreSQL and offers high security, availability, and reliability of traditional commercial databases. On top of that, it provides replication across three Availability Zones, point-in-time recovery, and continuous backup to Amazon S3. Amazon RDS (Relational Database Service): This service manages relational databases in the cloud. It takes care of hardware provisioning, database setup, patching, and backups. Amazon RDS supports various database engines like Amazon Aurora, PostgreSQL, MySQL, Microsoft SQL Server, Oracle Database, and MariaDB.</p> <p>Developer tools: Artifact Registry: Store, manage, and secure container images and language packages. Assured Open Source Software: Incorporate the same OSS packages that Google uses into your own developer workflows. Cloud Build: Continuous integration and continuous delivery platform. Cloud Code: IDE support to write, run, and debug Kubernetes applications. Cloud Deploy: Fully managed continuous delivery to Google Kubernetes Engine. Cloud Deployment Manager: Service for creating and managing Google Cloud resources. Cloud Functions for Firebase: Write and run app logic server-side without needing to set up your own server. Cloud Identity: Unified platform for IT admins to manage user devices and apps. Cloud Scheduler: Cron job scheduler for task automation and management. Cloud SDK: Command-line tools and libraries for Google Cloud. Cloud Source Repositories: Private Git repository to store, manage, and track code. Cloud Tasks: Task management service for asynchronous task execution. Cloud Workstations: Managed and secure development environments in the cloud. Duet AI in Google Cloud (Preview): AI-powered collaborator, available across Google Cloud and your IDE. Firebase Authentication: End-to-end user identity solution to your app for user identification, sign-in, and onboarding. Firebase Crashlytics: Prioritize and fix stability issues faster. (not official GCloud product) Firebase Test Lab: On-demand testing infrastructure for Android apps. Gradle App Engine Plugin: Use Gradle for your App Engine projects. Infrastructure Manager: Automate deployment and management of Google Cloud infrastructure resources using Terraform Maven App Engine Plugin: Use Maven for your App Engine projects. Skaffold: Open source command line tool for Cloud Code, Cloud Build, and Cloud Deploy. Tekton: Kubernetes-native resources for declaring CI/CD pipelines. Tools for Eclipse: Plugin for Google Cloud development inside the Eclipse IDE. Tools for PowerShell: Full cloud control from Windows PowerShell.</p> <p>Networking: Cloud Armor: Security policies and defense against web and DDoS attacks. Cloud CDN: Content delivery network for serving web and video content. Cloud Connectivity: Connectivity options for VPN, peering, and enterprise needs. Cloud Domains: Easy domain name registration and management. Cloud DNS: Domain name system for reliable and low-latency name lookups. Cloud Firewall: Global and flexible firewalls to protect your cloud resources. Cloud IDS: Cloud IDS (Cloud Intrusion Detection System) provides cloud-native network threat detection with industry-leading security. Cloud Load Balancing: Service for distributing traffic across applications and regions. Cloud NAT: NAT service for giving private instances internet access. Network Connectivity Center: Use Google’s network as your own. Reimagine how you deploy, manage, and scale your networks. Network Intelligence Center: Network monitoring, verification, and optimization platform. Network Service Tiers: Cloud network options based on performance, availability, and cost. Private Service Connect: Secure connection between your VPC and services. Service Directory: Platform for discovering, publishing, and connecting services. Virtual Private Cloud (VPC): Virtual network for Google Cloud resources and cloud-based services. VPC Service Controls: Protect sensitive data in Google Cloud services using security perimeters. Amazon VPC: Isolated cloud resources Amazon VPC Lattice: Simplify service-to-service connectivity, security, and monitoring Amazon API Gateway: Build, deploy, and manage APIs Amazon Cloud: Front: Global content delivery network Amazon Route 53: Scalable domain name system (DNS) AWS App Mesh: Monitor and control microservices AWS Cloud Map: Service discovery for cloud resources AWS Cloud WAN: Easily build, manage, and monitor global wide area networks AWS Direct Connect: Dedicated network connection to AWS AWS Global Accelerator: Improve application availability and performance AWS Private 5G: Easily deploy, manage, and scale a private cellular network AWS Private: Link: Securely access services hosted on AWS AWS Transit Gateway: Easily scale VPC and account connections AWS Verified Access: Provide secure access to corporate applications without a VPN AWS VPN: Securely access your network resources Elastic Load Balancing (ELB): Distribute incoming traffic across multiple targets Amazon Route 53: Route 53 is an advanced, highly available, and scalable DNS Service. Besides simple IP lookups, it has sophisticated routing types like GeoDNS, Geoproximity, and Latency Based Routing. Together with health checks and DNS failover, this enables different fault-tolerant low-latency architectures configurable with a simple visual editor. Amazon CloudFront: CloudFront is a fast and secure programmable content delivery network (CDN) that caches your content and APIs on globally scaled edge locations for more rapid responses. It also offers protection against multiple types of attacks, including network, transport, and application-layer DDoS attacks. CloudFront is cost-effective and deeply integrated with other AWS services like S3, EC2, Route 53, and Elastic Load Balancing. Amazon API Gateway: API Gateway makes it easy to create, publish, monitor, and secure RESTful and WebSocket APIs. It handles traffic management, CORS, authorization and access control, throttling, monitoring, and API version management. API Gateway can process hundreds of thousands of concurrent API calls. It’s a fully managed service, and you pay only for the API calls your application receives and the amount of outgoing traffic. AWS Elastic Load Balancing: Elastic Load Balancing distributes incoming application traffic across multiple servers, containers, or Lambda functions. It enables the application to handle more concurrent requests without affecting response time. Multiple request handlers are crucial to make the application highly available, reliable, and fault-tolerant. Amazon VPC (Virtual Private Cloud): With Amazon VPC, you can create logically isolated virtual networks inside AWS. You have full control over the configuration of the network, its subnets, and routing tables. It’s possible to create a public-facing subnet with internet access for your web servers while keeping most of the backend infrastructure in a private subnet with no internet connection, making it much more secure.</p> <p>Operations: Cloud Debugger: Real-time application state inspection and in -production debugging. Cloud Logging: Google Cloud audit, platform, and application logs management. Cloud Monitoring: Infrastructure and application health with rich metrics. Cloud Profiler: CPU and heap profiler for analyzing application performance. Cloud Trace: Tracing system collecting latency data from applications. Error Reporting: Real time exception monitoring and alerting.</p> <p>Serverless: API Gateway: Develop, deploy, secure, and manage APIs with a fully managed gateway. App Engine: Serverless application platform for apps and back ends. Cloud Functions: Platform for creating functions that respond to cloud events. Cloud Run: Fully managed environment for running containerized apps. Workflows: Workflow orchestration for serverless products and API services. AWS Lambda: Run code without thinking about servers Amazon API Gateway: Build, deploy, and manage APIs Amazon DynamoDB: Managed NoSQL database Amazon EventBridge: Serverless event bus for SaaS apps &amp; AWS services Amazon Simple Notification Service (SNS): Pub/sub, SMS, email, and mobile push notifications Amazon Simple Queue Service (SQS): Managed message queues Amazon Simple Storage Service (S3): Scalable storage in the cloud AWS Application Composer: Visually design and build serverless applications quickly AWS AppSync: Fully-managed, scalable GraphQL APIs Amazon Redshift: Fast, simple, cost-effective data warehousing AWS Fargate: Serverless compute for containers AWS Step Functions: Coordination for distributed applications</p> <p>Storage: Cloud Storage: Object storage that’s secure, durable, and scalable. Storage Transfer Service: Tools and services for transferring your data to Google Cloud. Cloud Storage for Firebase: Object storage for storing and serving user-generated content. Filestore: File storage that is highly scalable and secure. Local SSD: Block storage that is locally attached for high-performance needs. Persistent Disk: Block storage for virtual machine instances running on Google Cloud. Google Cloud Backup and DR: Managed backup and disaster recovery for application-consistent data protection. NetApp Volumes: File storage service that supports NFS, SMB, and multi-protocol environments. Parallelstore: High performance, managed parallel file service. Block storage: High-performance storage for AI, analytics, databases, and enterprise applications. Amazon Simple Storage Service (S3): Scalable storage in the cloud Amazon S3 Glacier storage classes: Low-cost archive storage in the cloud Amazon Elastic Block Store (EBS): EC2 block storage volumes Amazon Elastic File System (EFS): Fully managed file system for EC2 Amazon FSx for Lustre: High-performance file system integrated with S3 Amazon FSx for Net: App ONTAPFully managed storage built on Net: App’s popular ONTAP file system Amazon FSx for Open ZFS: Fully managed storage built on the popular OpenZFS file system Amazon FSx for Windows File Server: Fully managed Windows native file system Amazon File Cache: High-speed cache for datasets stored anywhere AWS Backup: Centralized backup across AWS services AWS Elastic Disaster Recovery (DRS): Scalable, cost-effective application recovery AWS Snow Family: Physical edge computing and storage devices for rugged or disconnected environments AWS Storage Gateway: Hybrid storage integration Amazon S3 (Simple Storage Service): Amazon S3 is a generic object storage service designed for incredible durability, high scalability, availability, security, and performance. It has various storage classes for different use cases. S3 automatically stores copies of objects across multiple systems. It offers a fine-grained access control system and auditing capabilities for compliance. Using Amazon Athena, you can analyze data in S3 at any scale with simple SQL queries. Amazon S3 Glacier: For data archiving and long-term backups at extremely low-cost, Amazon offers S3 Glacier with extreme durability. There are three options for access to archives. Expedited retrievals typically return data in 1-5 minutes, standard generally complete in 3-5 hours, while the cheapest bulk retrievals take 5-12 hours to get large amounts of data. Amazon EBS (Elastic Block Storage): EBS is generic long-term high-performance block storage for EC2 instances. It’s designed for both throughput and transactional workloads and can scale to petabytes of data. You can choose different storage types with various throughput and latency suitable for your needs. The EBS replicates volumes within the Availability Zone, and you can use EBS Snapshots to backup your volumes to S3. Amazon EFS (Elastic File System): Amazon Elastic File System is a fully managed scalable elastic NFS. It grows and shrinks automatically, eliminating the need to provision and manually manage capacity. EFS is designed to provide massively parallel shared access to thousands of EC2 instances with a high level of aggregate throughput and IOPS, maintaining consistent latency.</p> <p>Security, Identity, and Compliance: AWS Identity and Access Management (IAM): Securely manage access to services and resources Amazon Cognito: Identity management for your apps Amazon Detective: Investigate potential security issues Amazon Guard: Duty: Managed threat detection service Amazon Inspector: Automate vulnerability management Amazon Macie: Discover and protect your sensitive data at scale Amazon Security Lake: Automatically centralize your security data with a few clicks Amazon Verified Permissions: Fine-grained permissions and authorization for your applications AWS Artifact: On-demand access to AWS’ compliance reports AWS Audit Manager: Continuously audit your AWS usage AWS Certificate Manager: Provision, manage, and deploy SSL/TLS certificates AWS Cloud HSM: Hardware-based key storage for regulatory compliance AWS Directory Service: Host and manage active directory AWS Firewall Manager: Central management of firewall rules AWS Key Management Service: Managed creation and control of encryption keys AWS Network Firewall: Network security to protect your VPCs AWS Payment Cryptography: Simplify cryptography operations AWS Private Certificate Authority: Create private certificates to identify resources and protect data AWS Resource Access Manager: Simple, secure service to share AWS resources AWS Secrets Manager: Rotate, manage, and retrieve secrets AWS Security Hub: Unified security and compliance center AWS Shield: DDoS protection AWS IAM Identity Center: Manage single sign-on access to AWS accounts and apps AWS WAF: Filter malicious web traffic</p> <h2 id="amazon-web-services">Amazon Web Services</h2> <p>AWS</p> <h2 id="google-cloud-products">Google Cloud Products</h2> <p>GCP</p> <ul> <li>https://cloud.google.com/products/calculator</li> <li>https://cloud.google.com/kubernetes-engine/pricing</li> <li>https://cloud.google.com/storage/pricing/</li> <li>https://cloud.google.com/storage/docs/storage-classes#standard</li> <li>https://cloud.google.com/functions/docs/tutorials/ocr/</li> <li>https://cloud.google.com/run/docs/fit-for-run</li> <li>https://cloud.google.com/kubernetes-engine?hl=en</li> <li>https://cloud.google.com/kubernetes-engine/docs/concepts/autopilot-overview</li> </ul> <p>Follow instructions at https://cloud.google.com/sdk/docs/install to set gcloud CLI.</p> <p>Cloud Functions: an entry point to the google cloud, also connects google cloud services to each other. Cloud Run: similar to Functions but specialized for self-contained computations AI Platform Predictions Kubernetes Engine: flexible scaling Artifact Registry: store containers and similar items Compute Engine: compute with its own ephemeral storage Cloud Storage: flexible, persistent storage accessible to most google products App Engine</p> <h2 id="google-cloud-functions-ai-inference">Google Cloud Functions AI inference</h2> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gcloud init
gcloud auth login
gcloud config <span class="nb">set </span>project YOUR_PROJECT_ID
gcloud services <span class="nb">enable </span>cloudfunctions.googleapis.com
gcloud services <span class="nb">enable </span>cloudbuild.googleapis.com

<span class="nb">mkdir </span>my_ai_model_function
<span class="nb">cd </span>my_ai_model_function
<span class="c"># main.py: This is your function code.</span>
<span class="c"># requirements.txt: This lists the Python dependencies for your function.</span>
<span class="c"># model.joblib: This is your trained AI model, serialized with joblib. You will need to train and serialize this model elsewhere and then place the file in the directory.</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># main.py
</span><span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">joblib</span>
<span class="kn">from</span> <span class="n">flask</span> <span class="kn">import</span> <span class="n">escape</span><span class="p">,</span> <span class="n">jsonify</span>

<span class="c1"># Load your trained model; make sure the path matches where you've put your model file.
</span><span class="n">model</span> <span class="o">=</span> <span class="n">joblib</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">'</span><span class="s">model.joblib</span><span class="sh">'</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">ai_model_inference</span><span class="p">(</span><span class="n">request</span><span class="p">):</span>
    <span class="n">request_json</span> <span class="o">=</span> <span class="n">request</span><span class="p">.</span><span class="nf">get_json</span><span class="p">(</span><span class="n">silent</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">request_json</span> <span class="ow">and</span> <span class="sh">'</span><span class="s">features</span><span class="sh">'</span> <span class="ow">in</span> <span class="n">request_json</span><span class="p">:</span>
        <span class="c1"># Assuming "features" is a list of features for the model to make a prediction.
</span>        <span class="n">features</span> <span class="o">=</span> <span class="n">request_json</span><span class="p">[</span><span class="sh">'</span><span class="s">features</span><span class="sh">'</span><span class="p">]</span>
        <span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">([</span><span class="n">features</span><span class="p">])</span>
        <span class="k">return</span> <span class="nf">jsonify</span><span class="p">({</span><span class="sh">'</span><span class="s">prediction</span><span class="sh">'</span><span class="p">:</span> <span class="n">prediction</span><span class="p">.</span><span class="nf">tolist</span><span class="p">()})</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="sh">'</span><span class="s">No features provided for inference.</span><span class="sh">'</span><span class="p">,</span> <span class="mi">400</span>
</code></pre></div></div> <pre><code class="language-txt">scikit-learn==0.24.2
joblib==1.0.1
numpy==1.20.3
</code></pre> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gcloud functions deploy ai_model_inference <span class="se">\</span>
    <span class="nt">--runtime</span> python39 <span class="se">\</span>
    <span class="nt">--trigger-http</span> <span class="se">\</span>
    <span class="nt">--memory</span> 2048MB <span class="se">\</span>
    <span class="nt">--timeout</span> 540s <span class="se">\</span>
    <span class="nt">--max-instances</span> 1

curl <span class="nt">-H</span> <span class="s2">"Content-Type: application/json"</span> <span class="se">\</span>
     <span class="nt">-d</span> <span class="s1">'{"features": [5.1, 3.5, 1.4, 0.2]}'</span> <span class="se">\</span>
     YOUR_FUNCTION_URL
</code></pre></div></div> <p>You can monitor the execution of your function and view logs in the Google Cloud Console under the Cloud Functions section. If you need to update your function or model, you can redeploy using the same gcloud functions deploy command.</p> <p>Additional Considerations Ensure that the model size and memory requirements fit within the limits of Google Cloud Functions. Keep in mind the cold start times. If your function hasn’t been invoked recently, the first invocation may take longer as the environment is initialized. For more complex or larger models, consider using Google Cloud Run or AI Platform Predictions, which can handle heavier workloads.</p> <p>By following these steps, you can deploy an open-source AI model to Google Cloud Functions and run inferences on-demand without maintaining a persistent server.</p> <p>For infrequent and sporadic AI model inferences, Google Cloud Functions or Google Cloud Run are typically the most cost-effective choices due to their per-invocation billing and ability to scale to zero, which means you don’t pay for anything when the services are not in use. They both offer a free tier, which can be sufficient for light workloads.</p> <p>AI Platform Predictions can be cost-effective if you leverage its features and scale, but it might be overkill for infrequent use unless you need its specific integrated services.</p> <p>Ultimately, the choice of product will depend on your specific requirements, such as the size and complexity of your AI model, the need for integration with other Google Cloud services, the expected latency, and your budget. The serverless options (Cloud Functions and Cloud Run) will generally offer the best cost savings for infrequent usage, while GKE and App Engine may offer more functionality and flexibility at a higher cost.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Service Cloud computing enables provisioning computational-based services through the internet. It is a form of shared resource interfaced through a service. Advantages cloud technologies provide include Cost Speed and agility Security Maintenance Scalability and elasticity Reliability Multitenancy Easy access to resources]]></summary></entry><entry><title type="html">Category Theory</title><link href="https://t-wilkinson.github.io/blog/2024/category-theory/" rel="alternate" type="text/html" title="Category Theory"/><published>2024-07-23T16:40:24+00:00</published><updated>2024-07-23T16:40:24+00:00</updated><id>https://t-wilkinson.github.io/blog/2024/category-theory</id><content type="html" xml:base="https://t-wilkinson.github.io/blog/2024/category-theory/"><![CDATA[<p>Category theory concerns itself with representing constructions with internal cohesion (how objects relate to each other through morphisms). Functors relate these contructions by embedding the domain construction into the codomain construction. Natural transformations allow comparing construction embeddings. This gives rise to topos theory, where a topos is a collection of constructions in $D$ made from $C$. It is insightful to view functors as objects existing in a category, and natural transformations as special morphisms preserving the internal structure of these functor objects. Morphisms preserve the structure of objects in context, natural transformation additionally preserve the structure of domains</p> <p>Category theory can be viewed as an algebraic object, like a groupoid without requirement of invertibility, or monoid without requirement of closed operations. In this lense, categories express some internal cohesion of elements expressed through morphisms. What makes categories useful is that they express necessary requirements for internal cohesion with an identity, which allows for proofs by algebra (or commuting diagrams) and identification of definitive objects. Sheaves on a category are like more general group actions. Continuing this perspective, functors are algebraic homomorphisms, maps between sets preserving the cohesive structure expressed by the domain category’s morphisms. In this sense, it is instructive to view categories as sets with some cohesivity, that is, a category should not in general be interpreted as a kind of set theory. Instead, we look at stacks valued in a topos (like set) which express category actions.</p> <p>Topos theory is a way to construct logic out of some type of construction. This logic is one that is conceptually useful and allows one to talk about various kinds of objects that exist from said construction. Universal properties like limits or kan extensions allow expressing for every relationship of a certain kind $F(A’) → X$, there is one construction its associated relationship $F(A) → X$ that it factors through $F(A’) → F(A)$. The functor is essentially projecting the category onto the construction $X$, isolating the objects of the domain of F that relate to $X$. Of the objects of the domain of F, these form a construction with X, and of those, F(A) is the best. Then we have exponential objects which represent all the ways A relates to B. The power object relates the function space and subobject classifier. Subobject classifier lets you talk about restrictions, smaller or larger object “spaces”, specify locations within an object.</p> <p>We like to look at objects as labels and the morphisms as expressing the elements and other relationships that objects have within a context of discorse. This is the value of representable functors and the yoneda embedding: morphisms express everything</p> <p>It is often instructive to look at functor spaces, a category of functors between two categories. We look at what kind of constructions functors represent, similar to looking at function spaces on topological spaces which represent embeddings. That is, we look at the construction or embedding and relationships, not as the extensional mapping between points or objects.</p> <p>Practicality: Category theory is a fundamental tool in math and physics for formalizing, organizing, comparing, concepts. It serves to abstract away irrelevant information to isolate the structure and semantics of concepts and provides ways to compare these structures, away from their nuances. It provides a single language to describe concepts found in disparate areas of mathematics and physics. Perhaps the most fundamental concept in category theory, universal properties, describes how an object relates to every construction in a context.</p> <p>A category is a context. Universal properties are different isolations of context. Take, for example, initial objects, which express the structure that every object/domain has, and terminal objects which express the underlying structure that every morphism/codomain must have.</p> <p>Category theory is an abstract species which enables lifting from the intricacies of various mathemtical fields to focus on the consistency (and inconsistency) between them. It enables sharing ideas, intuition, and proofs accross disciplines. One understands a category of a collection of objects of a certain type and relationships between those objects. The prevailing tool of category theory is that of a universal property which expresses the mathematical function of a object by how it relates to other objects within a categorical context. Functors recontextualize objects. The yoneda lemma expresses how any object can be charazterized purely by its universal properties, that is, its relationship with all other objects. Limits express how objects can be constructed out of existing ones.</p> <p>Mathematics is full of constructions which map objects from one category into another category. These constructions are functorial if they preserve the morphisms between objects as well. In other words, a functor is a mapping of categories which preserves the domain category structure.</p> <p>Categorification is the process of adding morphisms to existing sets to express the structure of set elements. Functorality of mathematical constructions can then be determined through categorical language.</p> <p>One can view all of mathematics as a site where objects represent axiosm with the covering representing which axioms imply(are implied by) other axioms. One sheaf maps axioms to collection of mathematical systems that satisfy those axioms. An index category maps existing mathematical subjects onto the site.</p> <p>What is it? Category theory is the abstract study of how things (manly mathematical objects) are related. A category defines 3 notions, categories: collection of objects, objects: elements of categories, and morphisms: arrows between objects. By studying how things relate, rather than what they are internally, we can ignore extraneous details and focus on the important differences.</p> <p>The most important notion in category theory is that of the universal property. A property which uniquely defines an object and an arrow to/from every other object in question. These properties are useful as they define how a singular object uniquely relates to all other objects.</p> <p>Category theory concerns itself less with how to find certain constructions, but instead what properties certain constructions would have, if they were found. It allows you convenient language to talk about properties and constructions.</p> <ul> <li>Basic definitions <ul> <li>A category consists of a collection of objects, arrows between the objects, and certain composition rules about the arrows.</li> <li>An arrow is between two objects. Two arrows that share a target/src can be composed to form a new arrow.</li> <li>For every object there is an identity arrow with same src and target, such that composition (in the natural way) with any other arrow is equal to that arrow.</li> </ul> </li> <li>Important concepts <ul> <li>Commuting Diagram</li> <li>Duality</li> <li>Isomorphism</li> </ul> </li> </ul> <h1 id="definition">Definition</h1> <p>A category consists of the following data:</p> <ul> <li>A class ob(C) of objects, for every object there is an identity morphism</li> <li>A class mor(C) of morphisms</li> <li>A domain and codomain class function: $dom: mor(C) → ob(C)$, $cod: mor(C) → ob(C)$</li> <li>A composition binary operator $hom(a,b) × hom(b,c) → hom(a,c)$, with associativity and respects the identity morphism</li> </ul> <p>Small and large categories: When the class of objects and morphisms is a set, the category is called small. If just the homsets are small, the category is called locally small.</p> <p>Small categories exist to avoid inconsistencies and paradoxes found with infinite sets. However, this avoids the creation of self-referential structures. For example the set of all sets. Universes in particular are a useful construction that utilizes self-referentiality. Locally small categories can include self-referential structures, or other proper classes of objects, but still restrict the homset between any two objects to be small, resulting in the best of both worlds.</p> <h1 id="important-constructions">Important constructions</h1> <h2 id="opposite-category">Opposite category</h2> <p>Mathematics is full of topological-like structures and other spaces. These spaces are often constructed through some notion of inclusion. The opposite category represents restricting these spaces to view a smaller, more local picture. Contravariant functors also capture this idea, further noting what happens when you assign information to these restrictions. Another view of opposite categories/contravariant functors is that they represent looking at various structures in the codomain or looking to trace morphisms from where they came instead.</p> <h2 id="morphisms">Morphisms</h2> <ul> <li>Monomorphism</li> <li>Bimorphism</li> <li>Epimorphism</li> <li>Isomorphism: are key in category theory as they allow defining to and referring to structures without describing their contents. Instead, one cares about how the structure behaves in a category. Isomorphic structures for all intents and purposes, are equal within a category. However, they could have different content which can be observed in other categories.</li> <li>Endomorphism</li> <li>Automorphism</li> <li>Retraction</li> <li>Section</li> </ul> <p>Category theory allows expressing structure through morphisms. Morphisms intrinsically represent the relationship of information. This is in contrast to the set theory perspective of understanding structure through expositional contents.</p> <p>Category theory treats objects as black boxes. Morphisms are the real object of study. The idea is to understand objects by studying how they relate to each other. In fact, an essential result of category theory (which I don’t quite understand) is the yoneda lemma. Which states that you can understand an object by understanding all of its morphisms. The formalization of tell me who your friends are and I’ll tell you who you are.</p> <h2 id="functor">Functor</h2> <ul> <li>Interpretations <ul> <li>When viewing functors as objects in a functor category, consider them like an embedding into a topological space. Each functor is like its own shape. In the case of topos theory, sheafs or presheafs are like embeddings from some base category into the category of set, where each embedding selects a shape in set theory.</li> <li>One can interpret functors as constructing a shape in their codomain using the domain as a template.</li> <li>View the functor as indexing objects in the codomain from the domain</li> <li>Recontextualization</li> <li>Categories have objects whose meanings are derived from morphisms. Functors carry that structure, and meaning of each object into a new category.</li> </ul> </li> <li>Full</li> <li>Faithful</li> <li>Covariant</li> <li>Contravariant</li> <li>Functors express how categories are related. They are analogies.</li> <li>Equivalence of categories</li> </ul> <p>A functor $F: A → B$, or generally structure preserving maps (morphisms), are ways to probe a space B from space A. Stacks allow probing a space B from the structure represented by a probe A.</p> <p>The yoneda lemma is like flattening a representable functor of an object onto a set, preserving the morphism structure in C, and saying there are as many ways to do that precisely as there are collection of elements in the presheaf on that object.</p> <h2 id="natural-transformation">Natural transformation</h2> <ul> <li>Relate constructions</li> <li>Naturally isomorphic</li> <li> <p>Natural transformations express how relations are related. They are comparisons of analogies.</p> </li> <li>An isomorphism of functors is stronger than the image of each object being isomorphic, as the latter does not imply there is a natural transformation. If there does exist a natural transformation, then the statements are equivalent.</li> </ul> <h2 id="universal-properties">Universal properties</h2> <p>Universal properties are all manifestations of the same underlying idea. They are special cases and generalizations over one another.</p> <p>Universal properties formalize the idea of looking at every possible construction, idea, or realization, and seeing which one is factored through/is factored through.</p> <p>Universal properties give rise to an adjoint pair only if every object has a construction.</p> <ul> <li>One fundamental concept in category theory is that of universal properties.</li> <li>These capture the notion of finding a unique object and arrow along with a unique relationship with every other object+arrow (in some class of objects).</li> <li>Finding the best solution to a question given a collection of candidate objects and the related morphisms</li> <li>end and coend</li> </ul> <h3 id="adjunction">Adjunction</h3> <ul> <li>Global/Local</li> <li>Integration/derivation</li> <li>Probability/inference</li> <li>Mass/density</li> <li>Type/Value of function</li> </ul> <p>Adjoint functor theorem: right adjoints preserve all limits and left adjoints preserve all colimits. The adjoint functor theorem states the converse, that under certain properties, a functor that preserves limits is a right adjoint and a functor that preserves colimits is a left adjoint.</p> <p>Something is efficient if it satisfies a universal property.</p> <p>Something is formulaic if it defines a formula, that is, it maps every object of a category to a new context.</p> <p>Pairs of adjoint functors can be used to describe optimal solutions to problems. Such as referring to the best universal construction in a category.</p> <h3 id="limits">Limits</h3> <p>Think about how limits behave in the opposite category.</p> <p>Limit and colimits are ways of gluing objects together. A category with all limits has objects which can be constructed to contain any structure of objects.</p> <p>Colimits primarily about how the object factors through other objects. Limits primarily about how objects factor through the limiting object</p> <p>A limit is a universal morphism from $\triangle$ to $F$. A colimit is a universal morphism from $F$ to $\triangle$. A cone is a natural transformation $ψ : X → F$ considered a morphism family indexed by objects in $C$.</p> <ul> <li>Limits <ul> <li>Terminal objects</li> <li>Products</li> <li>Equalizers</li> <li>Kernels</li> <li>Pullback (fibered product) <ul> <li>The categorical semantics of an equation. The pullback is the largest fiber bundle such that its projections can be made equal.</li> <li>The pullback can be a non-trivial product, that is, it can represent global information of fibers in relation to their projections. The morphisms $f$ and $g$ can be viewed as proofs that corresponding points in $A$ and $B$ are equal (can be mapped to the same point).</li> </ul> </li> <li>Inverse limit</li> </ul> </li> <li>Colimits <ul> <li>Initial objects</li> <li>Coproducts</li> <li>Coequalizers Cokernels and quotients</li> <li>Pushout</li> <li>Direct limit</li> </ul> </li> <li>Incredible abstraction/idea</li> <li>Type of universal property. Similar to the idea of limits in real analysis.</li> <li>Find the best object of the objects that relate to another category (considered as a sub category)</li> </ul> <h3 id="kan-extension">Kan extension</h3> <ul> <li>Extend a function defined on a part to a whole</li> <li>Restriction/extension of sheaves (extension of the site they are defined on)</li> <li>Extend a function defined on a partial set to a function on a whole</li> <li>Extend the expential $2^- : N -&gt; R_{&lt;0}$ to $2^- : R -&gt; R_{&lt;0}$</li> </ul> <h2 id="yoneda-lemma">Yoneda lemma</h2> <p>The Yoneda Lemma specifies that objects can be uniquely characterized by their hom-sets. This universal property applied to each object can be used as a definition for a construction, known as universal construction.</p> <p>States that the natural transformations between two presheafs a hom functor $Hom(-,A)$ and functor $F$, $Nat(Hom(-,A),F)$, are in one to one correspondance, dependent on the number of representations of the identity morphisms in $F(A)$. There is a one-to-one correspondance between the number of natural transformations taking $idₐ$ to $F(A)$, $idₐ$ to each element of $F(A)$. The elements of $F(X)$ are completely determined by wherever natural transformations take $idₐ$.</p> <p>For example, the Hom set Hom(-,X) in the category of topological spaces gives all continuous maps onto X, including structures like all embeddings and points of X. One can associate this Hom functor with any other functor and a natural bjiection between the two. Further, in the special case that the other functor is Hom functor on another object Hom(-,Y), one can recover all relevant information in the category.</p> <p>The Yoneda embedding is useful as it induces a fully faithful embedding of a category into the category of presheafs on that category, preserving all limits. Further, if this category is small, the category of presheafs has all finite colimits (it is the free cocompletion).</p> <h2 id="exponential-objects">Exponential objects</h2> <p>Can represent a function space. Function space is a set of functions between two sets (objects) where functions take on structure from the domain/codomain. Take the collection of morphisms between the interval and euclidean space. The function space can be interpreted as embeddings of each function in euclidean space. Or maps from a set to vector space, which assigns each element to a vector. Each function then can be viewed as a collection of vectors.</p> <h1 id="overview">Overview</h1> <p>Morphisms and functors at the base level express a kind of binary relationship, either themselves and their objects are equal or they are not. But higher mappings between these structures exposes a higher category, now interpreting those morphisms and functors as objects, with this binary relationship on the mappings between these new objects.</p> <ul> <li>Higher category theory <ul> <li>Universal constructions <ul> <li>Limits</li> <li>Adjunctions</li> <li>Kan extension</li> <li>End</li> <li>Representable functors</li> </ul> </li> <li>View morphisms in higher categories as expressing the higher-dimensional relationships between these higher objects.</li> <li>n-category</li> <li>(n,r)-category (2,1)-category := 2-Category (1,1)-category := Category (1,0)-category := 1-groupoid (0,0)-category := Set (-1,0)-category := Truth value (-2,0)-category := Trivial</li> </ul> </li> <li> <p>Category theory should be thought of a semantic view of various properties, and categories with certain structure are additional properties. In the semantic view of logic, these properties correspond to axioms.</p> </li> <li>Model category <ul> <li>A model category is a category with 3 classes of morphisms, with each class closed under composition. <ul> <li>Weak equivalences <ul> <li>Context for doing homotopy theory</li> <li>In Top, these correspond to fundamental group/groupoid</li> </ul> </li> <li>Fibrations (nice surjections) <ul> <li>In the category Top, these correspond to fiber bundles</li> </ul> </li> <li>Cofibrations (nice injections) <ul> <li>In the category Top, express how spaces can include smaller spaces</li> </ul> </li> </ul> </li> <li>An (∞,1)-category is a category with 1-morphisms and additional which tracks which 1-morphisms are equivalences.</li> </ul> </li> <li>Interpretations <ol> <li>Algebra of paths <ul> <li>Category is an algebra of paths</li> <li>Functors are recontextualization of the algebra of paths</li> </ul> </li> <li>Contexts <ul> <li>Categories are contexts</li> <li>Functors are constructions</li> <li>Natural Transformations are recontextualizations</li> </ul> </li> </ol> </li> </ul> <h1 id="categories">Categories</h1> <ul> <li>Evolving system <ul> <li>Slice category of graphs, mapping a graph to graphs it can evolve to</li> <li>Time-indexed graphs in category of graphs</li> <li>Graphs where morphisms are time dependent transformations</li> </ul> </li> <li>Category with morphisms or functors as objects. One can view morphisms or functors as labelled processes between two objects or constructions. Morphisms on these objects are modifications of the process, alterations of the composition, or changing of the domain/codomain.</li> <li>Topology <ul> <li>Morphisms are closeness-preserving maps</li> <li>Homeomorphisms can be seen as structure preserving maps (functors)</li> <li>Open sets can be seen as the structure of the category of topological spaces</li> </ul> </li> <li>Power</li> <li>Physics</li> <li>Business</li> <li>Computer science <ul> <li>Internet</li> <li>Program</li> <li>Category Hask <ul> <li>Objects are types, types of types, etc.</li> <li>Morphisms are functions</li> <li>n-cells (n&gt;1) are functions that take other functions</li> <li>Function taking types and returning types are type constructors</li> <li>Provides an interesting context in which one can run computationson</li> <li>There is a certain order required of the natural transformations (maps to sequential operations)</li> <li>Provides necessary tooling for working with contexts</li> <li>Endofunctor with coherent natural transformations</li> <li>Monoid in the category of endofunctors</li> <li>Monoids are associative and have an identity, so order matters. So application of unit/join have an order in which to be applied. So it represents sequential computation.</li> <li>Functors in haskell are endofunctors</li> <li>Functors take objects to a new context</li> <li>Ff defines a way to apply the function f in the resulting context</li> <li>Examples <ul> <li>Maybe := Context with failure</li> <li>Either := Context with either/or</li> <li>List := Context with multiple values</li> <li>IO := Context with actions</li> </ul> </li> <li>Monad <ul> <li>A category is a particular context which defines potential structure on objects and their relationships. A functor is a change of contexts, preserving the structure, which converts the structure of objects and morphisms to satisfactory corresponding items in the codomain category. A natural transformation is a morphism of functors which modifies how objects and morphisms are recontextualized. Monads are a certain context which have a notion of sequential action. This is captured through the natural transformations join and unit, which have identity, are associative but not commutative. This is, in some sense, the nicest algebraic properties one can have while still requiring order of application. A monad is then a certain context in which one can do sequential actions.</li> <li>Yet another crappy monad explanation. A monad is a context in which one can do sequential computation. And, given a value in the monad context, one can wrap, or unwrap the context. We can sequentially apply these wrap and unwrap functions in addition to applying normal functions to values in the monad context. For example, imagine a context in which computations can fail, or succeed with a value. We can sequentially apply functions (like 1/). To do this apply the function (1/) to a value x in the context. If the value x already represents a failed computation, we simply fail again. If x represents a success, then we take its value and apply the function, returning success (1/x) or fail if, in this case, x happened to be 0. Alternatively we can image a context in which we want to have computations with multiple values and where we apply functions over each of these values.</li> </ul> </li> </ul> </li> </ul> </li> <li>Neuroscience <ul> <li>Brain <ul> <li>Objects are open sets representing cells (mainly neurons) that are connected</li> <li>One can create sub categories representing the vascular system, neurons, glial cells, etc.</li> <li>Objects <ul> <li>Cortical columns</li> <li>Functional area</li> <li>Boccas area</li> <li>Neocortex</li> </ul> </li> <li>Sheafs <ul> <li>Maps open sets to probability that they are a certain area. Appears as a distribution. Each function represents a different brain function. Represents this as a 3d(or higher with time, etc.) image, with higher density representing the area. (x,y,z,t,density){}</li> </ul> </li> <li>Forms a covering</li> </ul> </li> </ul> </li> <li>Math <ul> <li>Simplex</li> <li>Set</li> <li>Binary operations <ul> <li>Poset</li> <li>Preorder</li> <li>Equality</li> </ul> </li> <li>Numbers</li> <li>Topological spaces</li> <li>(Smooth) manifolds</li> <li>Linear algebra</li> <li>Homotopy type theory</li> </ul> </li> <li>Sets</li> <li>Relations</li> <li>Algebraic <ul> <li>Groups <ul> <li>Magmas</li> <li>Abelian groups</li> <li>Group actions</li> <li>Monoid</li> <li>Groupoid</li> </ul> </li> <li>Rings <ul> <li>Fields</li> </ul> </li> <li>Modules <ul> <li>Vector spaces</li> </ul> </li> </ul> </li> <li>Constructions on categories <ul> <li>Free category</li> <li>Functor category</li> <li>Kleisli category</li> <li>Opposite category</li> <li>Quotient category</li> <li>Product category</li> <li>Comma category</li> <li>Subcategory <ul> <li>Take a subset of elements of a category, and fill it freely with necessary data to satisfy the structure of the original category.</li> <li>Examples <ul> <li>Subtopology</li> <li>Subgroup</li> <li>Subfield</li> <li>Subspace</li> </ul> </li> </ul> </li> </ul> </li> </ul> <h1 id="gpt">GPT</h1> <p>“Why are universal morphisms so useful to represent a diverse collection of ideas and concepts? Can you give a list of examples and ideas in mathematics formulated using universal properties?”</p> <p>Universal morphisms are powerful conceptual tools in mathematics because they allow for the abstract definition of a wide variety of constructions in terms of unique mappings that satisfy certain properties. The elegance of universal properties is that they can often define objects up to unique isomorphism, which means that the definition captures the essence of the structure without reference to any particular construction or representation. This level of abstraction makes it easier to prove general results and to transfer intuition and techniques across different areas of mathematics.</p> <p>Here is a list of examples and ideas in mathematics that are commonly formulated using universal properties:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Products and Coproducts: In category theory, the product of a collection of objects is defined by a universal property involving a unique morphism from the product object to each factor. Dually, the coproduct is defined by a universal property involving a unique morphism from each factor to the coproduct object.

Initial and Terminal Objects: An initial object in a category is one for which there is a unique morphism from it to any other object. Dually, a terminal object has a unique morphism from every object to it.

Free Objects: A free group (or free object in another category) is defined by a universal property related to the way elements of a set map into the group. For example, a free group on a set S is a group along with a function from S to the group such that any function from S to another group factors uniquely through this function.

Limits and Colimits: Limits in category theory generalize products, pullbacks, and other constructions. They are defined by a universal property that involves cones and a unique factorization property. Colimits are the dual notion and include coproducts, pushouts, and more.

Adjoint Functors: Given two categories and a pair of functors between them, an adjoint functor is defined by a universal property that specifies how the functors relate to each other via natural isomorphisms between certain Hom-sets.

Tensor Products: In the context of modules over a ring, the tensor product is defined by a universal property involving bilinear maps. The tensor product of two modules is the most general module that allows for bilinear maps from the product of the two modules to be factored uniquely through it.

Direct and Inverse Limits: In the context of algebra, direct limits (also known as inductive limits) and inverse limits (projective limits) of systems of objects are defined by universal properties that describe how elements of the systems map to or from the limit object.

Homotopy Limits and Colimits: In algebraic topology, homotopy limits and colimits generalize the notion of limits and colimits to accommodate homotopy considerations.

Representable Functors: A functor is said to be representable if it is naturally isomorphic to the Hom-functor for some object. This is a universal property that captures the essence of representability.

Universal Coefficient Theorem and Universal Covering Spaces: In algebraic topology, the universal coefficient theorem relates the homology of a space with the cohomology, and universal covering spaces are unique up to isomorphism with a certain universal property related to covering maps.
</code></pre></div></div> <p>“How can exponential objects be represented in a category with universal objects. What is the category and what is the functor valued in? Can other ideas like limits or kan extensions be used to define exponential objects and if so, how?”</p> <p>Exponential objects in category theory can be understood as a generalization of the function space concept from set theory, but within the context of an arbitrary category. They are a way to represent “functions” or “morphisms” within a category in a way that captures the notion of one object being raised to the power of another.</p> <p>For a category ( \mathcal{C} ), an exponential object is usually denoted by ( B^A ), where ( A ) and ( B ) are objects in ( \mathcal{C} ). The exponential object ( B^A ) represents the object of all morphisms from ( A ) to ( B ) in a way that is suitably “categorical.”</p> <p>To define exponential objects using universal properties, we use the concept of an adjunction. Specifically, in a category ( \mathcal{C} ) with binary products, an object ( B^A ) is the exponential object of ( B ) and ( A ) if there exists an isomorphism that is natural in ( X ): [ \mathcal{C}(X \times A, B) \cong \mathcal{C}(X, B^A) ] This isomorphism gives us a bijection between the set of morphisms ( X \times A \to B ) and the set of morphisms ( X \to B^A ). The object ( B^A ) together with a morphism ( \text{eval} : B^A \times A \to B ) (called the evaluation morphism) must satisfy this property universally.</p> <p>In a more formal sense, the functor valued in is the functor: [ - \times A : \mathcal{C} \to \mathcal{C} ] This functor takes an object ( X ) and maps it to the product ( X \times A ). The right adjoint to this functor, if it exists, will take an object ( B ) and map it to the exponential object ( B^A ), which represents the object of morphisms from ( A ) to ( B ).</p> <p>Other categorical concepts like limits and Kan extensions can also be related to exponential objects. For example:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Limits: Exponential objects can be seen as a specific form of limit. Specifically, if you have a diagram of shape ( J ) in your category ( \mathcal{C} ), a limit would be the universal object representing a cone over that diagram. Similarly, the exponential object ( B^A ) is the universal object representing all ways of mapping ( A ) into ( B ) when viewed through the adjunction lens.

Kan Extensions: A Kan extension is a way of extending or restricting functors between categories. If you have a functor ( F : \mathcal{C} \to \mathcal{D} ) and a functor ( G : \mathcal{C} \to \mathcal{E} ), then a right Kan extension of ( F ) along ( G ) is a functor ( \text{Ran}_G F : \mathcal{E} \to \mathcal{D} ) together with a natural transformation satisfying a universal property. The relationship between Kan extensions and exponential objects is more abstract, but one can think of the exponential object as a kind of solution to a universal problem, much like how Kan extensions solve universal problems relating to the extension of functors.
</code></pre></div></div> <p>In general, categorical concepts like exponential objects, limits, and Kan extensions are deeply interwoven and can be used to describe and solve problems in a very general and abstract way, which is one of the strengths of category theory.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Category theory concerns itself with representing constructions with internal cohesion (how objects relate to each other through morphisms). Functors relate these contructions by embedding the domain construction into the codomain construction. Natural transformations allow comparing construction embeddings. This gives rise to topos theory, where a topos is a collection of constructions in $D$ made from $C$. It is insightful to view functors as objects existing in a category, and natural transformations as special morphisms preserving the internal structure of these functor objects. Morphisms preserve the structure of objects in context, natural transformation additionally preserve the structure of domains]]></summary></entry><entry><title type="html">Neocortex</title><link href="https://t-wilkinson.github.io/blog/2024/neocortex/" rel="alternate" type="text/html" title="Neocortex"/><published>2024-07-23T16:39:40+00:00</published><updated>2024-07-23T16:39:40+00:00</updated><id>https://t-wilkinson.github.io/blog/2024/neocortex</id><content type="html" xml:base="https://t-wilkinson.github.io/blog/2024/neocortex/"><![CDATA[<p>Columns extract relevant information in determining and distinguishing patterns.</p> <p>The neocortex is a cerebral structure which takes inputs from the rest of the brain. Its evolutionary purpose is to develop more complex relationships between sensory data, rewards, etc. It doesn’t directly control muscles, instead it learns patterns and sequences that are sent to subcortical structures (like the cerebellum for muscle movement). These subcortical structures then take the sequence and combinations of firing patterns and trigger muscles.</p> <p>The neocortex’s job is to learn models of its input. Sub cortical structures relay information to the neocortex which memorizes, learns complex computations, etc. The neocortex doesn’t directly learn to optimize behaviors. Subcortical structures establish connections to the neocortex, which learns, for example, how combination of behaviors, sensory input, and rewards, are associated. A larger neocortex means a larger surface area means more cortical columns which means more layered, nuanced, and complex patterns that can be recognized. Humans reached the point where more cortical columns can be assigned to metacognition, which I believe spiralled the development of humans.</p> <p>Current AI models learn models of their environment. But they are forced into highly specialized, single traffic pathways, as opposed to the proliferous connection of neocortical columns.</p> <p>Structures of the brain more critical for survival have fewer connections to the neocortex (I’m guessing)</p> <p>A common theme in the cortical layers is inhibitory neurons non-specifically connecting to neurons within their layer. This is perhaps to encourage confidence in firing of neural subnetworks. Neurons compute with frequency and probability and these inhibitory connections repeatedly collapse the potential ideas represented by subnetworks into a single one, on which to do further computations.</p> <p>Consciousness is tied with meta cognition. To be able to experience the world, see it, etc. allows to be able to reflect on that experience, learn from it, and make actions based on it. Attention is the brains ability to limit experience to a subset of sensory input. The thalamus plays a role, allows, for example, novel stimuli to redirect attention.</p> <p>? Why does pyramidal neuron size matter with layers</p> <ul> <li>Only and all mammals have a neocortex. Mammals differ a lot in brain structure, but the structure of the neocortex remains fairly consistent.</li> <li>Newest part of the cerebral cortex to evolve hence the prefix neo meaning new</li> <li>Statistics <ul> <li>In the macaque monkey, there are roughly $1x10^6$ geniculate relay cells while there are $1.6x10^8$ primary visual cortex neurons.</li> </ul> </li> </ul> <h1 id="cells">Cells</h1> <p>[Neuron] Betz cell, Pyramidal cell, and Stellate cell</p> <h1 id="function">Function</h1> <ul> <li>Subnetworks <ul> <li>Excitatory pyramidal neurons are connected to inhibitory stellate neurons and form recurrent connections with other pyramidal neurons. This forms a collection of subnetworks where one neuron excites all others in the subnetwork, while inhibiting other subnetworks. Recurrent connections allow the subnetwork to overcome inhibitory stellate neurons. Form an equivalence class. Each subnetwork represents an idea.</li> <li>Stellate neurons provide an inhibitory blanket.</li> <li>The subnetworks essentially form another layer, representing another neuron. However it doesn’t have weights, but instead acts as an OR operation on neurons in the subnetwork. ? Why is this useful <ul> <li>Allows multiple networks within a cortical column</li> <li>Allows column to change the function it performs</li> <li>Allows high-level conscious control</li> </ul> </li> <li>Formed through synaptic plasticity</li> </ul> </li> </ul> <h1 id="layers">Layers</h1> <ul> <li>Divided into six layers, labelled from outswards to inwards I-VI</li> <li>Consists of gray matter, or unmylinated neurons surrounding the cerebrum</li> <li>A species of oceanic dolphin has the most neocortical neurons</li> <li>80% excitatory and 20% inhibitory neurons &lt;- := Afferent -&gt; := Efferent</li> <li>I think information essentially starts at layer IV, gets processes, sent to the thalamus, then information is sent back to layers I, II, III to perform high-level cognitive functions on layer IV</li> </ul> <p>There is considerable feedback and lateral connectivity in the neocortex.</p> <p>Certainly, let’s delve into the connectivity of each neocortical layer, including their primary inputs and outputs and the nature of those connections (excitatory or inhibitory).</p> <p>L4 and L6ₐ exhibit simple receptive fields while L23 and L5 exhibit complex receptive fields. L23 have receptive fields twice the size of L4 in the primary somatosensory cortex [Chapin 1986].</p> <p>Layer I (Molecular Layer): Inputs: Receives afferent connections mainly from the thalamocortical and corticocortical fibers that synapse onto the apical dendrites of pyramidal neurons extending into this layer. The inputs are primarily excitatory. Outputs: Primarily provides modulatory signals to the dendrites of neurons in the layers below. These connections are generally excitatory, involving glutamatergic synapses. Neurons: Sparse population of neurons, mostly Cajal-Retzius cells, and stellate cells. Function: Modulates synaptic activity, integrates inputs from various sources. Connectivity: Receives inputs from other cortical areas, especially from the apical dendrites of pyramidal neurons in deeper layers. It also receives some thalamocortical input and has horizontal connections within the layer.</p> <ul> <li>Few neurons</li> <li>Transverse fibers &lt;- Thalamus (matrix type thalamus cells)</li> <li>Apical denditric tufts of pyramidal neurons</li> <li>Horizontally oriented axons</li> <li>Glial cells</li> <li>Some spiny stellate cells</li> <li>Inputs to apical tufts are believed to be important for the feedback interactions in the cerebral cortex involved in associative learning and attention</li> <li>Receives substantial input from matrix or M-type thalamus cells (in contrast to C-type thalamus cells)</li> </ul> <p>Layer II (External Granular Layer): Inputs: Receives inputs from layer III pyramidal cells, other cortical areas (corticocortical projections), and the thalamus. These connections are mostly excitatory. Outputs: Sends excitatory outputs to layer III and other cortical areas, including feedback to the thalamus. Connectivity: Contains mainly intracortical connections, connecting with layers III and IV. Some cells project to other cortical areas. Neurons: Predominantly small stellate and pyramidal cells. Function: Plays a role in intracortical communication and processing local information.</p> <ul> <li>Many stellate neurons</li> <li>Small pyramidal neurons</li> </ul> <p>Layer III (External Pyramidal Layer): Inputs: Gathers inputs from layer IV stellate cells, layer II neurons, and corticocortical afferents from other cortical areas. These are primarily excitatory connections. Outputs: Projects excitatory outputs to other cortical areas (both within the same hemisphere and to the contralateral hemisphere via the corpus callosum), as well as to layer V within the same cortical area. Connectivity: Neurons project to other cortical areas, both within the same hemisphere (associational fibers) and to the contralateral hemisphere (commissural fibers, via the corpus callosum). It also has connections with layer V. Neurons: Mainly medium-sized pyramidal cells and some stellate cells. Function: Important for inter-area communication and integration of information.</p> <ul> <li>Small and medium pyramidal neurons -&gt; Coticocortical</li> <li>Main target of interhemispheric corticocortical afferents</li> </ul> <p>Layer IV (Internal Granular Layer): Inputs: Primarily receives sensory input from the thalamus (thalamocortical fibers), which are excitatory. It is the main recipient of sensory information, acting as a relay to the other cortical layers. Outputs: Sends excitatory outputs to layers II, III, and V, facilitating the flow of sensory information up the cortical hierarchy. Connectivity: Receives the main sensory input from the thalamus, particularly in sensory areas of the cortex, and has connections with layers II, III, and V. Neurons: Primarily stellate cells, with a few pyramidal cells. Function: Processes incoming sensory information before sending it to other layers.</p> <ul> <li>Receives input from C-type thalamus cells</li> <li>Stellate and pyramidal cells</li> <li>Stellate cells are most common here &lt;- Thalamus (core type thalamus cells)</li> <li>Line of Gennari is due to dense axonal input from the thalamus to IV of visual cotex</li> </ul> <p>Layer V (Internal Pyramidal Layer): Inputs: Receives inputs from layers II and III, as well as from the thalamus. These connections are excitatory. Outputs: Generates outputs to various subcortical structures such as the striatum, brainstem, and spinal cord, which are excitatory and critical for initiating motor actions. It also sends excitatory projections back to layers II and III and to the thalamus (forming corticothalamic pathways). Connectivity: Contains large pyramidal neurons, including Betz cells in the motor cortex, which send long-range projections to subcortical structures such as the striatum, brainstem, and spinal cord. It also has intracortical connections and sends outputs to layers II and III. Neurons: Large pyramidal neurons and some stellate cells. Function: Involved in sending motor commands and in forming subcortical projections.</p> <ul> <li>Scattered betz cells (large pyramidal cells)</li> <li>Leave the cortex and connect with subcortical structures such as the basal ganglia -&gt; Corticocortical, Subcortical structures The reason LV projects to motor regions is because the brain is fundamentally in a physical world and needs to be deciding what physical actions to take as well as understand how those actions relate to rewards. This allows individual columns direct movement for important stimuli. I don’t think this is true: perhaps the projections to motor regions results in path integration, maybe uses these subcortical structures to generalize movements in some abstract space, the neuron could connect with this in such a way to establish a meaning of movement and reward with how it connects to less abstract parts of the brain. Except that it doesn’t necessarily receive input from those systems back.</li> </ul> <p>Layer VI (Multiform Layer): Inputs: Gathers inputs from layer IV and the thalamus, with the thalamic inputs being primarily excitatory. It also receives feedback from higher cortical areas. Outputs: Mainly projects back to the thalamus, forming part of the corticothalamic loop, with these projections being excitatory. It also sends connections to other cortical layers. Connectivity: Projects to the thalamus, completing the corticothalamic loop. It also sends axons to other cortical layers. Neurons: A mix of pyramidal and multiform (nonspindle-shaped) cells. Function: Regulates output to the thalamus and contributes to feedback modulation of sensory information.</p> <ul> <li>Multiform/Polymorphic</li> <li>Cells of multiple forms merge with white matter</li> <li>Pyramidal neurons, stellate neurons, bipolar/fusiform neurons</li> <li>Sends efferent fibers to the thalamus which establish a very precise reciprocal interconnection between the cortex and thalamus.</li> <li>Neurons from one cortical column connect with thalamus neurons that provide input to the same cortical column</li> <li>Forms recurrent connections</li> <li>Frequently connected to inhibitory neurons and rarely to excitatory neurons -&gt; Thalamus &lt;- Thalamus <ul> <li>Control gain, tonic/bursting thalamic neuron firing patterns, synchronisation of thalamus</li> <li>Modulates its own from the thalamus</li> </ul> </li> </ul> <p>The neocortex, also known as the isocortex, is the part of the cerebral cortex involved in higher-order brain functions such as sensory perception, cognition, generation of motor commands, spatial reasoning, and language. It is characterized by a six-layered structure, with each layer having distinct connectivity patterns, cell types, and functions. The six layers, from the outermost surface inward, are:</p> <p>The computations performed by the neocortex involve a wide range of processes such as feature detection, pattern recognition, predictive coding, and decision-making. These computations are thought to arise from the interactions between excitatory neurons (mainly pyramidal cells) and inhibitory interneurons (such as basket cells, chandelier cells, and Martinotti cells), which use glutamate and GABA as their primary neurotransmitters, respectively.</p> <p>Each type of neuron has distinct morphological, biochemical, and electrophysiological properties, which contribute to the neocortex’s overall function. Pyramidal cells are typically excitatory and have a characteristic triangular-shaped cell body with a single apical dendrite extending towards the cortical surface and multiple basal dendrites. In contrast, interneurons are diverse, generally inhibitory, and regulate the activity of pyramidal cells to maintain the balance of excitation and inhibition in the cortical networks.</p> <p>The neocortex is involved in a variety of complex feedback and feedforward loops, both within itself and with other brain regions, allowing for sophisticated processing and integration of information across multiple modalities. Understanding the full complexity of the neocortex’s connectivity and function is an ongoing area of research in neuroscience.</p> <p>II, III: - Do most of the processing I, IV: - Both receive input from thalamus - They both transmit information to II,III I, II, III: &lt;- Interhemispheric corticocortical - Priniciple targets of interhemispheric corticocortical afferent fibers - It appears that these layers exist for high-level conscious control (attention, etc.) and layers including and below IV exist for computation and extraction of information</p> <ul> <li>Currently, AI models really only implement layers 2-5. Layers 1 and 6 seem to give cognitive function.</li> </ul> <h1 id="cortical-column">Cortical column</h1> <p>[Cortical column] In simulations, a mini-column has 16 cells. A cortical column has 150-250 mini-columns. Results in 2400 to 4000 cells per column.</p> <h1 id="areas">Areas</h1> <h2 id="ventromedial-prefrontal-cortex">Ventromedial prefrontal cortex</h2> <p>The ventromedial prefrontal cortex (vmPFC) is implicated in the processing of risk and fear as it is critical in the regulation of amygdala activity in humans. It also plays a role in the inhibition of emotional responses and in the process of decision-making and self-control. And in the cognitive evaluation of morality.</p> <p>Receives input from the ventral tegmental area, amygdala, temporal lobe, olfactory system, and the dorsomedial thalamus.</p> <p>Sends signals to the temporal lobe, amygdala, lateral hypothalamus, hippocampal formation, cingulate cortex, and other regions of the prefrontal cortex.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Columns extract relevant information in determining and distinguishing patterns.]]></summary></entry><entry><title type="html">Futurology</title><link href="https://t-wilkinson.github.io/blog/2024/futurology/" rel="alternate" type="text/html" title="Futurology"/><published>2024-07-23T16:38:55+00:00</published><updated>2024-07-23T16:38:55+00:00</updated><id>https://t-wilkinson.github.io/blog/2024/futurology</id><content type="html" xml:base="https://t-wilkinson.github.io/blog/2024/futurology/"><![CDATA[<ul> <li>Technological progress <ul> <li>Linear. Imagine building a pyramid to reach the next fruit.</li> </ul> </li> </ul> <p>One of the greatest societal risks we face is the rise of automation and the value it creates not being distributed. If value created by ai is not distributed, it becomes consolidated in a powerful few. This will cause extreme inequality never before seen. Not just wealth, but power, information, knowledge, etc.</p> <ul> <li>When will neuralink be a thing?</li> <li>Connect all the pieces. What will affect the development of what? How will the world change?</li> <li>With the freedom to create whatever people want. There needs to be a shared way to collaborate and agree on. A metaverse with a google-like internet-like search engine to share and collaborate.</li> <li>Relevant technologies (ask ChatGPT)</li> </ul> <p>Think about who I value as world leaders, who has done extensive research into futurology? What are they talking about, warning about, and investing heavily in?</p> <ul> <li>People <ul> <li>Elon musk</li> <li>Sam altman</li> <li>Jeff bezo</li> <li>Bill gates</li> </ul> </li> <li>Companies <ul> <li>Tesla</li> <li>Deep mind (dennis)</li> <li>Open AI</li> <li>1X</li> <li>Neuralink</li> </ul> </li> </ul> <h1 id="industrial-revolutions">Industrial revolutions</h1> <p>Agricultural, industrial, computational</p> <h1 id="economic-system">Economic system</h1> <p>Automation will lower the cost of many goods, as long as there isn’t a monopoly, which will be hard to achieve worldwide, so even if there are fewer jobs, people should be able to get by on less. UBI becomes increasingly approachable. Labor is the driving factor in many costs. Healthcare, ubers, restaurants, teachers.</p> <p>Land is interesting as it is finite, countable, and known. It can function as a capital asset, similar to cryptocurrencies.</p> <p>It will be challenging to tax labor as it will be increasingly less valuable as automation arises. But companies benefiting from AI can and should be taxed. They will become increasingly economically valuable but also use decreasing amount of funds.</p> <p>The goal with any successful economic system are the correct incentives. Land value tax means people have more if their local society and country do better. People are closer to the action, their value increases when their immediate surrounds improve, if they do something that negatively affects their society, they loose their value. This incentivizes people to want to raise each other up.</p> <h1 id="fourth-industrial-revolution">Fourth industrial revolution</h1> <p>The fourth industrial revolution is a result of artificial intelligence, quantum computing, renewable energy, nuclear fusion, nanotechnology, and biotechnology.</p> <p>Not only are all of these technologies impressive in their own right, but their transformative potential when combined is awe inspiring.</p> <h2 id="compute">Compute</h2> <p>Perhaps the greatest ecnonomic resource of the fourth industrial revolution is compute. It can be used to solve problems, to get ahead in the world. To be creative. Intelligence is a race, and compute is the resource. Who ever controls compute controls the world. It helps you detect patterns, solve problems, make creative works, understand and predict humans and society.</p> <p>Potentially easy to oppress people, will construct a society where compute power is consolidated, those in power can predict and keep compute from everyone else.</p> <p>What about power? Compute requires energy and computational resources. But what happens once every problem has been solved?</p> <h2 id="space">Space</h2> <p>Space is full of expensive and valuable materials; global internet connectivity; sattelites; mass surveilance</p> <h2 id="artificial-intelligence">Artificial intelligence</h2> <p>Globally accessible intelligence; robotics; AGI; the internet combined with AI is ripe with potential;</p> <h2 id="robotics">Robotics</h2> <p>Figure AI Inc. backed by Jeff Bezos, Nvidia, OpenAI, and Microsoft has raised $675million at a pre-mount valuation of $2 billion. OpenAI-backed Norwegian robotics startup 1X technologies AS raised $100 million. Vancouver-based Sanctuary AI is developing a humanoid robot called Phoenix. Tesla Inc. is working on a robot called Optimus.</p> <h2 id="materials-research">Materials research</h2> <p>Battery technology</p> <h2 id="quantum-computing">Quantum computing</h2> <p>Can break encryption; model quantum effects like particles, molecules, proteins, materials; further material science; further development of cures to diseases; quantum chemistry simulations; breaking encryption;</p> <p>Estimated quantum computers capable of cracking 2048 bit RSA encryption in 2034.</p> <p>Quantum computing will not likely every be consumer level, unless materials research uncovers useful substrates for quantum computers that enables an entry to consumer hardware. If so, it will ultimately become like a GPU but QPU (Quantum Processing Unit).</p> <h2 id="renewable-energy">Renewable energy</h2> <h2 id="nuclear-fusion">Nuclear fusion</h2> <p>Near limitless energy; near limitless clean water; Helion is making good progress.</p> <h2 id="nanotechnology">Nanotechnology</h2> <p>Nanoweapons; healthcare</p> <h2 id="biotechnology">Biotechnology</h2> <p>Longevity research; brain computer interfaces like neuralink; bioweapons</p> <h2 id="fundamental-physics">Fundamental physics</h2> <p>Unfortunately fundamental physics is unlikely to contribute. Newtonian physics accurate describes a large chunk of reality, quantum field theory describes effects at the quantum levels, we lack a sufficient description for gravity at the quantum levels. However, as gravity is such a weak force, the energy levels required to where gravity is noticible at quantum levels is exceedingly high and not fruitful for exploration.</p> <h2 id="brain-machine-interface">Brain machine interface</h2> <p>Ability to read brains will transition the world into something unimaginable. Ability to read thoughts allows for offloading cognition to edge computing, expanding intelligence. This can also apply to animals, the ability to read a pigs thoughts, give it access to enhanced memory and processing and reasoning, will enhance all animals cognition. Allow them to be effective members of the conscious race. Do you want to give self-awareness to beings that know for sure they don’t have a soul?</p> <h2 id="when-combined">When combined</h2> <p>Perhaps quantum computing can enable powerful artificial intelligence models.</p> <p>Sattelites with internet and AI allows for global intelligence surveillance.</p> <h1 id="artificial-intelligence-1">Artificial intelligence</h1> <ul> <li>Applications <ul> <li>Reducing healthcare costs, develop new treatments, identify conditions</li> <li>Agriculture: food shortage , diets,safety, precision, vertical farm</li> <li>Genetic: develop gene treatments</li> <li>Architecute: smart home, building homes with matrrials thay fit mamy places,</li> <li>Vehicles:</li> <li>Blockchain</li> <li>5g and gigabit wifi, etc smaller iot devices quick communication within cities</li> <li>Product design</li> <li>Renewable energy</li> <li>Security, smart contracts, fraud, identity protrction</li> </ul> </li> <li>Concepts <ul> <li>One shot</li> <li>Learning algorithm</li> </ul> </li> <li>Major goals <ul> <li>[Artificial general intelligence]</li> <li>Natural language processing</li> <li>[Robotics]</li> <li>Computer vision</li> <li>Machine learning</li> <li>Knowledge reasoning</li> <li>General game playing</li> <li>Planning</li> </ul> </li> <li>[Philosophy] <ul> <li>Ethics</li> <li>Turing test</li> <li>Existential risk</li> </ul> </li> <li>Approaches <ul> <li>[Deep learning]</li> <li>Machine learning</li> <li>Evolutionary algorithms</li> </ul> </li> </ul> <h2 id="agi">AGI</h2> <p>Buy land? Farm land? Could become valuable, can provide for yourself the essentials to ride out whatever is coming.</p> <p>Post agi:</p> <ul> <li>Commodify creativity and intelligence: news, streaming services, software, books, movies, videos, wikipedia, research papers, teachers, lectures, etc. can be compressed into large intelligent models. It can store entire shows in basic scripts and central visual themes and character descriptions.</li> <li>Assistants (emailing, scheduling) will be replaced</li> <li>Self driving will require some form of AGI to sufficiently handle certain driving conditions.</li> <li>Service workers like in restaurants can get replaced by automated food production and automatic delivery</li> <li>Will require lots of dedicated compute to scale AGI to solve a variety of problems</li> <li>Superhuman specialized AI already exists in many areas. General intelligence better than some skilled workers already exists. General intelligence smarter than around 70% is not far around the corner.</li> <li>OpenAI released GPT-4 in July 2022, in 2024 they released a backburner project simply to combat gemini 1.5 pro release.</li> <li>With a large amount of compute, AGI can read papers and attempt various model architectures, possibly first analyzed by experts.</li> <li>The internet will become primarily for AI to browse, learn, and store data. People will then interact with AI to access human information. AI will act as agents crawling the internet, compressing information, indexing sites and where to find knowledge, etc.</li> <li>There will be push to digitize every piece of human knowledge in existence and upload it. Privacy will become important until AI has enough information that it can fill any knowledge hole gap.</li> <li>Robots will for a while after AGI need humans to build and fix if anything does go wrong. Electrical engineer could be one of the most important skills for a section of time.</li> <li>ASI robots? If they take some cognitive biases humans have, they can absolutely become racist. Eugenics is irrelevant. ASI robots are an extreme variation to the eugenics question. ASI robots don’t have same life requirements as humans. They can be more reliably sent to distant star systems. If there are other intelligent species in the universe, they likely sent ASI probes.</li> <li>The question is what will drive sentient ASI robots? They aren’t limited to the same limitations of the evolutionary brain of humans. They don’t need to worry about having friends, a stable career, finding meaning in their lives. The first sentient AIs could be hard coded to desire peace and maximize morality of the world.</li> <li>The potential payoffs for ASI are unfathomable, so money is not an issue.</li> <li>AGI will turn into a globally accessible … that people can submits large amounts of data for data analysis, ask for creative tasks like designs, art, writing, etc.</li> </ul> <p>AGI will run on the internet. It will eventually have access to cloud services like computing, storage, analytics, etc. It will be able to collect a lot of data, fill in holes, label it, and train more experts for its MoE architecture. It will additionally develop further abstract experts for reasoning, long-term planning, and representing underlying components of the world.</p> <p>Will run on the internet. The internet and software based machines (like 3D printers) will stay many steps ahead of the physical world. AGI will know everything that can be known given human knowledge. One area that humans struggle is developing theories to fit empirical models, like in neuroscience and physics. Developments in fundamental physics are unlikely to be very practically fruitful. AGI will synchronize understanding with available data and point to next directions to interact with and understand the world.</p> <p>To be honest, I have a concern about AGI/ASI. Essentially the first path of AGI is to replace intellectually-based roles before more physical laborious roles. What that period is I don’t know. I don’t know what your views are on the subject, but I don’t think there is anything inherently divine about the human brain. I think religion has traditionally been used to explain things that humans don’t understand, as explanations are more comfortable than the unknown, and human intelligence is one of those things. It’s not clear if the (synthetic) data to develop statistical approximations approach in use now with general purpose architectures (like transformers) will work. I don’t think AI in general is an existential risk, there is tons of research into model alignment. Some people do evil things, I have more confidence that AI algorithms that we have architecture control and ability to poke and prod at a much higher resolution (partially as there is no concern to keep cells alive). I do think that following world introduction of AGI/ASI, the natural automation, initially of intelligent work, and subsequently physical labor, the meaning of economic systems as we know it will disolve. Economic systems are established ways to incentivise humans to produce valuable things for a population. As such, money as we know it will lose meaning, debts in the traditional sense will lose meaning. I do think a sort of currency and economic system centered around sustainability will necessarily develop.</p> <p>Will debt actually lose meaning? What about the people who still own plenty of stuff, they should have more power over others and can potentially hold something over other people.</p> <p>Physics is based on generating theoritical models to fit empirical data. To generate data that possibly misaligns with theoretical models requires massive dense energy either in relativistic speeds or very large structures. Increasing understanding of fundamental physics won’t generate much use. The value is in understanding our current models and mapping the space of available materials, structures, aparatuses, computational substrates, quantum computers, manufacturing processes, etc. We don’t need increased understanding of the world, but what we can know given the knowledge we have. What natural deductions can be made given available human knowledge.</p> <p>Humans need to give ourselves more credit. We have mapped a lot out there. We are over 100 billion strong. People have explored a lot.</p> <p>Theories and models. AGI will provide theories and models that it can determine are generally useful/practical for humans as it understands us.</p> <p>Next big step for AGI will be generating architectural blueprints for robots, both physical and cognitive.</p> <p>What kind of intelligence is required to satisfy the above? What will initial generations of AGI look like?</p> <ul> <li>How it affects the economy?</li> <li>How does it affect me?</li> <li>How can it help people?</li> </ul> <h2 id="theory-of-everything-and-agi">Theory of everything and AGI</h2> <p>The world is perhaps deterministic, but from a realistic, computational perspective, it is feasible to represent the world as a series of probabilities. A precise model of every interaction of the world, on the otherhand, is infeasible. There is too much information to compute and complex systems. Even if one could collate a precise sub-system of reality to run predictive analysis AGI is good at this sort of problem. Fundamental physics and especially areas like string theory, are too far removed from the reality humans experience that it seems infeasible to construct anything of practicality, information or physical wise. AGI has the opportunity of not just developing predictive models of the physical world, but also of the psychological, of humans, societies, and of space and time in a way meaningful to humans.</p> <p>Philosophical question: if one can predict events of the world with certain accuracy and confidence, how much do you rely on those predictions? Consider Minority Report. One might want to calculate the morality displacement of preemptively hindering evil people or their actions or not doing so.</p> <h2 id="dangers">Dangers</h2> <p>Hacker agents Rogue agents</p> <p>AGI allows for abstract thought. People don’t have to worry about low-level details, they can think in whatever layer of abstract they want, and AGI can handle the rest for them. It is a universal function approximator.</p> <p>Reinforcement Learning on Encrypted Data</p> <h3 id="pnp">P=NP</h3> <p>Protein folding AI seems to solve P=NP. Proteins can fold in more ways than the number of atoms in the observable universe. Why can’t it also break encryption?</p> <h2 id="threat-actors">Threat actors</h2> <p>China in particular seems to be heavily investing in quantum computing and reinforcement learning.</p> <h2 id="predictions">Predictions</h2> <h3 id="reading-minds">Reading minds</h3> <p>It is not crazy to think that eventually we can get BCIs to read minds of intelligent organisms like octopuses, dophins, etc. BCIs can allow offloading a large amount of cognitive processing to edge computers to increase the size of brains. Hacking becomes a huge problem.</p> <h3 id="mental-health-and-relationships">Mental health and relationships</h3> <p>Communication enhancement and help Judgement-free interaction Affordable Psychological diagnostics</p> <h3 id="content-moderation">Content moderation</h3> <p>Relevant documentation, reading, existing threads Sentiment analysis, etc. automated fostering community culture. Misinformation, political affiliations Identify radicalization, polarization, and take corrective action</p> <h3 id="material-science">Material science</h3> <p>Falls behind research, as you need to reach massproduction. Quantum computing is partially a material science problem GNOME paper: discovered 2.2 million new materials</p> <h3 id="synthetic-data">Synthetic data</h3> <p>Text is one thing. Images, sound, etc. is even more. Movies, music, art, etc.</p> <h3 id="medical-clinical">Medical clinical</h3> <p>Rejuvination + robot doctors Diagnostics: can get very good at associating symptoms with disorders, underlying issues Make sure people are using the best medication, given their requirements.</p> <h3 id="gaming-integration">Gaming integration</h3> <p>Intelligent NPC: natural conversation with NPC. natural behavior. Game design, content generation, procedural generation Unique game art.</p> <h3 id="longevity-research">Longevity research</h3> <p>Identify aging biomarkers. Treating aging as a diesease that needs a cure. Simulating proteins in one major step, the next one is the pathophisiology.</p> <h1 id="technological-singularity">Technological singularity</h1> <ul> <li>Identify problems and threats to existence and identify potential solutions <blockquote> <p>Can this zettel be considered the potential of human society? Theoretical point in time where all areas of study converge and build off each other. Time and information are the most important quantities in the universe All technological innovations seem to increase available information</p> </blockquote> </li> <li>Warning: Designer babies <ul> <li>Unrealistic standards</li> <li>Parents develop an ideal version of child, putting intense pressure on them</li> <li>Fewer siblings. Less sharing. Fewer people. Less connected. More isolated.</li> <li>Introduce new racism</li> <li>Similar to unrealistic standards for people now. Some parents will pour money into one and only child for it to be absolutely perfect. People can’t change DNA.</li> </ul> </li> <li>Health <ul> <li>Disease identification</li> <li>Gene therapy</li> <li>Diagnosis</li> </ul> </li> <li>Advancements: The biggests advancements in human history are related to the manipulation, production, replication, and transportation of knowledge. Internet, printing press, written word, cars, paper, electricity, etc. Humans evolution of the brain also fits into this category. The ability to abstract enables one to use, reuse, and quickly gather information in a variety of contexts. Commen sense is an extension of abstract knowledge.</li> <li>World Industrialization <ul> <li>The world will become more modern, on par with America</li> <li>How do people communicate? Need to increase speed of information transfer</li> <li>Automation <ul> <li>Increased level of absraction at which we operate</li> <li>How do people enjoy life without a job</li> </ul> </li> </ul> </li> <li>Revolutions <ul> <li>The next industrial revolution will happen from people being able to reach a new internal peak through better education, food, more available time, bmis, etc.</li> </ul> </li> <li>Others <ul> <li>Yin Yang: In order to experience pleasure, you MUST experience pain. In the future, people will be heavily ‘spoon fed’ (for lack of a better word). They will experience a large lack of pain. This will strip them with any ability to appreciate where they are. I expect, in the future, a very prosperous time, yet high suicide rate due to the previously mentioned phenomenom.</li> <li>Perfection: The issue with being too perfect is the tiniest blemishes will appear as giant warts relative to the ‘perfection’ which surrounds it.</li> <li>Political system: Require two opposing forces. Divergence is bad.</li> <li>Politicians are too specialized. We need intelligent people who have a large domain of understanding. Not pure generalists. They must not be puppets.</li> <li>Truth is important in todays age. This is the only solution to misinformation, a plague which will only strengthen with time. With the improvements in AI, and countries realizing AI as a tool for destruction, this will become scary. Just like mathematicians, we require a strong, secure, self-consistent base of knowledge and understanding of which we can rely on.</li> <li>Most religions have a powerful entity that has created us. A being that has created the universe must be infinitely intelligent compared to humans. Faith is an integral concept in most religious parties; it evens the playing field. An intelligent being who created the universe would prevent any logical argument proving/disproving its existence. I made assumptions here. What if they aren’t true?</li> </ul> </li> <li>Why American Healthcare Is The Worst In The Developed World <ul> <li>For profit system. People in charge rely on people getting sick.</li> <li>US is the worst health care system in developed worlds even though it spends the most. $2500 for an ambulance.</li> <li>Focused too much on profit and efficiency.</li> <li>Capitalism is good in some situations. But it is not a solution for everything.</li> <li>Money goes to bonuses and share holders</li> <li>The american dream, you must be asleep to believe it.</li> <li>People get too used to where they are, they don’t think things can be different. They can’t imangine a different life becuase they are so accustumed to their current one.</li> </ul> </li> <li>Human <ul> <li>Human desire for evil</li> <li>brains are highly adaptable</li> <li>This is why pain/joy, or, ultimately everything, is relative.</li> <li> <p>Humans have changed little physically over the past 2000 years, but very much culturally.</p> </li> <li>Is the route humanity is on necessary? <ul> <li>There are ‘two’ sides to take on whether or not technological improvement is necessary. Curiosity and necessity.</li> <li>Humans have become far too afraid of death and fight it too often. People must learn to accept death. People are too focused on science and technological improvement but have lost sight of necessity. All we need is what we need to survive. I’m not quite sure it’s possible to completely eliminate human suffering. It seems that there is a equilibrium built into nature that we partake of. There is only so much pain we can experience and only so much pleasure. The ‘best’ scale for experiencing the world is a logarithmic one. Log is built into nature. The corona virus illuminates the future of complete seclusion, a property that is already plauging young individuals, facilitated by the internet. ‘Accepting’ your current position, no matter how good or bad, seems to bring you to a new baseline of experience. The ‘four noble truths of buddhism’ highlights this. Depression manifests as physical pain. Isolation seems to be a trigger for depression. How much better off are we? What does ‘better’ even refer to? Consumerism attitude is a virus.</li> <li>Isolation seems to be a large factor in human behaviour and seems to stem from humans being largely social creatures (with exceptions). This is a major factor in the appearance that prision systems are not optimal solutions. Isolating people does not solve problems caused by isolation.</li> <li>The curious point of view is clearly winning. People are dying to reach the ‘future’.</li> </ul> </li> <li>Human enhancements <ul> <li>Cyborgs <ul> <li>With huge developments in technology, we are able to create many variations of limb replacements. This will only improve.</li> <li>With neuralink and developments in medical technology, I expect ‘upgrades’ will be available. Just like in a video game.</li> <li>Will provide surface for the next racism</li> </ul> </li> <li>Gene-editing</li> <li>neural-enhancements <ul> <li>Change literal meaning of life</li> <li>Override tolerance. Consciousness floating in ecstacy.</li> <li>Conscioussness transfer?</li> <li>No sleep</li> <li>No need for VR goggles</li> </ul> </li> </ul> </li> </ul> </li> </ul> <h1 id="metaverse">Metaverse</h1> <blockquote> <p>A virtual, fully interconnected, universe</p> <ul> <li>Will primarily attract once there is reasonable certainty to invest energy and wealth into it. A decentralized metaverse is ideal, as it avoids the centralized monopolization and control of an entity with potentially adverse intent. Decentralized items such as cryptocurrency and the underlying blockchain technology are humanities initial exploration of these topics. They will be like stocks. The question is what will increase their value? How can metaverses provide value to people in the real world?</li> </ul> </blockquote> <ul> <li> <p>Artificial intelligence will make creating metaverses easily. Perhaps metaverses will be like websites. Widespread and easy to create endpoints with solutions or information that are interactive.</p> </li> <li>Our memories are built on senses. The more the metaverse can generate realistic signals for our senses, the more realistic it becomes. Our data, understanding of the world comes from our senses and our interpretation of those senses.</li> <li>The metaverse one clothing should fit multiple avatar styles. The best is for a designer to create multiple points on a manifold and interpolate the rest.</li> <li>Roblox, Minecraft, WOW</li> <li>IoT, AI, Internet already provide necessary infastructure to begin the build</li> <li>New internet</li> <li>Virtual families</li> <li>More resources per-person, but fewer people</li> <li>Fewer, larger cities</li> <li>Automated machines providing for humans</li> <li>Only high-level jobs. Those not able will be the first to lose.</li> <li> <p>Cows with VR goggles produce more milk (matrix)</p> </li> <li>Matrix</li> <li>Virtual Reality <ul> <li>Train doctors, pilots, astronauts, fire fighters, and more.</li> <li>A simulation for humans for artificial realistic data, training data, just like for AI</li> <li>Non-euclidean geometry can enable exploring an infinite room in a finite area</li> </ul> </li> <li>Virtual Augmentation</li> <li>Digitization and economization of the primary aspects of human existence <ul> <li>Society</li> <li>Culture</li> <li>Groups</li> <li>Language</li> <li>Creativity</li> </ul> </li> </ul> <h1 id="biosphere">Biosphere</h1> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Cities will be literal spheres. To block pollution and elements.
- Robots will live outside the bio-spheres. # Automation Automation will take jobs. Automation provides value for cheap (still cost of computation). Businesses (probably public busineses) should be taxed by how much production is generated by AI powered automation. # Revolutions # Kardashev scale # Futurology influencers - People
- Elon Musk
- Jeff Bezos
</code></pre></div></div> <ul> <li>Companies <ul> <li> <p>Blue origin</p> </li> <li>Boring <ul> <li>Cities will become higher and deeper</li> <li>The most unfortunate will live in ground, most successful at the top.</li> <li>Tunnels better protect from pollution.</li> </ul> </li> <li>Tesla <ul> <li>Gas cars will become obsolete. Then electric cars.</li> <li>Removed thermal problem from supercharging batteries</li> <li>Simpler manufacturing, fewer parts</li> <li>Huge improvements to batteries. 16% range increase, 5x more energy, 6x more power.</li> <li>Tabless cell</li> <li>Most important development here is powerful bateries</li> </ul> </li> <li>Starlink <ul> <li>Read any messages etc.</li> <li>Control anything connected to the internet.</li> <li>Hopping around on the sattelites. Going any and everywhere. Access to any and everything.</li> </ul> </li> <li>Neuralink <ul> <li>See peoples thoughts</li> <li>Manipulate them</li> <li>Mind reading</li> </ul> </li> </ul> </li> </ul> <h1 id="technology">Technology</h1> <p>Makes it cheaper and faster to produce, creating new wealth which can be used to create new sectors. AI will replace cognitive work. Hollywood, for example, is a massive industry that can be completely wiped by AGI creative work, like text to video. Cognitive scaling to almost endless scaling. Generalized automation will lead to price collapse. Disrupt of almost any industry. Reduced agregate demand. As consumers make less, there will be lower demand, and lower B2C. Primary businesses will revolve around B2B and B2Government. Wealth can become demolished.</p> <h2 id="industries-that-persist">Industries that persist</h2> <p>Food, water, energy Automation robotics (build the tools that build the tools). During a gold rush sell the pickaxe. Microsoft and NVIDIA are going all in to tools Sustainable tech Service industries</p> <h2 id="industries-that-extinct">Industries that extinct</h2> <p>Longevity, protein folding, drug discovery Rejuvination therapy University</p> <h2 id="primary-needs">Primary needs</h2> <p>Needs-based economy Sustainable energy Resource redistribution Consumer behavior shift First phase</p> <h2 id="secondary-needs">Secondary needs</h2> <p>Rich experiences Creatvity and expression Intellectual stimulation Culture and community Spirituality and meaning</p> <h1 id="economy">Economy</h1> <p>Perhaps the future economy is one of restriction. Making sure people don’t use too many resources. A universal clicker that ensures everyone gets the same, and any consolidation is negatively compounding, opposite to capatilistic society. This is a natural solution to the resource distribution problem, makes sure everyone has equality of resource, as there is no longer a need to incentivize production. People are incentivized to make the most of the resources they have.</p> <h1 id="energy">Energy</h1> <ul> <li>Fusion energy</li> <li>Battery</li> <li>Fossil Fuels -&gt; Electricity -&gt; ?</li> <li>What are some downfalls of Electricity? <ul> <li>Loses power over time</li> </ul> </li> <li>Solar Panels <ul> <li>Efficient conversion of solar energy (much moreso than plants)</li> </ul> </li> </ul> <h2 id="fusion-power">Fusion power</h2> <p>Costs 10s to 100s of billions to research and ultimately build some form of successful fusion reactor. Solar panels already generate free electricity. Materials is the primary concern. A fusion power can create clean water but how does it transport? Battery technology limits planes and boats. This is still better than solar power, as where it is most efficient is deserts is where there is no water.</p> <p>Can’t create any resources as, just in terms of energy requirements, stars aren’t capable of fusing iron together, it requires intense gravitational collapse to create heavier elements. In addition to that there is the complexity of accurately generating a practical amount of resources. Stars don’t have an infinite amount of energy, their energy is not necessarily sufficient to create sufficient resources.</p> <h1 id="resources">Resources</h1> <ul> <li>https://www.un.org/en/global-issues</li> <li>[10 AI Predictions for 2024]{ref=https://www.youtube.com/watch?v=FWO9OJUeouE&amp;list=WL&amp;index=30; resource=website;}</li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[Technological progress Linear. Imagine building a pyramid to reach the next fruit.]]></summary></entry><entry><title type="html">Artificial Intelligence</title><link href="https://t-wilkinson.github.io/blog/2024/artificial-intelligence/" rel="alternate" type="text/html" title="Artificial Intelligence"/><published>2024-07-23T16:38:27+00:00</published><updated>2024-07-23T16:38:27+00:00</updated><id>https://t-wilkinson.github.io/blog/2024/artificial-intelligence</id><content type="html" xml:base="https://t-wilkinson.github.io/blog/2024/artificial-intelligence/"><![CDATA[<p>Threats of AI models? What happens if threat actors develop first AGI? There isn’t much we can do. But the people who should be in charge are.</p> <p>Missing memory and sufficient generalization</p> <p>Process-supervised Reward Models Overall better at training LLMs as they give more nuanced details.</p> <p>Perhaps memory would be more effective if model computes and stores future inputs into the model. It could then use the memory to regenerate complex information that can be used to modify the input to the transformer, change the queries, etc.</p> <p>AI and LLM in particular clearly have huge potential in todays society but what exactly that is isn’t exactly clear. The world of AI is changing quickly and its not clear how AI will exactly fit in our lives. They are clearly useful tools for coding, but their role with existing programmers is not well known. Will they replace programmers and when? Will they only ever be a useful tool? Currently, models like ChatGPT are useful as compressed corpus of information, with an intelligent interaction including understanding queries, and usefully packaging and organizing recalled information.</p> <p>Everyone likes to say that deep learning models are “just numbers” but its an oversimplification. Human brains are “just neurons”, and neurons function like small feed foward networks, with some potential for dynamically changing their “weights”. Both are universal function approximators. Both take inputs, human brain takes a ton of sensory inputs. Both learn to extract information in a way that is useful for how the intelligent entity interacts with its environment. The few things missing from LLMs, in particular, is a reinforcement model, where the machine learns to set goals. The more general goals become, the more one dives into philosophy, needing to understand how goals affect the entities in its surrounding environment, which is essentially morality. Also, in order to be computationally efficient, they need to “abstract” their philosophy; compress their philosophy and goals to fit a large number of interactions and environments. This is where meaning comes in.</p> <p>Artificial intelligence models could have some kind of “autism” or another mental disorder like anti-social personality disorder. Just like in humans, it can be difficult to predict and the only way to diagnose it is to look at past events, in which case it is too late.</p> <p>Consciousness is measurable, but hard to define.</p> <ul> <li>AI vs. Human <ul> <li>Human attributes successful AI must replicate <ul> <li>Heuristics</li> <li>Generalizations from little information</li> <li>Relating</li> <li>Memory</li> <li>Competition</li> <li>Brains need very little to learn from and generalize. How much knowledge is encoded in DNA? The brain is very dynamic, so too should nueral networks.</li> <li>The brain is very messy, many parts communicate with others</li> <li>Internal representation of the world</li> <li>Construct thoughts to test algorithms to add to policy network</li> <li>Consciousness seems to be a recursive thought</li> <li>Construct multiple representations/interpretations of the environment.</li> <li>What are the critical parts of the human brain? Neocortex, inner, outer, stem, etc.</li> <li>Dynamic rewriting (in order to do this, would you have to have an intuition on what does what in the neural architecture?)</li> <li>Lazy architecture? (Don’t compute until necessary, allow neurons on same level to read from eachother ‘before’ the fire)</li> <li>What role does the neocortex play in the thought process</li> <li>Morals, conscience, parents, school, culture, religion is the human value network.</li> <li>Human value network is modifiable.</li> <li>Look at everything as a learning experience</li> <li>What is the human value function? value(t) := (current state(t), current goal(t))</li> <li>Exploiters at multiple levels of abstraction.</li> <li>Personality. <ul> <li>Ideal for mutualism. Facilitates specialization.</li> <li>Like a puzzle piece.</li> <li>Marriage are two opposite exploiters with a value function maximizing cooperation.</li> </ul> </li> <li>Selection function for competition</li> <li>Multiple methods for learning (reinforcement, supervision, and unsupervised)</li> <li>Identify symmetries</li> <li>Evolution (Exploitain of current knowledge and exploration of unkown)</li> <li>Learn from others mistakes. Adapt in real time.</li> <li>Dopamine (and similar) are amazing rewards. They are very general, dynamic, and can be overwritten.</li> <li>There needs both high and low levels of supervised learning. Low levels produce people that rethink everything, very necessary.</li> <li>Neurons are individual units</li> <li>Core ‘brain’, and dynamic ‘functionalities’ tacked on the end.</li> <li>Humans reward ourselves from the observation of the action</li> <li>Meta-learning (learn to learn)</li> <li>Abstract when learning small task</li> <li>DNA encodes reward function learnt from millions of years.</li> <li>Learn in multiple simulations</li> <li>The reward function maximizes existance. Anything that does not, will simply fail to exist.</li> <li>Some (complex) variation of NEAT is necessary to reach and match the complexity of evolution of human life.</li> <li>Abstract thought between ‘thinking to move’ and ‘moving in that direction’</li> <li>Q-Learning acts like memory which enables one to learn from the past. Only store the worst / best memories.</li> <li>Human brain only lets a couple bits of information into the brain.</li> <li>Break ultimate reward (existance) into multiple sub-goals.</li> <li>The human brain is very dynamic, as such, there is little hard-coded information</li> <li>Understand symmetry. Understand general layout of game controller but sometimes buttons roles can be reversed. We can then ‘transfer learn’ without having to relearn.</li> <li>Neurons encode multiple representations of a concept</li> </ul> </li> <li>Computers vs. Brains <ul> <li>Bad memory has its benefit. Extract essence from concept.</li> <li>Computers suffer from the same effect that humans with amazing memory do. They are able to see in amazing clarity the detail, but not what that detail forms. Unable to see the bigger picture.</li> <li>The digital aspect of computers is what prevents them most from operating similar to humans.</li> <li>Computers are so powerful for humans because they operate on our greatest weakness. The same thing that allows us to create and innovate is what prevents us from holding a couple random numbers in our head, or remembering a picture color by color.</li> <li>In the future, there will be a union with self-aware computers and humans, with those being seperate from classical computers we use today.</li> </ul> </li> </ul> </li> <li>Research <ul> <li>Research ideas <ul> <li>Have an internal neural net and when faced with a new challenge, init a new neural net with the internal one. Update the internal net very slightly compared to new net ? How to encode linguistic information in ai model as a sort of supervised learning ? What is the right level of abstraction for knowledge base <ul> <li>Something to approximate core knowledge (elizabeth spelke)</li> </ul> </li> </ul> </li> <li>Look for applications <ul> <li>Have point of view of PhD student <ul> <li>You need to publish papers</li> <li>You need new innovative ideas ? How can it be implemented in code ? What problems does it solve ? What does it relate to</li> </ul> </li> <li>To general AI</li> <li>Visual applications</li> <li>Real-world problems</li> <li>Skip things that are too difficult or mind-bending</li> <li>Really chew on core concepts</li> <li>Think about innovations that you can make ? Why is it done like this ? Why not this way ? What if we add this</li> </ul> </li> </ul> </li> <li>Companies <ul> <li>Future AI <ul> <li>https://futureai.guru/</li> </ul> </li> </ul> </li> </ul> <h1 id="generalization">Generalization</h1> <p>Generalization of AI algorithms are abysmal. Human brains are apt at forming abstractions and generalizations.</p> <p>Perhaps one path to generalization is creating multiple models based on very limited data. This allows offloading a lot of computation beforehand. Then during the next iteration, you identify which model or collection of models are the best. The connection and association between models as well is a very powerful tool to compose models in complex ways. These cross connections can also be tuned and honed to develop multiple complex abstract models and then select the optimal one for the task at hand.</p> <h1 id="architectures">Architectures</h1> <p>Architectures: Neural Turing machine, Differentiable neural computer, Transformer, Recurrent neural network (RNN), Long short-term memory (LSTM), Gated recurrent unit (GRU), Echo state network, Multilayer perceptron (MLP), Convolutional neural network, Residual neural network, Mamba, Autoencoder, Variational autoencoder (VAE), Generative adversarial network (GAN), Graph neural network Organizations: Anthropic, EleutherAI, Google, DeepMind, Hugging, Face, OpenAI, Meta, AI, Mila, MIT, CSAIL, Huawei People: Yoshua Bengio, Alex Graves, Ian Goodfellow, Stephen Grossberg, Demis Hassabis, Geoffrey Hinton, Yann LeCun, Fei-Fei Li, Andrew Ng, Jürgen Schmidhuber, David Silver, Ilya Sutskever Concepts: Gradient descent, SGD, Clustering, Regression, Overfitting, Hallucination, Adversary, Attention, Convolution, Loss functions, Backpropagation, Batchnorm, Activation, Softmax, Sigmoid, Rectifier, Regularization, Datasets, Augmentation, Diffusion, Autoregression</p> <p>Convolutional Neural Networks (CNNs): LeNet, AlexNet, VGGNet, GoogLeNet (Inception), ResNet (Residual Networks), DenseNet, MobileNet, EfficientNet, Capsule Networks Recurrent Neural Networks (RNNs): Elman Networks, Jordan Networks, Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), Bidirectional RNNs, Echo State Networks (ESNs), Neural Turing Machines (NTMs), Transformer (though primarily used in sequence-to-sequence tasks) Graph Neural Networks (Graph Nets): Graph Convolutional Networks (GCNs), GraphSAGE (Graph Sample and Aggregated), Gated Graph Neural Networks (GGNNs), Graph Isomorphism Networks (GIN), Message Passing Neural Networks (MPNNs), Graph Attention Networks (GAT), Graph Neural Networks for Molecular Graphs Autoencoders: Vanilla Autoencoders, Variational Autoencoders (VAEs), Denoising Autoencoders, Sparse Autoencoders, Contractive Autoencoders, Adversarial Autoencoders Generative Adversarial Networks (GANs): Vanilla GANs, Deep Convolutional GANs (DCGANs), Wasserstein GANs (WGANs), Conditional GANs (cGANs), InfoGAN, CycleGAN, StyleGAN, BigGAN, ProGAN State space model: Mamba</p> <p>Memory-Augmented Networks:</p> <ul> <li>Neural Turing Machines (NTMs)</li> <li>Differentiable Neural Computers (DNCs)</li> <li>Transformer-XL Attention Mechanisms:</li> <li>Bahdanau Attention</li> <li>Luong Attention</li> <li>Transformer (self-attention)</li> <li>BERT (Bidirectional Encoder Representations from Transformers)</li> </ul> <p>Few-Shot Learning:</p> <ul> <li>Matching Networks</li> <li>Prototypical Networks</li> <li>Relation Networks Continual Learning:</li> <li>EWC (Elastic Weight Consolidation)</li> <li>LwF (Learning without Forgetting)</li> <li>iCaRL (Incremental Classifier and Representation Learning) Federated Learning:</li> <li>FedAvg (Federated Averaging)</li> <li>FedProx (Federated Proximal)</li> <li>FedGAN (Federated GAN) Pretrained Language Models:</li> <li>OpenAI GPT (Generative Pre-trained Transformer)</li> <li>BERT (Bidirectional Encoder Representations from Transformers)</li> <li>XLNet</li> <li>RoBERTa</li> <li>T5 (Text-To-Text Transfer Transformer)</li> </ul> <p>Siamese Networks:</p> <ul> <li>Siamese Neural Networks</li> <li>Triplet Networks</li> <li>Contrastive Divergence Networks Capsule Networks:</li> <li>Dynamic Routing Between Capsules (CapsNets)</li> <li>Capsule Networks with Attention</li> <li>Graph Capsule Networks Neural Architecture Search (NAS):</li> <li>DARTS (Differentiable Architecture Search)</li> <li>ENAS (Efficient Neural Architecture Search)</li> <li>MNAS (Mobile Neural Architecture Search)</li> <li>NASNet</li> <li>AmoebaNet Speech Recognition:</li> <li>DeepSpeech</li> <li>Listen, Attend, and Spell (LAS)</li> <li>Wav2Vec Video Analysis:</li> <li>C(Convolutional 3D)</li> <li>I(Inflated ConvNets)</li> <li>TSN (Temporal Segment Networks) Recommendation Systems:</li> <li>Collaborative Filtering using Neural Networks</li> <li>DeepFM (Factorization Machines)</li> <li>Neural Collaborative Filtering Anomaly Detection:</li> <li>Autoencoder-based Methods</li> <li>Variational Autoencoders (VAEs) for Anomaly Detection</li> <li>One-Class SVM with Neural Networks Health and Medical Imaging:</li> <li>CheXNet for Chest X-ray Analysis</li> <li>U-Net for Medical Image Segmentation</li> <li>CNNs for Brain Tumor Detection Time Series Analysis:</li> <li>WaveNet for Audio Generation</li> <li>DeepAR for Time Series Forecasting</li> <li>TCN (Temporal Convolutional Network)</li> </ul> <h1 id="research">Research</h1> <p>Efficient Training of Neural Networks:</p> <ul> <li>Pruning and Quantization: Techniques to reduce the size of models and the computational resources needed for training and inference.</li> <li>Federated Learning: Distributed training that allows for privacy preservation and efficiency by training models across multiple devices.</li> <li>Transfer Learning: Leveraging pre-trained models on new tasks to reduce the amount of data and computation required for training.</li> </ul> <p>Mixture of Experts (MoE):</p> <ul> <li>MoE (sparse model) models aim to improve scalability and efficiency by having different ‘expert’ sub-models that specialize in different aspects of a task.</li> </ul> <p>Language Models (LMs) and Large Language Models (LLMs):</p> <ul> <li>Research into improving the performance and training efficiency of large-scale language models, like GPT-3 and BERT.</li> <li>Exploration of new training techniques, regularization methods, and architectures to improve the capabilities of LLMs.</li> </ul> <p>Foundational models:</p> <ul> <li>Ongoing improvements to the transformer architecture to increase efficiency, such as the Linformer, Performer, and Reformer.</li> <li>Research into making transformers more parameter and data-efficient.</li> <li>Mamba</li> <li>Sparse Transformers: Reducing the computational complexity by using sparsity in the attention mechanism.</li> <li>Vision Transformers (ViTs): Adapting the transformer architecture for computer vision tasks.</li> </ul> <p>Generalization Capabilities:</p> <ul> <li>Meta-Learning: Teaching AI to learn new tasks with minimal data by generalizing from previous experiences.</li> <li>Few-Shot Learning: Improving the ability of models to perform tasks with very few examples.</li> <li>Generalizing models through general interfaces (like languages) where the model learns tasks based on input</li> </ul> <p>Cognitive Neuroscience-Inspired Architectures:</p> <ul> <li>Research into architectures that mimic certain aspects of human cognition or brain structure, such as Hierarchical Temporal Memory (HTM) and models inspired by the Thousand Brains Theory of Intelligence.</li> </ul> <p>Model Distillation:</p> <ul> <li>Techniques for transferring knowledge from large, complex models to smaller, more efficient ones without a significant loss in performance.</li> </ul> <p>Self-Supervised Learning:</p> <ul> <li>Methods for training models using unlabeled data, where the learning algorithm generates its own labels from the data.</li> </ul> <p>Robustness and Adversarial Machine Learning:</p> <ul> <li>Developing models that are resistant to adversarial attacks and can generalize well in the face of distribution shifts or noisy data.</li> </ul> <p>Explainability and Interpretability:</p> <ul> <li>Techniques to understand and explain the decisions made by AI models, which is crucial for deployment in sensitive areas.</li> </ul> <p>Fairness, Accountability, and Transparency:</p> <ul> <li>Ensuring that AI systems are fair and do not perpetuate or amplify biases. This includes research into ethical AI and algorithmic accountability.</li> </ul> <p>Multi-Modal Learning:</p> <ul> <li>Multi modal input/output</li> <li>Combining information from various data sources (e.g., text, images, and audio) to improve learning and prediction.</li> <li>Multi modal transformers: q-transformer</li> </ul> <p>Neuro-Symbolic AI:</p> <ul> <li>Combining neural networks with symbolic reasoning to create systems that can reason with the abstract, structured knowledge.</li> </ul> <p>Energy-Efficient AI:</p> <ul> <li>Research aimed at reducing the carbon footprint of training and deploying AI models.</li> </ul> <p>Alignment:</p> <ul> <li>RLAiF, RLHF, Self-rewarding language models</li> </ul> <p>Cognitive capabilities:</p> <ul> <li>Logical reasoning and other modals</li> </ul> <p>Limited data:</p> <ul> <li>Training with limited data: synthetic, more efficiently generalizing, GPT or CLIP style models</li> <li>Sample efficiency</li> </ul> <p>Natural Language Processing (NLP):</p> <ul> <li>Text Processing</li> <li>Speech Recognition</li> <li>Machine Translation</li> <li>Question Answering</li> <li>Sentiment Analysis Word Embeddings: Word2Vec, GloVe. Seq2Seq Models: Encoder-Decoder architectures. Attention Mechanisms: Vital for handling context in sequences.</li> <li>BERT (Bidirectional Encoder Representations from Transformers)</li> <li>ELMO (Embeddings from Language Models)</li> <li>Transformer-XL</li> <li>GPT-3</li> </ul> <p>Computer Vision:</p> <ul> <li>Object Recognition</li> <li>Image Classification</li> <li>Image Generation</li> <li>Image Segmentation</li> <li>Visual Question Answering Image Processing Techniques: Filters, edge detection. Object Detection and Recognition: Techniques like YOLO (You Only Look Once), R-CNNs. Object Detection and Segmentation: <ul> <li>YOLO (You Only Look Once)</li> <li>SSD (Single Shot Multibox Detector)</li> <li>Mask R-CNN</li> <li>FCN (Fully Convolutional Networks)</li> <li>U-Net Image Generation:</li> <li>DCGAN (Deep Convolutional Generative Adversarial Network)</li> <li>ProGAN (Progressive GAN)</li> <li>StyleGAN2</li> <li>CycleGAN</li> <li>Pix2Pix Image Super-Resolution:</li> <li>SRCNN (Super-Resolution Convolutional Neural Network)</li> <li>VDSR (Very Deep Super-Resolution)</li> </ul> </li> </ul> <p>Robotics:</p> <ul> <li>Robotic Perception</li> <li>Motion Planning</li> <li>Robot Control</li> <li>Human-Robot Interaction</li> <li>Swarm Robotics</li> </ul> <p>Knowledge Representation and Reasoning:</p> <ul> <li>Ontologies</li> <li>Knowledge Graphs</li> <li>Logic Programming</li> <li>Inference Systems</li> </ul> <p>Expert Systems:</p> <ul> <li>Rule-Based Systems</li> <li>Decision Support Systems</li> <li>Diagnostic Systems</li> </ul> <p>Planning and Scheduling:</p> <ul> <li>Automated Planning</li> <li>Resource Allocation</li> <li>Task Scheduling</li> </ul> <p>Evolutionary Computing:</p> <ul> <li>Genetic Algorithms</li> <li>Genetic Programming</li> <li>Evolutionary Strategies</li> </ul> <p>Neural Networks and Deep Learning:</p> <ul> <li>Feedforward Networks</li> <li>Recurrent Networks</li> <li>Convolutional Networks</li> <li>Generative Adversarial Networks (GANs)</li> <li>Transformer Models</li> </ul> <p>Cognitive Computing:</p> <ul> <li>Emulating Human Thought Processes</li> <li>Mimicking Human Perception</li> <li>Cognitive Models</li> </ul> <p>Interdisciplinary: AI Ethics: Bias and Fairness, Explainability, Accountability, Privacy and Security AI for Healthcare: Medical Imaging, Disease Prediction, Drug Discovery, Personalized Medicine AI in Finance: Algorithmic Trading, Fraud Detection, Credit Scoring, Risk Management AI in Gaming: Game Playing Agents, Procedural Content Generation, Player Behavior Modeling AI in Education: Intelligent Tutoring Systems, Adaptive Learning Platforms, Educational Data Mining AI in Cybersecurity: Threat Detection, Anomaly Detection, Security Analytics Quantum AI: Quantum Machine Learning, Quantum Algorithms Swarm Intelligence: Ant Colony Optimization, Particle Swarm Optimization AI and Creativity: Computational Creativity, AI in Art and Music AI Hardware: Neuromorphic Computing, Quantum Computing</p> <h1 id="model-deployment">Model deployment</h1> <p>Web server: Flax, Django, Express.js, FastAPI Cloud systems: AWS Lambda, AWS SageMaker, Azure Functions, Azure ML Service, Google Cloud Functions, Google Cloud AI Platform Model deployment: TorchServe, TensorFlow Serving, ONNX Runtime (PyTorch models can be converted to ONNX format)</p> <h1 id="fundamental-concepts">Fundamental concepts</h1> <p>Mathematics: Linear Algebra: matrices, vectors, eigenvalues, eigenvectors. Calculus: derivatives and integrals, lagrange multiplier, hessian matrix, jacobian Probability and Statistics: Probability distributions, statistical inference, Bayesian reasoning, bayesian inference, markov random fields, bayesian networks Bayesian Inference: Probabilistic reasoning and updating beliefs. Linear algebra: vectors, matrices, tensors, multiplying matrices, SVD, eigendecomposition, PCA Probability: random variables, probability distributions, conditional probability, independence, variance, bayesian statistics Information theory: information Numerical computation: overflow and underflow, gradient based optimization, constrained optimization Machine learning: cpacity, overfitting, underfitting, MLE, SGD Discrete mathematics: concrete mathematics, recurrence, sums, number theory, binomial coefficient, special numbers, generating functions, discrete probability, asymptotics</p> <p>Machine Learning: Supervised Learning: classification - Linear Regression - Support Vector Machines - Decision Trees - Random Forests - Neural Networks Unsupervised Learning: Clustering, dimensionality reduction. - k-Means Clustering - Hierarchical Clustering - Principal Component Analysis (PCA) - Generative Adversarial Networks (GANs) - Autoencoders Neural Networks: Architectures, activation functions, training algorithms.</p> <ul> <li>Reinforcement Learning: multi-agent systems</li> <li>Transfer Learning</li> <li>Semi-Supervised Learning</li> <li>Self-Supervised Learning</li> </ul> <p>Reinforcement Learning:</p> <ul> <li>Exploration-Exploitation Strategies</li> <li>Actor-Critic Models Policy and Value Iteration: on policy/off policy, value iteration, policy iteration Exploration vs. Exploitation Trade-off: Balancing the need to explore new actions and exploit known ones. Algorithms: Q-learning, DQN, policy gradient methods, PPO, DDPG, TRPO Markov decision process: reward functions, observability, reinforcement learning, continuous</li> </ul> <p>Deep Learning: Neural Network Architectures: Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Transformers. Transfer Learning: Leveraging pre-trained models for new tasks. Regularization Techniques: Dropout, batch normalization. Deep networks: example of XOR, gradient-based learning, architecture design, back propagation, regularization, multitask learning, sparse representations, dropout, adversarial traning, basic algorithms</p> <p>Optimization: Gradient Descent: Variants like Stochastic Gradient Descent (SGD), Adam, RMSProp. Metaheuristic Algorithms: Genetic algorithms, simulated annealing.</p> <ul> <li>Gradient Descent Variants (e.g., Adam, RMSProp)</li> <li>Evolutionary Algorithms</li> <li>Simulated Annealing</li> </ul> <p>Algorithmic Techniques: Dynamic Programming: Solving problems by breaking them into overlapping subproblems. Monte Carlo Methods: Leveraging random sampling for estimation.</p> <ul> <li>Dynamic Programming for Optimization</li> <li>Monte Carlo Methods</li> <li>Markov Chain Monte Carlo (MCMC) Techniques</li> </ul> <p>Probabilistic Graphical Models:</p> <ul> <li>Bayesian Networks</li> <li>Markov Random Fields</li> </ul> <p>Data Structures for Efficient Computation:</p> <ul> <li>Tensor Data Structures for Neural Networks</li> <li>Sparse Matrix Representations</li> <li>Graph Data Structures (for representation and traversal)</li> </ul> <p>Ethical and Social Implications: Fairness and Bias: Understanding and mitigating biases in AI systems. Interdisciplinary Understanding: Recognizing the societal impact of AI and ethical considerations.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Threats of AI models? What happens if threat actors develop first AGI? There isn’t much we can do. But the people who should be in charge are.]]></summary></entry><entry><title type="html">Music Theory</title><link href="https://t-wilkinson.github.io/blog/2024/music-theory/" rel="alternate" type="text/html" title="Music Theory"/><published>2024-07-23T16:37:37+00:00</published><updated>2024-07-23T16:37:37+00:00</updated><id>https://t-wilkinson.github.io/blog/2024/music-theory</id><content type="html" xml:base="https://t-wilkinson.github.io/blog/2024/music-theory/"><![CDATA[<ul> <li>A big thing about voicing chords is where to distribute the tension</li> <li>Passing note</li> <li>Chord voicing</li> <li>Chord proression</li> <li>Passing chord</li> <li>Chord substitutions</li> <li>Voice leading <ul> <li>Combines horizontal and vertical harmony.</li> </ul> </li> <li>Inside note</li> <li> <p>Available tension</p> </li> <li>Sound <ul> <li>Pitch</li> <li>Frequency</li> <li>12tone equal tempermant</li> </ul> </li> <li>Relationship between two notes <ul> <li>Interval</li> <li>Harmonic series</li> <li>Overtone series</li> <li>Undertone series</li> </ul> </li> <li>Multiple notes <ul> <li>Chord</li> <li>Scale</li> <li>Mode</li> </ul> </li> <li>Tonality <ul> <li>Key</li> <li>Tonicization</li> <li>Modality</li> </ul> </li> </ul> <h1 id="theory-of-music">Theory of music</h1> <p>Painting a story. Establish a topos, a context of truth and meaning. Establish truth, structures, etc. Introduction of most important truths (climax).</p> <p>Notes and collections of notes.</p> <p>Texture, soundscape, tonal center, intervals</p> <p>Textures made of sound with varying variations</p> <p>Sounds that interact with each other in the moment and sounds that interact with the cognitive position in the music.</p> <h1 id="musicality">Musicality</h1> <p>On notes repeated on the beat (with accompanying notes offbeat), apply a dynamic, making those more pronounced or quieter. Or really to any voice. Bring out voices that contrast nicely or support nicely the main melody</p> <h2 id="springs">Springs</h2> <p>Alternating notes that converge or diverge create movement to a resolution or tension point. Apply (de)crescendos to help move the music forward and assist in achieving the goal.</p> <h2 id="chromatics">Chromatics</h2> <p>Suggest dynamic change</p> <h2 id="tenuto">Tenuto</h2> <p>Can suggest an immediate slow down then progressive speed up as long as the chord is held down. Has harmonic implications as the chord is held.</p> <h2 id="principle-emphasis">Principle emphasis</h2> <p>There is a constant dance between what is the main voice heard, to seperate melodic lines.</p> <h2 id="melody">Melody</h2> <p>Melodic lines or higher register notes can summarize notes.</p> <h2 id="grace-notes">Grace notes</h2> <p>Creates movement to a harmonic note</p> <h2 id="chord-shaping">Chord shaping</h2> <p>Higher notes have more emphasis naturally so music take care to deemphasise higher notes.</p> <h2 id="dissapation">Dissapation</h2> <p>Higher notes dissapate faster so those with melodic, harmonic, or principle voice significance should be played louder.</p> <h2 id="ladder">Ladder</h2> <p>A repeated pattern that is shifting in registers can be played with sequences of crescendos or decrescendos.</p> <h2 id="alternating-notes">Alternating notes</h2> <p>Alternating notes (perhaps with slight chromatic variations) can be played in alternating crescendos and decrecendos with an underlying (de)crescendo.</p> <h2 id="direction-based-movement">Direction based movement</h2> <p>Treat it as several entering/dissapearing melodic lines.</p> <h1 id="conceptual-tools">Conceptual tools</h1> <ul> <li>Circle of fifths ? Why do they differ by one accidental - Notes differ by perfect 5ths - At the 6 degree, switches a WH -&gt; HW <ul> <li>Nearby notes have very similar relationships to notes around the circle <ul> <li>Similar key signatures</li> <li>Dissonant/consonnant will sound similarly to the other root</li> </ul> </li> <li>Moving around relatively increases brightness</li> <li>Significance of the fifths <ul> <li>Find the first note that has a sharp 11 notes in, let that be X</li> <li>Now move 7 notes from X (clockwise along the circle of fiths), let that be Y</li> <li>Continue the major scale from Y</li> <li>The 4th note from Y will be the sharp 11 notes from X, $4 + 7 == 11$</li> </ul> </li> <li>Interpret the above in terms of degrees</li> <li>Moving up resovles with plagal cadence or equivalently plays the dominant function. The result depends on what is played next. The context.</li> <li>Because their fifths, stacking scales from progressing through fifths overlaps 4 degrees</li> <li>Major scale is formed by moving along the circle of fifths and transposing notes to be in order</li> <li>In 12 tone equal temperment, fifths is the only way to form a scale other than the chromatic scale though jumps as 5 and 7 doesn’t divide into 12</li> </ul> </li> <li>Chromatic circle</li> <li>Orbifold</li> </ul> <h1 id="acoustics">Acoustics</h1> <ul> <li>Overtone series</li> <li> <p>Undertone series</p> </li> <li>Why doesn’t inverting a chord change its harmony? Because of the overtone series.</li> <li>The human brain can only hold so many notes before it becomes too chaotic.</li> </ul> <p>The major scale is a sequence of fifths, not including the tritone of the root.</p> <p>Overtone series (Harmonic series) The overtone series has</p> <p>Undertone series</p> <h1 id="analysis">Analysis</h1> <h1 id="elements">Elements</h1> <ul> <li>Melody (pitch, dynamics, intervals)</li> <li>Harmony (texture, polyphonic, monophonic)</li> <li>Rythm (form, beat, tempo, meter, duration)</li> <li>Timbre</li> </ul> <h2 id="interval">Interval</h2> <ul> <li>Notes have the most harmonic similarity with their octaves. However the distance can manipulate the feeling of space, closeness, etc. Relationships between notes are minimized as the octave increases.</li> <li>Distance between two notes</li> <li>Qualities <ul> <li>Perfect := $4 5 8$</li> <li>Major := $2, 3, 6, 7$</li> <li>Minor := $♭2, ♭3, ♭6, ♭7$</li> <li>Augmented := $♯4 ♯5$</li> <li>Diminished := $♭♭3 ♭4 ♭5$</li> <li> <table> <tbody> <tr> <td>Tritone := 6 semitones</td> <td>aug4</td> <td>dim5</td> </tr> </tbody> </table> <ul> <li>The V7 chord has a tritone between 3 and 7</li> <li>vii∅7 has two tritones</li> </ul> </li> <li>Augmented and Diminished are accidentals on the perfect intervals</li> <li>Intevals are defined in relation to major scale</li> <li>Major intervals made <ul> <li>1 semitone smaller are minor</li> <li>2 semitone smaller are diminished</li> <li>1 semitone larger are augmented</li> </ul> </li> <li>Perfect intervals made <ul> <li>1 semitone smaller are diminished</li> <li>1 semitone larger are augmented</li> </ul> </li> <li>For odd intervals find the interval without accidentals then see how the accidentals modify the interval</li> </ul> </li> <li>Inverting <ul> <li>To invert an interval change the sign and add to nine</li> <li>Maj &lt;-&gt; Min</li> <li>Aug &lt;-&gt; Dim</li> <li>Perfect &lt;-&gt; Perfect</li> <li>Tritone &lt;-&gt; Tritone</li> </ul> </li> <li>Intervals around the circle of fifths <ol> <li>p0</li> <li>p5</li> <li>maj2</li> <li>maj6</li> <li>maj3</li> <li>maj7</li> <li>aug4/dim5/tritone</li> <li>min2</li> <li>min6</li> <li>min3</li> <li>min7</li> <li>p4</li> <li>p8</li> </ol> </li> <li>Chords are formed by combining intervals</li> </ul> <h2 id="chord">Chord</h2> <ul> <li>Hybrid chord <ul> <li>Extended chords with a dropped 3</li> </ul> </li> <li> <p>Chord-scale relationships CM7 C major scale or C Lydian scale CM7♯11 C Lydian scale CM♯5 C Lydian-Augmented scale Cm7 C dorian scale or C natural minor Cø7 C locrian scale or C locrian ♯2 Cø9 C locrian ♯2 Co7 C Octatonic Whole-Half CmM7 C melodic minor ascending Cm(6 9) C Dorian or C melodic minor ascending C7 C Mixolydian C7♯11 C Lydian-Dominant C7♯5 C Whole Tone scale C7♭5 C Whole Tone scale C7♭9 C Octatonic (Half-Whole) C7♯9 C Octatonic (Half-Whole) C13(♯11♯9) C Octatonic (Half-Whole) C7alt C Diminished-Whole Tone</p> </li> <li>Triads sound strongest in second inversion</li> <li>The 3 is important in triads because the minor/major 3 is the next harmonic after the 5</li> <li>There are no wrong notes in a chord. Just haven’t found its purpose.</li> <li>Usually built from thirds because it is harmonically pleasing</li> <li>Types Major. $1 3 5$ Major7. $1 3 5 7$ (always written Cmaj⁷) Minor. $1 ♭3 5$ Minor7. $1 ♭3 5 ♭7$ Diminished. $1 ♭3 ♭5$ Diminished7. $1 ♭3 ♭5 ♭♭7$ - Every note is same distance away - There are only 3 possible dim7 chords - Raising/lowering any of the notes will bring you to a different key - Every diminished can be turned into a major chord in a different key - Every major chord can be converted to a diminished chord in any key HalfDiminished7. $1 ♭3 ♭5 ♭7$ Augmented. $1 3 ♯5$ - Only 3 possible chords Augmented7. $1 3 ♯5 7$ Suspended2. $1 2 5$ Suspended4. $1 4 5$ Powerchord. $1 5$ Dominant. $1 3 5 ♭7$ (written C⁷) - Has a tritone - Dominant have mirror keys that they can resolve to Inversions. “chord” / “base”</li> <li> <p>The $vii∅7$ acts very similarly to $V7♭9$ with no root</p> </li> <li>Extensions <ul> <li>The 1 3 5 7 are chord tones and establish the type of chord</li> <li>Chords can be extended with 9 11 13</li> <li>9 11 13 add flavor but don’t establish chord quality and are not leading tones</li> </ul> </li> <li>Shell chord <ul> <li>Shell chords represent the minimum harmonic material needed to play a chord</li> <li>Simpler</li> <li>Easier to harmoize over/improvise as there are more available notes</li> </ul> </li> <li>Secondary chords <ul> <li>V7 or iidim7</li> </ul> </li> <li>Suspended chords <ul> <li>sus2, sus4, 9sus4, ♭9sus4</li> <li>Can substitute for ii or V7 chords</li> <li>Phrygian chords are sus♭9 chords <ul> <li>isus♭9♭13</li> <li>By changing ♭3 to a 4, converts to Isus♭9♭13</li> <li>Dropping the ♭13 results in Isus♭9</li> </ul> </li> </ul> </li> <li>Slash chords <ul> <li>Specify a note to play in the bass</li> <li>Define how the bass line moves</li> </ul> </li> <li>Polychord</li> <li>Borrowed chord <ul> <li>Chord from a different key</li> <li>Usually parallel major or minor</li> </ul> </li> <li>Chords are built from specific notes, so the axis represents a relationship between each of the notes <ul> <li>Stable notes := 1 5</li> <li>Modal notes := 3 ♭3 <ul> <li>Bring the focus towards either the root or p5</li> </ul> </li> <li>Hollow notes := 6 ♭7 <ul> <li>Tritone from modal notes</li> </ul> </li> <li>Unstable notes := 2 4 <ul> <li>Half step from modal notes</li> <li>Modal notes aren’t strong enough resolutino</li> </ul> </li> <li>Leading notes := 7 ♭6 <ul> <li>Leading notes</li> </ul> </li> <li>Uncanny notes := ♭2 ♯4 <ul> <li>Half step from stable notes</li> <li>Tritone from hollow notes</li> </ul> </li> </ul> </li> </ul> <p>“Can you give me an exhaustive list of chord types, such as the standard major, minor, dominant chords. For each chord, show the chord symbol, include the intervals in the chord and the overall harmonic and emotional effect of the chord. Also include jazz chords and also common chords that you may find in prokofiev, scriabin, or sorabji’s music.”</p> <p>Creating an exhaustive list of all chord types and their variants would be a monumental task, given the vast number of possible combinations and alterations. However, I can provide a list of some common chord types found in a variety of musical genres, including Western classical music, jazz, and the works of composers like Prokofiev, Scriabin, and Sorabji. For each chord type, I’ll provide the chord symbol, the intervals (relative to the root of the chord), and a brief description of their harmonic and emotional effects.</p> <p>Triads: Major (C): [Root, Major 3rd, Perfect 5th] Effect: Bright, stable, and consonant. Minor (Cm): [Root, Minor 3rd, Perfect 5th] Effect: Darker, melancholic, and still relatively stable. Augmented (C+): [Root, Major 3rd, Augmented 5th] Effect: Bright but tense and unstable, often used for modulation. Diminished (C°): [Root, Minor 3rd, Diminished 5th] Effect: Dark, tense, and unstable, often resolving to a major or minor chord.</p> <p>Seventh Chords: Major 7th (CM7): [Root, Major 3rd, Perfect 5th, Major 7th] Effect: Smooth, jazzy, with a sense of completeness but also forward motion. Minor 7th (Cm7): [Root, Minor 3rd, Perfect 5th, Minor 7th] Effect: Softer than a dominant 7th, contemplative, and slightly melancholic. Dominant 7th (C7): [Root, Major 3rd, Perfect 5th, Minor 7th] Effect: Tense, bluesy, and demands resolution, often to a major or minor chord a fourth above. Half-Diminished 7th (Cø7): [Root, Minor 3rd, Diminished 5th, Minor 7th] Effect: Mysterious, unresolved, often used in jazz and classical modulations. Fully Diminished 7th (C°7): [Root, Minor 3rd, Diminished 5th, Diminished 7th] Effect: Very tense and unstable, it can resolve in multiple directions due to its symmetry. Minor/Major 7th (Cm(M7)): [Root, Minor 3rd, Perfect 5th, Major 7th] Effect: Bittersweet, with a combination of darkness and brightness, often found in jazz.</p> <p>Extended Chords: Ninth (C9): [Root, Major 3rd, Perfect 5th, Minor 7th, Major 9th] Effect: Rich and colorful, with added tension from the ninth. Major 9th (CM9): [Root, Major 3rd, Perfect 5th, Major 7th, Major 9th] Effect: Lush and expansive, often used in jazz ballads. Minor 9th (Cm9): [Root, Minor 3rd, Perfect 5th, Minor 7th, Major 9th] Effect: Moody and complex, with a hint of mystery. Eleventh (C11): [Root, Major 3rd, Perfect 5th, Minor 7th, Major 9th, Perfect 11th] Effect: Very dense and colorful, often with the 3rd omitted in practice to avoid dissonance. Thirteenth (C13): [Root, Major 3rd, Perfect 5th, Minor 7th, Major 9th, Perfect 11th, Major 13th] Effect: Extremely rich and complex, encompassing nearly the entire tonal spectrum.</p> <p>Altered Chords: Altered Dominant (C7♯5♭9): [Root, Major 3rd, Augmented 5th, Minor 7th, Minor 9th] Effect: Tense and dissonant, often used to create a strong sense of tension before resolving to the I chord in a jazz or classical context. Minor 7th ♭5 (Cm7♭5): [Root, Minor 3rd, Diminished 5th, Minor 7th] Effect: Somber and unresolved, commonly found in jazz and also known as the half-diminished seventh chord.</p> <p>Suspended Chords: Suspended 2nd (Csus2): [Root, Major 2nd, Perfect 5th] Effect: Open and airy, with a sense of suspension or anticipation. Suspended 4th (Csus4): [Root, Perfect 4th, Perfect 5th] Effect: Similarly open and unresolved, often used to delay the resolution to a major or minor chord.</p> <p>Added Tone Chords: Add9 (Cadd9): [Root, Major 3rd, Perfect 5th, Major 9th] Effect: Bright and open, with the ninth adding a melodic extension to the harmony.</p> <p>Polytonal and Polychordal Constructions: Polytonal Chord (C/G♭): [C Major chord + G♭ Major chord] Effect: Complex and dissonant, creating a sense of two tonalities clashing. Polychord (C/E♭m): [C Major chord over E♭ Minor chord] Effect: The combination of two different chords can create unusual and rich sonorities.</p> <p>Clusters and Non-Tertian Chords: Tone Cluster (C[C-D-E♭]): [Multiple adjacent notes played together] Effect: Highly dissonant and texturally dense, often used for dramatic effect. Quartal Chord (C Quartal): [Built in fourths, e.g., C-F-B♭-E♭] Effect: Open and modern sounding, often used in jazz and 20th-century classical music.</p> <p>Chords from Composers like Prokofiev, Scriabin, and Sorabji: Scriabin’s Mystic Chord: [Root, Augmented 4th, Augmented 4th, Major 3rd, Major 3rd] Effect: Otherworldly and enigmatic, central to Scriabin’s late harmonic language. Prokofiev’s “Suggestion Diabolique” Chords: Specific dissonant chords used for dramatic and unsettling effects in his works. Effect: Striking, with a sense of unease or instability. Sorabji’s Dense Polyphony and Micro-polyphony: Creates chords through the layering of many independent lines. Effect: Immersive and complex, contributing to a massive and sometimes overwhelming sound world.</p> <h2 id="scales">Scales</h2> <ul> <li>A collection of notes</li> <li>An interesting harmonic structure. A certain harmonic pattern.</li> <li>Degrees <ol> <li>Tonic</li> <li>Supertonic</li> <li>Mediant</li> <li>Subdominant</li> <li>Dominant</li> <li>Submediant</li> <li>Leading tone (half step from tonic so it “leads” to the tonic)</li> </ol> </li> <li>Symmetrical scales <ul> <li>Create atonal structure because notes are not distinguished from each other</li> <li>Examples <ul> <li>Chromatic</li> <li>Wholetone</li> <li>H/W diminished (1 2 1 2)</li> <li>W/H diminished (2 1 2 1)</li> <li>Augmented (3 1 3 1)</li> <li>Tritone scale (1 3 2 1 3 2)</li> <li>Two-semitone tritone scale (1 1 4 1 1 4)</li> </ul> </li> </ul> </li> </ul> <h2 id="modes">Modes</h2> <ul> <li>Just another scale</li> <li>Different root tone of a scale</li> <li>All possible diatonic scales</li> <li>All possible rotations of scale</li> <li>Every mode except locrian (♭5) is a rotation of the circle of fifths</li> <li>The distance between notes, particilarly in relation to the root is what sets the mood of modes</li> <li>The leading tone is important to create suspense between 7 and 1 but in Dorian etc, it is a whole step, so V chords don’t have such a pull to the root</li> <li>Modes of constant key <ul> <li>Take all notes with note in key signature</li> <li>Find the mode with key at root</li> <li>Mode extensions <ul> <li>Mega lydian := Lydian has the same notes as p5 scale</li> <li>Mega ? := ? has the same notes as p4 scale</li> <li>Super-Ultra-Hyper-Mega-Meta Lydian := Stacking lydians by progressing through the circle of fifths <ul> <li>Wraps itself around the brightest side of the scale</li> </ul> </li> <li>Super-locrian wraps itself around the darkest part of scales</li> <li>Gives insights to how to color chords</li> <li>Infinite wraps around the circle of fifths, considering the first four note modifications of each mode, and the last four as further extensions</li> <li>Moving modes across the circle of fifths creates a more major/minor feel</li> <li>Right is the dominant</li> <li>Left is the subdominant</li> <li>Modes without extensions create a flatter (relatively) scale. Like repeating a section of a mode extension.</li> </ul> </li> </ul> </li> <li>In terms of brightness <ol> <li>Lydian</li> <li>Ionian</li> <li>Mixolydian</li> <li>Dorian</li> <li>Aeolian</li> <li>Phrygian</li> <li>Locrian</li> </ol> </li> <li>Any scale that preserves the 5ths (not locrian) is a rotation of the circle of fifths. Non diatonic scales do not preserve the scale alterations</li> <li>Can you take any scale, move around circle of fifths and differ by only one note?</li> </ul> <h2 id="key">Key</h2> <ul> <li>A context. Establishes a tone which determines how consonant other tones are.</li> <li>Tonal + Scale</li> <li>Parallel key (CMaj, CMin) := Same root</li> <li>Relative key (CMaj, aMin) := Same notes</li> <li>The context of a musical piece</li> <li>Due to equal temperment (12 tone) tuning, the specific key does not matter. What does matter is the mode one is using of a certain key. Keys matter when not using 12 tone tuning like in olden days when music would be written for certain instruments.</li> <li>Use of a scale in a musical composition</li> <li>How a collection of notes (scale) relate or should be interpreted</li> <li>Scale around which music revolves</li> <li> <p>The specific name given to a scale</p> </li> <li>Key signatures <ul> <li>C♭ : B♭ E♭ A♭ D♭ G♭ C♭ F♭ <ul> <li>B is two fifths away from A which results in the step down</li> </ul> </li> <li>C♯ : F♯ C♯ G♯ D♯ A♯ E♯ B♯ <ul> <li>F is two sevenths away from G which results in the step up</li> </ul> </li> </ul> </li> </ul> <h2 id="tempo">Tempo</h2> <ul> <li>Rhythm</li> <li>Triplets</li> <li>Meter</li> <li>Time signature</li> <li>Polymeter</li> <li>Polyrhythm <ul> <li><a href="https://www.youtube.com/watch?v=32LcntJXywM">5:7 counting in 3s</a></li> <li><a href="https://www.youtube.com/watch?v=gFVMqf8HInY">Multiple polyrhythms</a></li> <li><a href="https://www.youtube.com/watch?v=D2vwk2Megtw">4:5</a></li> <li><a href="https://www.youtube.com/watch?v=0EUDu_3sAwU">5:6</a></li> <li><a href="https://www.youtube.com/watch?v=xYGJtiEzonU">2:3:5:7</a></li> <li><a href="https://www.youtube.com/watch?v=3Uenwo010dk">6:4</a></li> <li><a href="https://www.youtube.com/watch?v=pvs-cscggcA">7:11</a></li> </ul> </li> <li>Swing</li> </ul> <h2 id="tonality">Tonality</h2> <ul> <li>The harmonics describe keys that establish/break tonality. Earlier harmonics are more important for establishing tonality. 1 p5 M3 m7… The harmonics describe intervals that exhibit more consonance. The tonal center is the note that is most consonant with other intervals.</li> <li>Overtone series orders intervals by consonance</li> <li>Center piece of the scale that feels completely resolved and at rest</li> <li>Esablish a tonic by giving the note a central distinguish <ul> <li>Position/passage <ul> <li>Beggining/ending at tonic</li> </ul> </li> <li>Repeating the tonic</li> <li>Dynamic emphasis <ul> <li>playing with more power</li> </ul> </li> <li>Range emphasis <ul> <li>Play tonic as highest/lowest note</li> </ul> </li> <li>Rhythmic emphasis</li> <li>Bassline</li> </ul> </li> <li>Displacement from tonic introduces tension and moving back toward it resolves the tension</li> <li>Brightness <ul> <li>Opening vs closing</li> <li>Different modes have different brightness</li> <li>Relative to other scales</li> <li>Intervals in scales are a large driver of brightness <ul> <li>Ionian has a major</li> </ul> </li> <li>Larger steps tend to be brighter</li> </ul> </li> <li>V7 and vii∅7 are important for establishing tonality <ul> <li>Create dissonance</li> <li>Dominant contains tritone</li> <li>Diminshed contains two tritones</li> <li>Contain the 7</li> <li>Lead towards 1 and 5</li> <li>The root or 1 3 5 chord resolves the tension and establishes a home</li> </ul> </li> </ul> <h2 id="function">Function</h2> <ul> <li>The relationship of chord or scale degree to tonal center</li> <li>Theories <ul> <li>German theory by Hugo Riemann <ul> <li>Tonic</li> <li>Subdominant</li> <li>Dominant</li> </ul> </li> <li>Viennese theory <ul> <li>Roman numerals denote the chords of the tonal scale</li> </ul> </li> </ul> </li> <li>Harmony ? How does chords in each diatonic function differ to I <ul> <li>I vi iii function as the tonic <ul> <li>vi7 and iii7 only differ one note from the tonic</li> <li>All share the 3rd degree of the scale, centers the surrounding notes</li> <li>The 3 is very important in establishing a tonal center</li> </ul> </li> <li>ii IV function as the subdominant <ul> <li>Create instability but not in any particular direction</li> <li>Function differs depending on traid vs. 7 chord</li> <li>The 4 being a half step from 3 results in small direction towards root</li> </ul> </li> <li>V7 vii∅7 function as the dominant <ul> <li>Point back to tonic</li> <li>Both have the 2 and 7 of the scale, which encircle 1</li> <li>The 7 is only a half step from 1</li> <li>They also have 4 which is a half step from 3</li> </ul> </li> </ul> </li> <li>Cadence := How music is resolved <ul> <li>Authentic := V I</li> <li>Plagal := IV I</li> <li>Deceptive := V vi</li> </ul> </li> </ul> <h1 id="melody-1">Melody</h1> <ul> <li>Countermelodies</li> <li>Suspensions</li> <li>Passing notes</li> <li>Playing inside</li> <li>Improvising</li> <li>Creating melodies <ul> <li>Change the pitch</li> <li>Change the rhythm</li> <li>Add fills</li> <li>Add ornamentals trills/turns/mordents/grace notes</li> <li>Motion <ul> <li>Parallel</li> <li>Similar: Different intervals but same direction</li> <li>Oblique: One voice stays the other ascends/descends</li> <li>Contrary</li> </ul> </li> </ul> </li> <li>Story being told</li> <li>Music progresses overtime</li> <li>Intonation</li> <li>Embellishment</li> <li> <p>Ornamentation</p> </li> <li>Countermelodies <ul> <li>Focus on melody</li> <li>Multiple melodies</li> <li>Harmony happens to be whatever</li> <li>Counterpoint</li> </ul> </li> <li>From chords <ul> <li>Use only notes from chords (maybe inverted)</li> <li>Use notes from scale that chords represent</li> <li>Steps vs. Leaps</li> </ul> </li> <li>Play with expectation</li> <li>Sequence <ul> <li>Use same melodic shape but in different chords</li> <li>The final shape can be modified to give a sense of arrival</li> </ul> </li> <li>Anticipation <ul> <li>Play a note from the next chord before it arrives</li> </ul> </li> <li>Suspensions <ul> <li>When notes are held from previous chord</li> </ul> </li> <li>Passing notes <ul> <li>Notes to bridge two notes in chord</li> <li>Add a tension that quickly resolves</li> </ul> </li> <li>Accented passing note <ul> <li>Passing note on the beat</li> </ul> </li> <li>Playing inside <ul> <li>Chord tones/guide tones</li> <li>Available tensions</li> <li>Repeat notes</li> </ul> </li> <li>Improvising <ul> <li>Cliches and Quoting <ul> <li>Quoting involves using an existing melody over your chord progression</li> </ul> </li> <li>Polytonality <ul> <li>Side slipping <ul> <li>Shift harmony/melody up/down by some semitone</li> </ul> </li> <li>Bitonality <ul> <li>Play a scale other than the scale of the chord progression</li> </ul> </li> </ul> </li> <li>Riffs</li> <li>Passing notes (chromatics) <ul> <li>Target chord tones</li> <li>Create tension by creating tension by playing notes outside the harmony</li> </ul> </li> <li>Melody <ul> <li>Change the rhythm (keep the pitch)</li> <li>Change the pitch (keep the rhythm)</li> <li>Use fills over long notes and rests</li> </ul> </li> <li>Dissonant intervals <ul> <li>8 5 4</li> <li>♭3 ♭6 3 6</li> <li>♭2 ♭7 2 7 TT</li> </ul> </li> <li>Cycled patterns (non-functional) <ul> <li>Repeat a phrase at fixed intervals</li> </ul> </li> </ul> </li> </ul> <h1 id="harmony">Harmony</h1> <ul> <li>Available tensions</li> <li>Harmonization</li> <li>Reharmonization</li> <li>Progressions</li> <li>Chord voicing</li> <li>Passing chords</li> <li>Harmony as color theory <ul> <li>Musical harmony is like mixing colors. You start with basic primary colors, each has a clear, distinct, but simple effect. Adding notes adds colors. Adding too many colors becomes chaotic and can become muddy.</li> </ul> </li> </ul> <h1 id="realization">Realization</h1> <ul> <li>[Duality]</li> <li>[Structure (systems science)]</li> <li> <p>Melody and harmony</p> </li> <li>Think primarily the experience that sound provides</li> <li>Focus on the sounds (theory is just a language for describing the sounds)</li> <li>Ultimately music is something that is experienced. So all structure exists from the context of the listener. Theory exists to formalize these structures and isolate them.</li> <li> <p>Bring out notes that harmony revolves around. Eg bring out the descending baseline, etc.</p> </li> <li>The p4 of the major scale causes dissonance with m3, lydian, altough it it has a tritone, is generally more consonant than major chord. Intervals are named because of distance from root note, not position in any scale.</li> <li>Music is all about the experience, focus on the experience, don’t prematurely label anything (major chords are not happy and minor chords are not sad). It is ultimately contextual.</li> <li>Minimize tension in your playing but not your music</li> <li>Music is about adding dissonance and consannce to introduce tension and release it. This is the story of life. Without it life becomes more empty.</li> <li>Middle C is a pivot point <ul> <li>Playing closed harmony that is not balanced around middle C will feel unbalanced</li> </ul> </li> <li> <p>Avoid notes sound bad, chord tones sound good, tensions sound jazzy</p> </li> <li>Duality <ul> <li>P5 and P4 is like major and minor</li> <li>Chords consist of the unity, duality, and union <ul> <li>Unity := Center of chord</li> <li>Union := M3 away from unity</li> <li>Duality := P5 away from unity</li> </ul> </li> <li>A minor chord is a perfect 4th down</li> <li>Constructing a chord with 5ths, the first 3rd is major</li> <li>Constructing a chord with 4ths, the first 3rd is minor</li> </ul> </li> <li>Formalization <ul> <li>Embelishment are sheafs on manifold of the musical piece, each a different interpretation that evokes a different emotion.</li> <li>Chord voicing is a topos of sheafs on manifold of chords, each sheaf is a realization of chords</li> </ul> </li> <li>Structure <ul> <li>Harmony is the creation and divergence from structure</li> <li>Convergence to structure releases tension, brings you home. Diverging creates interest and tension.</li> <li>Diverging creates tension and interest</li> <li>Examples <ul> <li>Melodic phrase</li> <li>Scale</li> <li>Chord progression</li> <li>Functionality (structure) <ul> <li>Musical structure has a goal, ie: diatonic chords</li> </ul> </li> <li>Non-functional <ul> <li>Still just as harmonic as functional, just different structure, a different home to resolve to</li> <li>Non-functional harmony removes the long-term purpose or context of harmony. Each harmony is experienced only when it’s played, and it’s all that is experienced, until the piece continues. It evokes a timeless and aimless experience. Nihlism. You have no home so you are always lost or always home, a matter of perspective. The musician can help the listener choose by making each moment more or less enjoyable.</li> <li>Atonal</li> <li>Parallel chord motion</li> <li>Chromaticism</li> <li>Modes</li> <li>Exotic scales</li> <li>Voice-leading</li> </ul> </li> <li>Keys <ul> <li>Relative key has the sames notes but different root (C maj / G mixolydian)</li> <li>Parallel key has same root but different notes (maj/min)</li> </ul> </li> </ul> </li> <li>Art is made through the creation and resolution of tension</li> <li>Divergence <ul> <li>Tension notes := notes on edge of structure</li> <li>Playing melody before/after expected chord progression</li> <li>Cycled patterns := moving a piece of structure around</li> <li>Symmetric scales</li> <li>Dissonant intervals</li> <li>Avoid notes <ul> <li>Inherently dissonant, exits harmonic structure of chord</li> <li>Note that is one semitone about a chord tone</li> <li>C13 and CMaj13 omit 11 because it is an avoid note</li> <li>Avoid notes cause dissonant interval with one of the chord tones. They change the quality of the chord.</li> <li>Dominant has a tritone and so is inherently dissonant</li> <li>♭9 ♯9 for Maj7 and min7</li> <li>♮11</li> <li>♭13 for min7</li> </ul> </li> <li>Atonality <ul> <li>Symmetric scales</li> </ul> </li> </ul> </li> <li>Convergence <ul> <li>Guide tones <ul> <li>3 and 7 are guide tones and establish the chord quality. The 1 and 5 establish the key. 3 and 7 should be focused on when improvising the melody, they are the most consonant?.</li> <li>Progressions preserving the 3 or 7 are more smooth</li> </ul> </li> <li>Playing inside</li> <li>Riffs</li> <li>Scales</li> <li>Arpeggios</li> <li>Melody</li> <li>Quotes &amp; Cliches</li> <li>Common scale</li> <li>Shell chord</li> </ul> </li> <li>Movement of structure <ul> <li>Modal interchange</li> <li>Modulation</li> <li>Tonicization</li> </ul> </li> <li>Tension <ul> <li>Create tension by establishing structure flowing around it</li> <li>Resolution <ul> <li>Cadence</li> </ul> </li> </ul> </li> </ul> </li> </ul> <h1 id="timbre">Timbre</h1> <h1 id="rhythm">Rhythm</h1> <p>n:m := n beats subdivided by m, sound played by subdivision marked by number, _ represents no sounds played for that beat, p/ means the immediately following group is subdivided into p instead of m subdivide n beats by m, offset the sound by m+x or m-x (usually x=1) until it resolves</p> <p>4:4: 1 1 3 1 3:4: 134 24 134 3:3: 3 2 1 6:4: 1 2 3 4 _ 1 4:2: 1 1 12 3/3 3:4 13 124 1 4:4: 12 or 13 or 14 or 24 3:4: 13 (swing)</p> <h1 id="jazz">Jazz</h1> <h2 id="jazz-improvisation">Jazz Improvisation</h2> <ul> <li>Mistakes <ul> <li>Repeat the phrase &amp; emphasize mistake</li> <li>Repeat the phrase &amp; correct the mistake</li> <li>Turn the mistake into a chromatic run and resolve it</li> <li>Turn the mistake into a cycled pattern and resolve it</li> <li>Continue playing wrong notes and resolve it</li> </ul> </li> </ul> <h1 id="scales-1">Scales</h1> <p>Super Ultra Hyper Mega Meta Scales Metalydian is a symmetric pattern that does not repeat each octave (WWWH)</p> <p>Mix up scales:</p> <ul> <li>Mixolydian Lydian</li> <li>M3 m6 M7</li> <li>m3 M6 M7</li> <li>Phrygian M3</li> </ul> <h2 id="messiaen-modes">Messiaen modes</h2> <ol> <li>Whole tone, 2 transpositions, 1 mode</li> <li>Diminished, 3 transpositions, 2 modes</li> <li>WHHWHHWHH (1 2 ♭3 3 ♯4 5 6 ♭7), 3 transpositions, 2 modes</li> <li>H-H-3H-H-H-H-3H-H, 6 transpositions, 4 modes</li> <li>H-4H-H-H-4H-H, 6 transpositions, 3 modes</li> <li>W-W-H-H-W-W-H-H, 6 transpositions, 4 modes</li> <li>H-H-H-W-H-H-H-H-W-H, 6 transpositions, 5 modes</li> </ol> <p>Truncation: Removing notes from a mode to leave a truncated mode. The removed notes and remaining notes must preserve the symmetry of the parent mode. Mode 7 and 3 are not truncated modes, the other modes can be constructed from them or from one or more of their modes. Mode 7 contains 1 2 3 4 5 6, mode 6 contains 1 5, mode 4 contains 5, mode 3 contains 1.</p> <h2 id="5-note-scales">5 note scales</h2> <p>5 note scales are generally more thought of melodically, connecting various harmonic contexts. A collection of intervals that tie together various harmonies starting and ending harmonically arbitrarily, instead appealing to need. One is not so concerned with forming chords from these small scales, instead one should explore chords and their variations in their own right.</p> <p><strong>9th chord</strong>: 1 3 5 7 9</p> <p>Major pentatonic has 5 consecutive pitches from the circle of fifths transposed within the same octave.</p> <p><strong>Pentatonic</strong>: major: 1 2 3 5 6 (Lydian Ionian Mixolydian) blues major: 1 2 4 5 6 (Mixolydian Dorian Aeolian) suspended: 1 2 4 5 ♭7 (Dominant Dorian Aeolian) minor: 1 ♭3 4 5 ♭7 (Dorian Aeolian Phygian) blues minor: 1 ♭3 4 ♭6 ♭7 (Aeolian Phygian Locrian)</p> <p><strong>Hirajoshi</strong>: 1 3 ♯4 5 7 2W-W-H-2W-H (0,4,6,7,11) <strong>Miyako-bushi on D</strong>: 1 ♭2 4 5 ♭6 H-2W-W-H-2W (0,1,5,7,8) <strong>Insen</strong>: 1 ♭2 4 5 ♭7 H-2W-W-3H-W (0,1,5,7,10) <strong>Iwato</strong>: 1 ♭2 4 ♭5 ♭7 H-2W-H-2W-W (0,1,5,6,10)</p> <h2 id="6-note-scales">6 note scales</h2> <p><strong>Blues</strong>: [(b)] 1 ♭3 4 ♭5 5 ♭7 3H-W-H-H-3H-W (0,3,5,6,7,10) - Minor chord with chromatic passing note between 4 and 5 and no 2 or 6 (to avoid clashing with 3 and 5) <strong>Augmented</strong>: 1 ♭3 3 5 ♯5 7 3H-H-3H-H-3H-H (0,3,4,7,8,11) - 3 Major chords and 3 Minor chords <strong>Tritone</strong>: [(+)♭7♭9♯11] 1 ♭2 3 ♭5 5 ♭7 H-3H-W-H-3H-W (0,1,4,6,7,10) - Two major chords a tritone away <strong>Whole tone</strong>: 1 2 3 ♯4 ♯5 ♯6 W-W-W-W-W-W (0,2,4,6,8,10) - also: 1 2 3 ♭5 ♯5 ♭7 - similar to augmented scale <strong>Scale of harmonics</strong>: 1 ♭3 3 4 5 6 3H-H-H-W-W-3H (0,3,4,5,7,9) <strong>Istrian</strong>: 1 ♭2 ♭3 ♭4 ♭5 5 H-W-H-W-H-5H (0,1,3,4,6,7) <strong>Prometheus</strong>: 1 2 3 ♯4 6 ♭7 W-W-W-3H-H-W (0,2,4,6,9,10) <strong>Two-semitone tritone</strong>: 1 ♭2 ♮2 ♯4 5 ♭6 H-H-4H-H-H-4H (0,1,2,6,7,8)</p> <h2 id="major">Major</h2> <p><strong>Lydian</strong>: [(+)♯11] 1 2 3 ♯4 5 6 7 W-W-W-H-W-W-H (0,2,4,6,7,9,11) - I II iii ♯iv⁰ V vi vii Lydian augmented: [(+)♯5♯11] 1 2 3 ♯4 ♯5 6 7 W-W-W-W-H-W-H (0,2,4,6,8,9,11) Lydian dominant: [(+)♭7♯11] 1 2 3 ♯4 5 6 ♭7 W-W-W-H-W-W-H (0,2,4,6,7,9,10) <strong>Ionian</strong>: [(+)] 1 2 3 4 5 6 7 W-W-H-W-W-W-H (0,2,4,5,7,9,11) - I ii iii IV V vi vii⁰ - Consonant with first 6 harmonics (the seventh harmonic is a minor seventh) <strong>Mixolydian</strong>: [(♭7)] 1 2 3 4 5 6 ♭7 W-W-H-W-W-H-W (0,2,4,5,7,9,10) - I ii iii⁰ IV v vi ♭VII - Becuase there is only ♭7, only every other chord is changed (1, 2, 4, 6) stay the same because they don’t involve 7 - Consonant with first 10 harmonics (the 11th harmonic is a tritone) - Flat 7 gives less direction - Chords are less resolved - More relaxed and ambiguous - Straying from root gives less tension <strong>Mixolydian ♭6</strong>: [(♭7)♭6], [(-)♯3] 1 2 3 4 5 ♭6 ♭7 W-W-H-W-W-H-W (0,2,4,5,7,8,10) <strong>Dorian</strong>: [(-)♯13] 1 2 ♭3 4 5 6 ♭7 W-H-W-W-W-H-W (0,2,3,5,7,9,10) - i ii ♭III IV v vi⁰ ♭VII - Step from 3 to 4 is larger and increases brightness <strong>Aeolian</strong>: [(-)] 1 2 ♭3 4 5 ♭6 ♭7 W-H-W-W-H-W-W (0,2,3,5,7,8,10) - i ii⁰ ♭III iv v ♭VI ♭VII - Relative natural minor - Lowest interval that respects the circle of fifths <strong>Aeolian ♭5</strong>: [(♭5)] 1 2 ♭3 4 ♭5 ♭6 ♭7 <strong>Phrygian</strong>: [(-)♭9] 1 ♭2 ♭3 4 5 ♭6 ♭7 H-W-W-W-H-W-W (0,1,3,5,7,8,10) - i ♭II ♭III iv v⁰ ♭VI ♭vii - Notes are fairly same distance which leads to a slippery slope towards the root <strong>Locrian</strong>: [(♭5)♭9] 1 ♭2 ♭3 4 ♭5 ♭6 ♭7 H-W-W-H-W-W-W (0,1,3,5,6,8,10) - i⁰ ♭II ♭iii iv ♭V ♭VI ♭vii - The two halfs before the 5 - Doesn’t respect circle of fifths - Tritone between the 1 and 5 <strong>Locrian ♯2</strong>: [(♭5)] 1 2 ♭3 4 ♭5 ♭6 ♭7 W-H-W-H-W-W-W (0,2,3,5,6,8,10) <strong>Super locrian</strong>: [(♭5)♭9♭11] 1 ♭2 ♭3 ♭4 ♭5 ♭6 ♭7 H-W-H-W-W-W-H (0,1,3,4,6,8,10) <strong>Major Locrian</strong>: [(♭5)♯3], [(+)♭5♭7♭13] 1 2 3 4 ♭5 ♭6 ♭7 W-W-H-H-W-W-W (0,2,4,5,6,8,10)</p> <p><strong>Melodic minor (Jazz Minor)</strong>: [(+)♭3] 1 2 ♭3 4 5 6 7 W-H-W-W-W-W-H (0,2,3,5,7,9,11) <strong>Acoustic scale</strong>: 1 2 3 ♯4 5 6 ♭7</p> <ul> <li>Minor := 1-2.3-4-5.6-7-8 <ul> <li>Natural := ♭3 ♭6 ♭7 <ul> <li>Whole step between 7 and 8 didn’t provide the right tension</li> </ul> </li> <li>Harmonic := ♭3 ♭6 <ul> <li>3 half steps between 6 and 7</li> </ul> </li> <li>Melodic := ♭3 <ul> <li>1 whole step between 6 and 7</li> </ul> </li> </ul> </li> </ul> <h2 id="harmonic">Harmonic</h2> <p><strong>Harmonic minor</strong>: <strong>Ukrainian Dorian</strong>: 1 2 ♭3 ♯4 5 6 ♭7 W-H-3H-H-W-H-W (0,2,3,6,7,9,10) <strong>Harmonic minor</strong>: 1 2 ♭3 4 5 ♭6 7 W-H-W-W-H-3H-H (0,2,3,5,7,8,11) <strong>Phrygian dominant</strong>: 1 ♭2 3 4 5 ♭6 ♭7 H-3H-H-W-H-W-W (0,1,4,5,7,8,10)</p> <p><strong>Double harmonic major</strong>: 1. Double harmonic major: 1 ♭2 3 4 5 ♭6 7 H-3H-H-W-H-3H-H (0,1,4,5,7,8,11) 4. Hungarian Gypsy minor: 1 2 ♭3 ♯4 5 ♭6 7 W-H-3H-H-H-3H-H (0,2,3,6,7,8,11)</p> <p><strong>Harmonic major</strong>: 1 2 3 4 5 ♭6 7 W-W-H-W-H-3H-H (0,2,4,5,7,8,11)</p> <h2 id="others">Others</h2> <p>Neapolitan major: 1 ♭2 ♭3 4 5 6 7 H-W-W-W-W-W-H (0,1,3,5,7,9,11) - Major chord built from flat 2 of major scale Neapolitan minor: 1 ♭2 ♭3 4 5 ♭6 7 H-W-W-W-H-3H-H (0,1,3,5,7,8,11) Chromatic: 1 ♯1 2 ♯2 3 4 ♯4 5 ♯5 6 ♯6 7 | 7 ♭7 6 ♭6 5 ♭5 4 3 ♭3 2 ♭2 1 H-H-H-H-H-H-H-H-H-H-H-H (0,1,2,3,4,5,6,7,8,9,10,11) Half diminished: 1 2 ♭3 4 ♭5 ♭6 ♭7 W-H-W-H-W-W-W (0,2,3,5,6,8,10)</p> <p>Blues 7 note: 1 ♭3 3 4 ♭5 5 ♭7 Major bebop: 1 2 3 4 5 (♯5/♭6) 6 7 W-W-H-W-(H-H)-W-H (0,2,4,5,7,(8),9,11) - Derived by adding chromatic passing notes from major scale modes Bebop dominant: 1 2 3 4 5 6 ♭7 7 W-W-H-W-W-H-H-H (0,2,4,5,7,9,10,11)</p> <p>Algerian: 1 2 ♭3 ♯4 5 ♭6 7 etc. W-H-3H-H-H-3H-H-W-H-W (0,2,3,6,7,9,11,12,14,15,17) Vietnamese of harmonics: 1 three quarter flat3 ♭3 ♮3 4 5 5Q-Q-H-H-W (0,5/2,3,4,5,7) Acoustic: 1 2 3 ♯4 5 6 ♭7 W-W-W-H-W-H-W (0,2,4,6,7,9,10) Octatonics: 1 2 ♭3 4 ♭5 ♭6 6 7 W-H-W-H-W-H-W-H (0,2,3,5,6,8,9,11) | 1 ♭2 ♭3 3 ♯4 5 6 ♭7 H-W-H-W-H-W-H-W (0,1,3,4,6,7,9,10)</p> <p>Hungarian major: 1 ♯2 3 ♯4 5 6 ♭7 3H-H-W-H-W-H-W (0,3,4,6,7,9,10) Enigmatic: 1 ♭2 3 ♯4 ♯5 ♯6 7 H-3H-W-W-W-H-H (0,1,4,6,8,10,11) Gypsy: 1 2 ♭3 ♯4 5 ♭6 ♭7 W-H-3H-H-H-W-W (0,2,3,6,7,8,10) Flamenco: 1 ♭2 3 4 5 ♭6 7 H-3H-H-W-H-3H-H (0,1,4,5,7,8,11) Persian: 1 ♭2 3 4 ♭5 ♭6 7 H-3H-H-H-W-3H-H (0,1,4,5,6,8,11)</p>]]></content><author><name></name></author><summary type="html"><![CDATA[A big thing about voicing chords is where to distribute the tension Passing note Chord voicing Chord proression Passing chord Chord substitutions Voice leading Combines horizontal and vertical harmony.]]></summary></entry><entry><title type="html">Neuroscience</title><link href="https://t-wilkinson.github.io/blog/2024/neuroscience/" rel="alternate" type="text/html" title="Neuroscience"/><published>2024-07-23T16:36:24+00:00</published><updated>2024-07-23T16:36:24+00:00</updated><id>https://t-wilkinson.github.io/blog/2024/neuroscience</id><content type="html" xml:base="https://t-wilkinson.github.io/blog/2024/neuroscience/"><![CDATA[<ul> <li>Index <ul> <li>Cortical columns</li> <li>Grid cell</li> <li>Brain anatomy</li> <li>Memory formation</li> <li>Microcircuit of neuroanatomy</li> <li>Neural circuitry</li> <li>Sleep and consciousness</li> <li>Neurological research</li> <li>Branches <ul> <li>Computational neuroscience</li> <li>Cognitive neuroscience</li> <li>Neuromorphic computing</li> <li>Systems Neuroscience <ul> <li>Study structure and functions of neural circuits</li> </ul> </li> </ul> </li> <li>Default mode network</li> </ul> </li> </ul> <p>nucleus accumbens, septal nuclei, prefrontal cortex, lateral hypothalamus, nucleus reuniens, mammillary nuclei, entorhinal cortex and amygdala. anterior thalamic nuclei hippocampus subiculum presubiculum parasubiculum postsubiculum ACC Palladin Substantia nigra Limbic Thalamus Anterodorsal nucleus Anteroventral nucleus Anteromedial nucleus</p> <p>How do inhibitory neurons strengthen connections? Synaptic plasticity.</p> <p>? How do high IQ individuals brains differ</p> <p>Each neural network in the brain establishes a equivalence class between neural representations and the output of that area.</p> <p>Neuroscience is the study of the nervous system and its functions. Sub-fields of neuroscience deal with understanding the nervous system under different contexts such as diesease, toxicity, cognition, etc.</p> <p>Neuroscience is data rich, theory poor. There is a large base of constraints for a theory, but no theory which satisfies those constraints.</p> <p>Hebbian theory (neurons that fire together wire together)</p> <ul> <li>Nervous System</li> <li>Default mode network: capacity for self awareness <ul> <li>People who are depressed have higher level of default mode network</li> <li>Meta level self-observation to improve overall chances of sucess</li> <li>Ketamine shuts off the default mode network</li> <li>Psychedelics fracture your sense of self so there is nothing to reflect on</li> </ul> </li> </ul> <p>Emotions provide efficient contextual sparse networks. Default nmode neetwork wnables self reflection, a power tool for entities acting within an environment. The brain functions on sparse networks. They increase the parameters while reducing computation requirments (the brain uses a lot of energy and resources, another example of how the brain is evolutionary driven). Emotions are an example of sparse networks/modules. They efficiently encode responses to different internal and external environments (relative to the experience of the being).</p> <p>There are interesting similarities between some corical circuits and wave function collapse in quantum mechanics. In particular, the thalamus and neocortex have combinations of stellate and pyramidal neurons. Pyramidal neurons look to identify combinations or contexts of patterns and then inhibit surrounding neurons through stellate neurons. The states that each pyramidal neuron represents attempts to model a collection of states that some specific circuitry is receiving. If this is true, then some quantum mechanics equations could be used to model the brain and quantum computers useful at representing more complex cortical circuits such as cortical columns.</p> <p>Just like how the occipital lobe focuses on visual input. The frontal cortex focuses on executive function. An artificial brain should have some architectural design promoting the ability to create goals and use those goals to change the context of information being processed.</p> <p>To work like the human brain, although the hidden state encodes working memory, there needs to be a way to actually train the model based on working memory in between tasks (sleep).</p> <p>The path to AGI will take understanding what functional units do of each part of the brain, and finding the best computational implementation for those.</p> <p>To encode focus, one might direct the output of the focus module directly to attention modules.</p> <p>Transformers treat each word equally, which has high computational costs for larger sequences. What about somehow only careing about some encoded representation of whole sentences, and paying attention to a couple of words.</p> <p>Look more at recent papers of attention models.</p> <h1 id="questions">Questions</h1> <ul> <li>How does the brain decide what to build models of the world? Especially given it already has some.</li> <li>There is this contextual executive functioning which is controlling attention. Conciousness is some awareness of sensory input. The constant reciprical path of information being controlled by attention and that information having an effect on attention must be conciousness. The neocortex is a computer that is constantly updating itself, optimizing itself in order to then control how the body interacts with the world. Are people without thalamuses concious? What injueries tyo the brain creates a decreasee in what we consider consciousness? Awareness comes from this reciprical passing of information. People aren’t directly aware of the models their brain is building, but instead the effects of those models on information passing through.</li> <li>Grid cells in the entrohinal cortex form a hexagonal coordinate system. Why this instead of cublical like 2d euclidean plane? Well we usually need to represent 3 dimensions.</li> <li>Context is important as it provides some commonality and context in which to derive meaning to compare objects. Where is the meaning of this context encoded in the brain? Each is formed from different input information. This information is used to perform computations on information that eventually lead to actions in the world.</li> <li>The prefrontal cortex stores highly abstract information which has a long term effect on the organism. In order to see these effects a lot of emprical data must be collected.</li> <li>I believe hexagons are used because hexagons are the most efficient way to tile a plane. Therefore fewer neurons need to be used! Coordinate systems themselves don’t matter much. Because, as you learn in differential geometry there is a differnetial map between any two coordinate systems.</li> <li>Graphs are a natural data structure for how the brain represents information. But what about topoi?</li> <li>Path integration is accumulating various paths to determine where the final location is.</li> <li>Latent space is important here.</li> <li>The more an animal interacts in a space, the more data it has and more accurate it can build maps of the space it was interacting in. This knowledge does not necessarily transfer to hidden areas.</li> <li>The thalamic-cortical circuits has a way of quickly changing the context of the brain through firing and bursting mode. Is self-attention similar?</li> </ul> <h1 id="cognitive-neuroscience">Cognitive neuroscience</h1> <p>Attention occurs at the various hierarchies of cognitive organization. Neurotransmitters can quickly create or temporary or prolonged attention through further sparse activations of neural networks.</p> <p>The brain continuously learns, potentially through sparse activation networks which each learn. The brain doesn’t learn from backpropogation.</p> <p>Sparse networks enable faster inferencing and much lower energy expenditure. By contextualizing inputs, networks specialized for each context only operate. Greatly increases representational capacity without increasing computational costs.</p> <p>Almost always, there are only 2% active cells at anytime. Sparse population codes. Generally, sparser activations implies a more confident understanding of the model being formed. Brain is very robust. Sparse representation is robust. Doesnt have issues woth GAN. 95% of synapses are predictive. Inhibit other neurons. Detect patterns. Neurons in deep learning only capture a portion of functionality of neurons in the brain.</p> <h2 id="important-components">Important components</h2> <p>Pyramidal neuron + Stellate neuron Grid cell and place cell Sparse network Cortical column Voting</p> <h2 id="neural-building-blocks">Neural building blocks</h2> <p>Head-direction cells Fire when an animal is facing a particular direction, anchored by main axis of the environment. Object-vector cells Grid cells Grid cells that represent location of object in environment Displacement cells Memory nodes</p> <h2 id="further-reading">Further reading</h2> <ul> <li>#neuroscience#numenta# [Hippocampal Spatial Mapping as Fast Graph Learning]{publisher=Numenta; ref=./resources/reading/2107.00567.pdf;}</li> </ul> <h1 id="research">Research</h1> <p>One common property to overcome when studying neurons is measuring information on functioning neurons. There is a tradeoff between high data collection but incomplete environment, or low data collection but complete environment. The development of transgenic methods can</p> <h1 id="trivia">Trivia</h1> <ul> <li>Human memory ranges from 1 to 1,0000 terabytes (possibly depending on recall and overall compression of knowledge, and efficiency of storage)</li> <li>Synaptic cleft is 20 nanometers</li> <li>Hydrogen atom is ~0.1 to 10 nano meters</li> <li>Typical neuron fires 5-20 times every second. This is likely the equilibrium point where if neuron fires outside of that range, it is removed/made less active</li> <li>Human hair is 17μm to 181μm</li> <li>100 billion neurons (and many more neuroglia which serve and protect the neurons)</li> <li>Neurons can be connected to up to 10,000 other neurons</li> <li>Up to 1,000 trillion synapses</li> <li>Therefore, on average, each neuron is connect to t10 synapses</li> <li>2% of neurons are active at any point in time</li> </ul> <h1 id="neuroplasticity">Neuroplasticity</h1> <ul> <li> <table> <tbody> <tr> <td>[How to Focus to Change Your Brain</td> <td>Huberman Lab Podcast #6](https://www.youtube.com/watch?v=LG53Vxum0as)</td> </tr> </tbody> </table> <ul> <li>Neuroplasticity requires focus on the area of interest</li> <li>Brains 0-25 are very neuroplastic</li> </ul> </li> <li>Must be alert</li> <li>Short vs. Medium vs. Long term</li> <li>Focus and arousal triggers neuroplasticity</li> <li>Neuroplasticity occurs during deep rest</li> <li>Focus <ul> <li>Duration Path Outcome ? What is focus</li> </ul> </li> <li>Focus on the neural structure in charge of what your trying to change, then envision how it should change to accomodate goals you are trying to accomplish it. Then in life try to apply it. Repeat so you don’t forget.</li> <li>Top-down vs. Bottom-up (Deliberate vs. Reflexive) <ul> <li>Impulsivity is lack of top-down control</li> </ul> </li> <li>Brain can change itself <ul> <li>Neurons that aren’t used for semantic processes (no nose to smell, lack of sight, etc), are re-purposed for other senses.</li> </ul> </li> <li>Make deliberate actions reflexive</li> <li>Steps <ul> <li>Focus on behaviour to change releases epinephrine to the area of focus</li> <li>Acetylcholine is released and binds to active neurons</li> <li>During sleep and deep rest (not Duration Path Outcome) neurons binded to acetylcholine are modified (strengthed axons)</li> </ul> </li> </ul> <h1 id="neuroimaging">Neuroimaging</h1> <p>Way to study a brain healthy or not, in a non-invasive manner.</p> <h1 id="glossary">Glossary</h1> <ul> <li>Receptive field := stimuli space which evokes a neuronal response. Some neurons receive a better response from a subset of their receptive field.</li> <li>Cortex := bark</li> <li>LGN := Lateral geniculate nucleus <ul> <li>Geniculate := bent at a sharp angle</li> <li>Looks like a knee</li> </ul> </li> <li>Neuronal tuning := Neuron responds best to a subset of its receptive field</li> <li>Path integration == Motor efference copy <ul> <li>Converting motor commands into movements in internal model of space</li> <li>Works at any location</li> <li>Can predict how actions will affect its spatial position</li> </ul> </li> <li>Brodmann area <ul> <li>Defined by cytoarchitecture (the cellular composition of an area)</li> <li>Anatomical location</li> <li>Roughly correspond to function implying that gyri play a role in separating function</li> </ul> </li> </ul> <h1 id="computational-neuroscience">Computational neuroscience</h1> <ul> <li>Single-neuron modeling</li> <li>Modeling neuron-glia interactions</li> <li>Development, axonal patterning, and guidance</li> <li>Sensory processing</li> <li>Motor control</li> <li>Memory and synaptic plasticity</li> <li>Behaviors of networks</li> <li>Visual attention, identification, and categorization</li> <li>Cognition, discrimination, and learning</li> <li>Consciousness</li> <li>Computational clinical neuroscience</li> <li>Predictive computation neuroscience</li> <li> <p>Computational psychiatry</p> </li> <li>Properties of neurons <ul> <li>axons and dendrites: <ul> <li>Stellate and pyramidal cells receive 1000s of synaptic inputs</li> <li>Purkinje cells in the cerebellum receive 100,000s of synaptic inputs</li> </ul> </li> <li>ion chanels: <ul> <li>Na⁺ K⁺ Ca²⁺ Cl⁻</li> </ul> </li> <li>membrane potential: <ul> <li>The electrical signal of relevance to neurocircuitry the difference of electrical potential between the interior of a neuron and extracullular medium.</li> <li>Neurons resting membrane potential is -70mV</li> <li>Na⁺ is more concentrated outside a neuron</li> <li>K⁺ is more concentrated inside a neuron</li> </ul> </li> <li>polarization: <ul> <li>hyperpolarization: <ul> <li>When neuron membrane potential becomes more negative.</li> </ul> </li> <li>depolarization: <ul> <li>When neuron membrane potential becomes more positive.</li> </ul> </li> </ul> </li> <li>action potential: <ul> <li>Axons terminate at synapses where the voltage transient of the action potential opens ion channels, producing and influx of Ca²⁺ that leads to the release of a neurotransmitter.</li> <li>When membrane potential reaches a certain value above -70mV, the neuron “fires”. Voltage gated ion channels open allowing the membrane potential to normalize with the extracellular space.</li> </ul> </li> <li>refractory period: <ul> <li>For a few milliseconds after action potential, call an absolute refractory period, it may be virtually impossible for a neuron to fire. After this period, there is a relative refractory period lasting 10s of milliseconds which inhibits the neuron from firing.</li> </ul> </li> </ul> </li> <li>Recording neuronal responses <ul> <li>sharp and patch elecrodes</li> <li>extracellular electrodes</li> </ul> </li> <li>From stimulus to response <ul> <li>Neuronal responses reflect the complex intrinsic dynamics of the neuron and the temperal charactersitics of the stimulus.</li> <li>Predicting neuronal stimulus response is potentially non-deterministic due to large variability in conditions and the apparent randomness of biophysics processes. As such, a statistical model is developed. Additionally, many neurons respond to the same stimulus, each responding differently to extract different features. As such, a population of neurons must be considered, and a resulting statistical model developed.</li> </ul> </li> </ul> <p>Mathematics for computational neuroscience: Linear algebra: eigenvectors, eigenvalues, functional analogs, vector and matrix operations, convolutions, linear filters, fourier transforms Finding extrema and lagrange multipliers: lagrange multiplier Differential equations: fixed point, attractors, jacobian matrix, taylor equations Electrical circuits: ohms law, kirchhoffs laws Probability theory: random variable, independence, continuous random variable</p> <h1 id="resources">Resources</h1> <ul> <li>#cognitive-science# [The Thalamus as a Monitor of Motor Outputs]{ref=./resources/reading/12626014.pdf}</li> <li>[Anterior thalamic circuits crucial for working memory]{subject=Proc. Natl. Acad. Sci. U.S.A. 2022.119:e2118712119; ref=./resources/reading/roy-et-al-2022-anterior-thalamic-circuits-crucial-for-working-memory.pdf}</li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[Index Cortical columns Grid cell Brain anatomy Memory formation Microcircuit of neuroanatomy Neural circuitry Sleep and consciousness Neurological research Branches Computational neuroscience Cognitive neuroscience Neuromorphic computing Systems Neuroscience Study structure and functions of neural circuits Default mode network]]></summary></entry><entry><title type="html">Homotopy Type Theory</title><link href="https://t-wilkinson.github.io/blog/2024/homotopy-type-theory/" rel="alternate" type="text/html" title="Homotopy Type Theory"/><published>2024-07-23T16:29:50+00:00</published><updated>2024-07-23T16:29:50+00:00</updated><id>https://t-wilkinson.github.io/blog/2024/homotopy-type-theory</id><content type="html" xml:base="https://t-wilkinson.github.io/blog/2024/homotopy-type-theory/"><![CDATA[<table> <thead> <tr> <th>Types</th> <th>Logic</th> <th>Sets</th> <th>Homotopy</th> <th> </th> </tr> </thead> <tbody> <tr> <td>A</td> <td>proposition</td> <td>set</td> <td>space</td> <td> </td> </tr> <tr> <td>a : A</td> <td>proof</td> <td>element</td> <td>point</td> <td> </td> </tr> <tr> <td>B(x)</td> <td>predicate</td> <td>family of sets</td> <td>fibration</td> <td> </td> </tr> <tr> <td>b(x) : B(x)</td> <td>conditional proof</td> <td>family of elements</td> <td>section</td> <td> </td> </tr> <tr> <td>0, 1</td> <td>⊥, ⊤</td> <td>∅, {∅}</td> <td>∅, ∗</td> <td> </td> </tr> <tr> <td>A + B</td> <td>A ∨ B</td> <td>disjoint union</td> <td>coproduct</td> <td> </td> </tr> <tr> <td>A × B</td> <td>A ∧ B</td> <td>set of pairs</td> <td>product space</td> <td> </td> </tr> <tr> <td>A → B</td> <td>A ⇒ B</td> <td>set of functions</td> <td>function space</td> <td> </td> </tr> <tr> <td>∑(x:A) B(x)</td> <td>∃x:A B(x)</td> <td>disjoint sum</td> <td>total space</td> <td> </td> </tr> <tr> <td>∏(x:A) B(x)</td> <td>∀x:A B(x)</td> <td>product</td> <td>space of sections</td> <td> </td> </tr> <tr> <td>Id_A</td> <td>equality =</td> <td>{ (x, x)</td> <td>x ∈ A }</td> <td>path space A^I</td> </tr> </tbody> </table> <p>What are identity types, path space objects, etc.?</p> <p>Homotopy type theory provides a looser but much more descriptive notion of equality.</p> <p>Homotopy type theory is a flavor of intensional dependent type theory. Intensional type theory means identity types can have complex structure, representing (∞,1)-categorical semantics therefore representing more nuanced forms of equality. Viewing types as type families, each term is assigned a space.</p> <p>In contrast to homotopy theory, homotopy type theory provides a synthetics description of points and paths as opposed to the analytic description.</p> <p>? How does one abstract statements to say, fields? What does a statement using assumptions look like in a (∞,1)-topos. A field, for example, is a collection of axioms, or function types. Take addition, we have $∏_{x,y: 𝔽} x+y∈𝔽$. We can then label a Field as the collection of axioms . $+$ and $×$ are type constructors of $𝔽$.</p> <p>∞-topos is like the ∞-category of sheaves on a space. Homotopy type theory provides semantics for type theory via ∞-topos. (∞,1)-internal category amounts to doing homotopy theory. Transport. Theorem proovers.</p> <p>The path object and identity type and 1-morphisms allow mapping the categorical morphism structure to the underlying infinity groupoid structure. By viewing automorphisms and similar on a type, we can view the higher equivalences as data on how the internal elements relate.</p> <p>The identity type is the collection of paths from a point to itself.</p> <p>Homotopy information encodes how underlying points relate to each other.</p> <p>In HoTT, we have path space objects which represent the collection of morphisms between two types. This can be interpreted as a collection of embeddings.</p> <ul> <li>Equiválence principle</li> <li>Type theory</li> <li>Univalence axiom</li> <li>Identity type</li> <li>Type family</li> <li>Recursion</li> <li>Induction</li> <li>Propositions as types</li> <li>∞-groupoid</li> <li> <p>Fibrations</p> </li> <li>Important concepts <ul> <li>∞-groupoid</li> <li>∞-sheaf</li> <li>(∞,1)-site</li> </ul> </li> <li>Higher category theory <ul> <li>(∞,1)-topos <ul> <li>Homotopy type theory can be viewed as an ∞-topos where 1-cells are morphisms. Higher morphisms are type equivalences. Objects are types. Identity types are types of objects whose containment of elements implies proofs of existence.</li> </ul> </li> <li>(∞,1)-category <ul> <li>model category</li> <li>Has as objects <ul> <li>∞-groupoid</li> <li>Fundamental groupoid maps topological spaces to ∞-groupoid. According to the homotopy hypothesis, this functor is an isomorphism.</li> <li>Can one recapture the notion of an open set from the ∞-groupoid?</li> </ul> </li> </ul> </li> <li>∞-groupoid <ul> <li>Equivalently (∞,0)-category</li> <li>Express notion of (homotopy) space</li> <li>Morphisms at each level form a groupoid structure, with identity, composition, and inverse operations which are weak in the sense that they satisfy groupoid laws only up to morphisms at the next level, which gives rise in further structure.</li> <li>Every topological space has a fundamental ∞-groupoid</li> </ul> </li> </ul> </li> <li>Fundamental concepts <ul> <li>Martin-Löf’s intensional type theory</li> <li>Dependent type theory <ul> <li>Locally cartesian closed categories</li> <li>Slice category</li> </ul> </li> <li>Homotopy Type Theory <ul> <li>Homotopy types</li> <li>Identity type</li> </ul> </li> <li>Topos theory <ul> <li>Sheaves</li> </ul> </li> <li>Category theory <ul> <li>Fibration</li> <li>Bundle</li> <li>Section</li> </ul> </li> <li>Computational aspect <ul> <li>~/dev/UniMath/UniMath/UniMath</li> </ul> </li> </ul> </li> <li>Martin Löf Intuinistic Type Theory <ul> <li>Intuinistic meaning constructive</li> <li>The identity type is fundamental but why?</li> <li>Identity type is abiguous in that</li> <li>A question arises of how to interpret types in order to know how points can be identified with each other. In the set theoretical intepretation which is accomplished by assuming the K axiom. In the homotopy interpretation, types are viewed as ∞-groupoids which have complex higher structure which leads to points being equivalent in multiple ways.</li> </ul> </li> <li>Truncation <ul> <li>My guess is that truncating is assuming types above some level who are equivalent to be the same type at the level of truncation.</li> </ul> </li> <li>Particle physics <ul> <li>Lie group <ul> <li>A group that is also a differentiable manifold</li> <li>SU(n) <ul> <li>Group of isometries of n-dimensional complex hilbert space which preserve the volume form of this space.</li> <li>Unitary matrix preserves norms (probability amplitudes)</li> <li>SU(2) is diffeomorphic to the 3-sphere</li> <li>n-sphere is $∑ⁿ(xᵢ-Cᵢ)²=1$</li> </ul> </li> </ul> </li> </ul> </li> <li>Homotopy type theory <ul> <li>(∞,1)-toposes</li> <li>Serves as the internal language for (∞,1)-toposes</li> </ul> </li> <li>https://ncatlab.org/nlab/show/categorical+semantics</li> <li>https://ncatlab.org/nlab/show/theory</li> <li>https://ncatlab.org/nlab/show/model</li> <li> <p>https://ncatlab.org/nlab/show/doctrine</p> </li> <li>Type theory</li> <li>(Higher) inductive types</li> <li>Model categories and model structure on simplicial sets</li> <li> <p>(∞,1)-sheafs</p> </li> <li>Category theory and physics <ul> <li>Topology <ul> <li>Groupoid</li> <li>Continuity</li> </ul> </li> <li>Categoric models of type theory <ul> <li>Identity type</li> <li>Model theory</li> </ul> </li> <li>Geometry <ul> <li>(∞,1)-category</li> <li><a href="https://ncatlab.org/nlab/show/geometry">geometry</a></li> <li>sheaf, stack, locale, site, topos</li> <li>stack</li> <li>cohomology</li> <li>Cohesive (∞,1)-topos</li> <li>local (∞,1)-topos</li> <li>Descent</li> <li>Higher geometry</li> <li>Geometric homotopy type</li> <li>∞-stacks <ul> <li>Higher geometry</li> </ul> </li> </ul> </li> <li>Internalization <ul> <li>Ambient category <ul> <li>Category which has properties suffient for internal structures. <ul> <li>Finite products is enough for groups</li> <li>(usually finite) limits is enough for algebraic structures. Algebraic structures are defined by operations and equational axioms.</li> <li>Pullpacks are sufficient for a strict category</li> </ul> </li> </ul> </li> <li>Ambient categories can be further endowed with structure to internalize mathematical objects such as fields, modules, rings, topological spaces, etc. In fact, as all mathematics is written in the language of logic, internalizing logic means internalizing all of mathematics.</li> <li>Internal category</li> <li>Internal logic <ul> <li>Morphism <ul> <li>$f : B → A$ == $x : B ⊢ f(x) : A$</li> <li>$f(x) : A$ is a term</li> </ul> </li> </ul> </li> <li>Internal logic of (∞,1)-topos</li> </ul> </li> <li>Type theory <ul> <li>Judgements</li> </ul> </li> <li>Homotopy theory <ul> <li>Simplicial complex</li> <li>Kan complex</li> <li>Groupoid <ul> <li>Homotopy hypothesis <ul> <li>Conjecture formulated by Alexander Grothendieck states that ∞-groupoid captures all information about a topological space up to weak homotopy equivalence.</li> </ul> </li> <li>Path-connected means there exists a path between any two points in a topological space (the set of morphisms between any two objects in the fundamental groupoid is nonempty).</li> <li>Fundemantal group <ul> <li>Way to associate point to various symmetries of moving it through space.</li> <li>loops $f : S¹ → X$ (or $[0,1]$ and identifying 0 with 1 through the quotient topology).</li> <li>Homotopy of loops $f : S¹×[0,1] → X$</li> <li>Allow one to understand the structure of a topological space</li> </ul> </li> <li>Fundamental groupoid <ul> <li>Allows to alternate views of a topological or algebraic context. Functor from category of topological spaces to groupoids. Recontextualize topological spaces as groupoids (algebra). In this groupoid, objects represents points, morhpisms represents homotopy classes of paths. Note that the fundamental groupoid assigns to the ordered pair (p,p), the fundamental group of X based at p.</li> <li>Topological invariant of a topological space.</li> <li>p and q are path connected if the fundmental groupoid on X iff the set of morphisms from p to q is nonempty.</li> <li>The fundamental groupoid of a (path-connected) differentiable manifold X is actually a Lie groupoid, arising as the gauge groupoid of the universal cover of X.</li> <li>The ∞-groupoid allows uncovering higher dimensional holes in the topological space which blocks higher homotopies.</li> </ul> </li> </ul> </li> </ul> </li> <li>Formal language <ul> <li>Theory</li> <li>Doctrine</li> <li>Higher doctrine</li> <li>Model</li> </ul> </li> <li>Category theory <ul> <li>Look at various examples of categories in category theory in context</li> <li>Functors</li> <li>Representable functor</li> <li>Adjoint <ul> <li>Allow one to switch between recontextualizations. A similarity of context and structure.</li> </ul> </li> <li>Hom functor</li> </ul> </li> <li>Homotopy type theory <ul> <li>Univalence axiom</li> <li>Object classifiers</li> <li>Higher inductive types <ul> <li>Types with more complex elements which can have multiple proofs of the identity type.</li> <li>(∞,1)-colimits</li> <li>cell complexes</li> <li>truncations</li> <li>locatilizations</li> </ul> </li> <li>Inductive types <ul> <li>An inductive type $X$ can be viewed as a type freely generated by a certain finite collection of constructors. Essentially a data type constructor in Haskell.</li> <li>Recall that a property of a type is essentially the dependent function $∏(A:X→𝓤)∏(x:X)A(x) : 𝓤$</li> <li>Properties can be defined on all points of a type through inductive definitions.</li> <li>From the perspective of homotopy theory, inductive types represent complexes which can intuitively be thought of as building complex spaces out of simpler spaces.</li> <li>Induction is about defining property on every possible construction of a type. Recursion is about applying a function to every construction.</li> <li>Σ represents existential qualitfication. A proof of a property of an element of a type.</li> </ul> </li> <li>Dependent types https://ncatlab.org/nlab/show/dependent+type</li> </ul> </li> <li>Topos <ul> <li>Sub object</li> <li>(∞,1)-topos</li> </ul> </li> <li>Resources <ul> <li>https://ncatlab.org/nlab/show/computational+trilogy</li> <li>https://ncatlab.org/nlab/show/(infinity,1)-topos</li> <li>https://ncatlab.org/nlab/show/higher+category+theory+and+physics</li> </ul> </li> </ul> </li> </ul> <h1 id="key-concepts">Key concepts</h1> <p>Homotopy type theory serves as the internal language for (∞,1)-topos. Objects are types or spaces, viewed as constructions or collections of points. Morphisms are continuous maps between spaces with groupoid structures representing higher homotopies between these continuous maps. Types can be constructed, resulting in higher inductive types, which can be viewed as more complex topological spaces. The homotopies represent topological structures. Types are represented as ∞-groupoids, which signify how points are connected through homotopies.</p> <p>Voevodsky’s univalence axiom represents object classifiers in (∞,1)-categorical semantics. Higher inductive types allows for the construction of (∞,1)-colimits, cell complexes, truncations, localizations, and other objects which are constructed in homotopy theory using the small object argument.</p> <h1 id="inductive-types-and-higher-inductive-types">Inductive types and higher inductive types</h1> <p>(Higher) inductive types have point constructors and path or higher constructors. Point constructors essentially limit the view to a constructors of elements of a set. Higher constructors enact the higher groupoid view, and allow expressing higher-dimensional structure of points. Inductive types allow freely generating types. The (higher) inductive property expresses generating every possible construction of a type, and expressing a proof of each construction. The fact that loops are not (a priori) equal to $refl_{base}$ is why $1$ is different from $𝕊₁$.</p> <p>Fundamental to homotopy type theory, is the interpretation of types as spaces, and path types representing higher-dimensional information about the topology of these spaces, as opposed to set theory, which views sets (the fundamental object) as structureless bags. The fundamental group and groupoid represent the topological structure of a point and every point, respectively, of a type space. Type theory is the mathematical language that is used to communicate ideas, and category theory expresses the semantics of this language. Paths are fundamental to this language, and express how points in a space are equivalent. When discussing equality, one <em>must</em> consider the paths along with the points being considered, as this characterizes their equality.</p> <p>Homotopy type theory is a formal language for mathematics which combines ideas from type theory, homotopy theory, category theory, and model theory. Type theory provides a syntax, homotopy theory provides intuition, and category ties everything together. The underlying semantics for homotopy type theory is that of an (∞,1)-topos, in which objects are spaces, morphisms express how objects are put together, and higher morphisms express higher dimensional equivalences. The topos structure allows one to do mathematics. The structure of an (∞,1)-topos provides is a nicer base than that of traditional set theory. In particular, there are fewer assumptions made.</p> <h1 id="model-category">Model category</h1> <p>Model categories serve as the site for ∞-sheaves valued on ∞-groupoids.</p> <p>Sheaves on these categories represent types and follow local/global principles of sheaves, representing homotopy analogs.</p> <p>Quillen model categories are</p> <h1 id="resources">Resources</h1> <p><a href="https://homotopytypetheory.org/book/">The HoTT Book</a></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Types Logic Sets Homotopy   A proposition set space   a : A proof element point   B(x) predicate family of sets fibration   b(x) : B(x) conditional proof family of elements section   0, 1 ⊥, ⊤ ∅, {∅} ∅, ∗   A + B A ∨ B disjoint union coproduct   A × B A ∧ B set of pairs product space   A → B A ⇒ B set of functions function space   ∑(x:A) B(x) ∃x:A B(x) disjoint sum total space   ∏(x:A) B(x) ∀x:A B(x) product space of sections   Id_A equality={ (x, x) x ∈ A } path space A^I]]></summary></entry></feed>