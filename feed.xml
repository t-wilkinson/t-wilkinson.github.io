<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://t-wilkinson.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://t-wilkinson.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-07-24T19:04:59+00:00</updated><id>https://t-wilkinson.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">My personal notes</title><link href="https://t-wilkinson.github.io/blog/2024/my-personal-notes/" rel="alternate" type="text/html" title="My personal notes"/><published>2024-07-24T14:59:39+00:00</published><updated>2024-07-24T14:59:39+00:00</updated><id>https://t-wilkinson.github.io/blog/2024/my-personal-notes</id><content type="html" xml:base="https://t-wilkinson.github.io/blog/2024/my-personal-notes/"><![CDATA[<p>This is the current outline of my personal notes (except for some redacted).</p> <ul> <li>The arts <ul> <li>[Art] <ul> <li>[Design]</li> <li>[Harmony]</li> <li>[Origami]</li> <li>[Performance]</li> </ul> </li> <li>[Music] <ul> <li>[Composition]</li> <li>[Performance]</li> <li>[Piano]</li> <li>[Gaspard de la Nuit]</li> <li>[Music theory]</li> <li>[Harmony (music)]</li> </ul> </li> </ul> </li> <li>[Cognitive science] <ul> <li>[Linguistics]</li> <li>[Cognition] <ul> <li>[Logic]</li> <li>[Agent] <ul> <li>[Power]</li> <li>[Focus]</li> <li>[Consciousness]</li> </ul> </li> <li>[Knowledge] <ul> <li>Encoding</li> <li>[Model]</li> <li>[How to learn]</li> <li>[How to research]</li> <li>[Knowledge management]</li> <li>[Art of memory]</li> <li>[Knowledge contexts]</li> <li>[Reasoning]</li> <li>Epistemology</li> <li>Ontology</li> <li>Knowledge representation</li> <li>[Homotopy type theory]</li> </ul> </li> </ul> </li> <li>[Artificial intelligence] <ul> <li>[Learning algorithm]</li> <li>[Machine learning]</li> <li>[Reinforcement learning]</li> <li>[Deep learning]</li> <li>[Artificial neural network]</li> <li>[Dangers of artificial intelligence]</li> <li>[Artificial general intelligence] == [AGI]</li> <li>[Model architectures]</li> <li>[Model performance and scalability]</li> <li>[Cloud technologies]</li> <li>[MLOps]</li> </ul> </li> <li>[Neuroscience] <ul> <li>[Cell biology]</li> <li>[Neuron]</li> <li>[Nervous system] <ul> <li>[Brain]</li> <li>[Thalamus]</li> <li>[Cerebral cortex]</li> <li>[Neocortex]</li> <li>[Cortical column]</li> </ul> </li> <li>[Cell signaling] <ul> <li>[Neurotransmitter]</li> <li>[Synapse]</li> <li>[Membrane potential]</li> </ul> </li> </ul> </li> <li>[Psychology] <ul> <li>[Mental health] <ul> <li>[Mental disorders]</li> <li>[Addiction]</li> <li>[Depression]</li> </ul> </li> <li>[Jungian psychology]</li> <li>[Cognitive development]</li> <li>[Creativity]</li> <li>[Intelligence]</li> <li>[Thousand brains theory]</li> </ul> </li> <li>[Philosophy] <ul> <li>[Religion]</li> <li>[Ethics]</li> <li>[Knowledge]</li> <li>[Virtues]</li> </ul> </li> </ul> </li> <li>[Futurology] <ul> <li>[Global issues]</li> </ul> </li> <li>[Humanities] <ul> <li>[Religion] <ul> <li>[Religious symbols]</li> <li>[Muslim]</li> <li>[Christianity]</li> <li>[Church of Jesus Christ of Latter Day Saints] == [LDS] == [Mormon]</li> </ul> </li> <li>[Philosophy]</li> <li>[Entertainment]</li> </ul> </li> <li>[Mathematics] <ul> <li>Foundations of mathematics <ul> <li>[Mathematical logic]</li> <li>[Homotopy type theory]</li> <li>[Homotopy theory]</li> <li>[Type theory]</li> <li>[Topos theory]</li> <li>[Category theory]</li> </ul> </li> <li>[Algebra] <ul> <li>[Numbers]</li> <li>[Linear algebra]</li> <li>[Abstract algebra]</li> <li>[Algebraic geometry]</li> </ul> </li> <li>Analysis <ul> <li>[Functional analysis]</li> <li>[Sequences and series]</li> <li>[Calculus]</li> </ul> </li> <li>[Geometry]</li> <li>[Topology] <ul> <li>[Manifold]</li> </ul> </li> <li>Applied <ul> <li>[Probability]</li> <li>[Statistics]</li> </ul> </li> </ul> </li> <li>[Me] <ul> <li>[Magnum opus]</li> <li>[Manifesto]</li> <li>[My church talks]</li> <li>[My art]</li> <li>[My abstractions]</li> <li>[My research]</li> <li>[My career]</li> <li>[Projects] <ul> <li>Work</li> <li>[AI Presentation]</li> <li>[Mr. Robot]</li> <li>[Zortex]</li> </ul> </li> </ul> </li> <li>[People] <ul> <li>[Health] <ul> <li>[Martial arts]</li> <li>[Rock climbing]</li> <li>[Excercise]</li> <li>[Strength training]</li> <li>[Nutrition]</li> <li>[Mental health]</li> </ul> </li> <li>[Communication] <ul> <li>[Dating]</li> </ul> </li> <li>[Human development]</li> </ul> </li> <li>[Science] <ul> <li>[Systems science]</li> <li>[Biology] <ul> <li>[Cell biology]</li> <li>[Neuroscience]</li> </ul> </li> <li>[Chemistry]</li> <li>[Computer science] <ul> <li>[Web development]</li> <li>[Programming languages]</li> <li> <p>[AI] == [Artificial intelligence]</p> </li> <li>Computer system <ul> <li>[Electronics]</li> <li>[Computer hardware]</li> <li>[Computer architecture] <ul> <li>[Computer arithmetic]</li> </ul> </li> <li>[Computer performance]</li> <li>[Data structures and algorithms]</li> <li>[Kernel]</li> <li>[Operating system]</li> <li>[System design] <ul> <li>[Database]</li> </ul> </li> <li>[Program execution]</li> <li>[System administration]</li> <li>[Cloud technologies] <ul> <li>[Virtualization software]</li> </ul> </li> </ul> </li> <li>[Software] <ul> <li>[Virtualization software]</li> <li>[Software architecture]</li> <li>[Software engineering]</li> <li>List of software <ul> <li>[Nix]</li> <li>[PyTorch]</li> <li>[Vim]</li> </ul> </li> </ul> </li> <li>Networks <ul> <li>[Network architecture]</li> <li>[Internet protocol suite]</li> </ul> </li> <li>Security <ul> <li>[Anonymity]</li> <li>[Cyber security]</li> <li>[Cryptography]</li> </ul> </li> </ul> </li> <li>[Physics]</li> </ul> </li> <li>[Social science] <ul> <li>[Countries]</li> <li>[History]</li> <li>[Economics] <ul> <li>[Asset]</li> <li>[Market]</li> <li>[Personal finance]</li> <li>[Economy]</li> <li>[Economic systems]</li> <li>[Post labor economics]</li> <li>[Business] <ul> <li>[Marketing]</li> <li>[Accounting]</li> <li>[Business administration] == [Business management]</li> <li>[Business analysis]</li> <li>[Business development]</li> <li>[Business organization]</li> <li>[Business services]</li> <li>[Business plan]</li> <li>[Finance]</li> <li>[Startup business]</li> <li>[Client communication]</li> </ul> </li> </ul> </li> <li>[Education]</li> <li>[Linguistics] <ul> <li>[English]</li> <li>[Russian]</li> <li>[Spanish]</li> </ul> </li> <li>[Psychology]</li> <li>[War]</li> <li>[Careers] <ul> <li>[Certificates]</li> </ul> </li> </ul> </li> <li>Special <ul> <li>[Test]</li> <li>[Resources]</li> <li>Zortex <ul> <li>[Storage]</li> <li>[Projects]</li> <li>[Structure]</li> <li>[Schedule]</li> </ul> </li> <li>Disambiguation <ul> <li>[Structure (disambiguation)]</li> <li>[Harmony (disambiguation)]</li> </ul> </li> <li>[History]</li> <li>Outlines</li> <li>[Glossary]</li> <li>[Vocabulary]</li> <li>Timeline</li> <li>Overview</li> <li>Lists <ul> <li>[List of people]</li> </ul> </li> <li>Un categorized <ul> <li>[Philosophy of disciplines]</li> </ul> </li> </ul> </li> <li>Technology <ul> <li>[History of technology]</li> <li>[Electronics]</li> <li>[Robotics]</li> <li>[Engineering]</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="me"/><summary type="html"><![CDATA[This is the current outline of my personal notes (except for some redacted).]]></summary></entry><entry><title type="html">My favorite pieces of music</title><link href="https://t-wilkinson.github.io/blog/2024/my-favorite-pieces-of-music/" rel="alternate" type="text/html" title="My favorite pieces of music"/><published>2024-07-24T14:57:59+00:00</published><updated>2024-07-24T14:57:59+00:00</updated><id>https://t-wilkinson.github.io/blog/2024/my-favorite-pieces-of-music</id><content type="html" xml:base="https://t-wilkinson.github.io/blog/2024/my-favorite-pieces-of-music/"><![CDATA[<ul> <li>Scriabin - Sonata 5/8</li> <li>Hammerklavier (sonata no. 29 op. 106) - Beethoven</li> <li>Giant Steps : John Coltrane</li> <li>Take the A train (performed by Oscar Peterson)</li> <li>Ives Sonata no. 2 “Concord”</li> <li>Gaspard de la Nuit by Ravel</li> <li> <p>Mists by Xenakis</p> </li> <li>Sorabji <ul> <li>Opus Clavicembalisticum <ul> <li>VII cadenza I: pg101</li> <li>IX interludium B: pg139</li> <li>IX Toccata: pg139</li> <li>X cadenza II: pg196</li> </ul> </li> <li>100 Transcendental Studies <ul> <li>75: passacaglia</li> </ul> </li> <li>Symphonic Variations <ul> <li>27 The garden of Iram</li> </ul> </li> </ul> </li> <li>Franz Liszt <ul> <li>Transcendental etudes <ol> <li>Preludio 1:30</li> <li>Mezzepa 7:00</li> <li>Feux Follets 4:00 <ul> <li>Features double notes where the uppervoice is played legato (and notes are very proximal) and the lower voice is stocatto</li> </ul> </li> <li>Wilde Jagd 5:00</li> <li>Chasse Neige 5:00</li> </ol> </li> <li>Beethoven 9 - Franz List</li> </ul> </li> <li>Rachmaninoff <ul> <li>Rach studied bach so it uses heavy melodic flavored harmony</li> <li>Rachmaninoff concerto 3 <ul> <li>Outline<br/> 0:00 intro<br/> 0:04 theme 1, played by the piano<br/> 0:49 roles inverted. piano plays the accompaniment, violas and horns play the melody<br/> 1:35 transition part 1<br/> 2:04 transition part 2. A fragment of the 2nd subject played by the flutes, oboes, solo horn and trumpets<br/> 2:17 transition part 3<br/> 2:34 transition part 4. 1st subject played by bassoon and lower strings<br/> 3:12 theme 2a<br/> 3:46 theme 2b<br/> 4:04 Counter theme, played by solo Bassoon<br/> 4:59 theme 2c<br/> 5:41 transition<br/> 6:08 closing section. Cantabile version of Theme 2A, played by the Piano<br/> 8:24 Closing Section Part 2. Counter melody played by the Clarinet</li> </ul> </li> </ul> </li> <li>Prokofiev piano concerto no.2</li> <li>Ligeti piano concerto</li> </ul>]]></content><author><name></name></author><category term="music"/><summary type="html"><![CDATA[Scriabin - Sonata 5/8 Hammerklavier (sonata no. 29 op. 106) - Beethoven Giant Steps : John Coltrane Take the A train (performed by Oscar Peterson) Ives Sonata no. 2 “Concord” Gaspard de la Nuit by Ravel Mists by Xenakis]]></summary></entry><entry><title type="html">How to learn</title><link href="https://t-wilkinson.github.io/blog/2024/how-to-learn/" rel="alternate" type="text/html" title="How to learn"/><published>2024-07-24T14:56:59+00:00</published><updated>2024-07-24T14:56:59+00:00</updated><id>https://t-wilkinson.github.io/blog/2024/how-to-learn</id><content type="html" xml:base="https://t-wilkinson.github.io/blog/2024/how-to-learn/"><![CDATA[<p>Abstacting information is like being given a red square and blue square, and developing topology to understand their similarities and differences.</p> <p>Ultimately, learning is about storing information in a way that can be retrieved optimally for its purpose. Brain is a map of experience. It builds models of the information it is fed. Information generally most useful when complex connections are understood. Encoding and retrieval are two sides of the same coin. You can do both. Encode high-level understanding and retrieve those high level concepts spaced, interleaved, in different contexts, etc.</p> <p>You need to spend time both deeply learning foundational concepts, but also getting a broad understanding of a variety of concepts. Each are very important ways to develop contexts in which to relate other concepts. There is a big difference between limited knowledge, and broad shallow knowledge. Be comfortable ignoring details in pursuit of a broad shallow knowledge. Be comfortable ignoring various details to deeply understand one idea.</p> <p>Taxonomies:</p> <ul> <li>My taxonomy: isolation -&gt; context -&gt; groups in context -&gt; compare groups in same context -&gt; compare groups in different contexts <ul> <li>Understand how a concept is constructed under different goals of understanding a concept</li> <li>Understand each construction (particularly the chosen context) in a larger context and how they all relate to each other</li> </ul> </li> <li>Blooms taxonomy: remember -&gt; understand -&gt; apply -&gt; analyze -&gt; evaluate -&gt; create</li> <li>SOLO taxonomy: pre-structural -&gt; uni-structural -&gt; multi-structural -&gt; relational -&gt; extended abstract</li> </ul> <h1 id="encoding">Encoding</h1> <p>The key to finding the right encoding method, is to understand how the brain stores, represents, recalls, and utilizes information. Although, regardless of whatever low-level mechanisms the brain uses to manage information, storing information in terms of how you want to use it is the best strategy.</p> <p>An alternative to contexts is developing graph knowledge, and attaching information to relationships.</p> <p>Encode knowledge in as much complexity as soon as possible.<br/> Don’t really need to spend as much time on the lower levels of learning.<br/> Focus on higher level thinking. You will need to fill in the rest of the dots necessarily to reach the goal.<br/> Prestudy: before your study session, prepare your mind. plan out where things will go, etc. prime it.<br/> Writing notes lets your brain immediately forget and offload to the paper. you’re not really learning anything.<br/> When reading papers: Group words, isolate key words. Take breaks to understand what you just read and how it fits with everything else.</p> <p>Encoding types:</p> <ul> <li>Levels of abstractions: each level has concepts at certain level of abstraction</li> <li>Probes: Functors from a category point, graph, sequence, tree, discrete collection, etc. to the encoding category</li> <li>Tree with root at highest level of abstraction</li> </ul> <p>How to think: abstract Information. Have a list of information or textual data. Combine abstract data through a graph and sequential structure. On one level, have a graph relating abstract structures or contexts that are well understood. Separately, have a graph of concepts, definitions, etc. Associate these graphs with each other. Infact, there can be a hierarchy of graphs with differing levels of abstraction, each overlapping the other. Additionally, you pick out a tree or graph from these layerings of abstraction, and chunk those together. Then treat those chunks as their own concept to include reciprically in the previous relationships. Balance Precise formal understanding, visual, generalized, definitions, vocabulary, etc. These modals each have their own graph as well as interplay between concept structures.</p> <h1 id="contexts">Contexts</h1> <p>Break the concept into a category. Categories allow exploring the structure of a context, so you look at various categories. Then you find a way of comparing these structures. Because concepts should form a sort of limit, you can embed multiple of these limits inside the same category, and still have access to objects and morphisms. You then form 1-cells to compare objects and 2-cells to compare morphisms.</p> <p>Break a concept into subconcepts, under various contexts. In other words, identify the structure of a concept under various contexts. With all the contexts and their substructures, create a graph structure relating everything. In particular the contexts and how their structures relate to each other. This is all in order to learn the structure of a concept. Next step is to identify how the concept relates to structures of other concepts under various contexts. In other words, repeat the first stage, but at a level higher.</p> <p>Contexts provide the underlying frame of reference or embedding space for concepts. Develop a solid understanding of the embedding space and relative relationships between various concepts that can be placed in these embedding spaces. People either develop these spaces through establishing relation to existing concepts, or by building the contexts and placing concepts within these.</p> <p>Learning a concept in multiple contexts or multiple ways is like building redundant information. Like stripes in a RAID system. If you forget a chunk of information, you still have enough to fill the gaps. Each context is like a separate hook or pointer for finding the concept. A concept is “remembered” forever, you just need the hooks to find it again. If you distribute a large amount of hooks around, you never forget the idea.</p> <p>Some contexts include: vocabulary (synonyms, antonymns, making sentences, etymology), abstractions (category theory, topos theory, calculus, etc.), history, examples, how something is contructed, applications, related ideas, resources (citations, authors), visu</p> <p><strong>Vocabulary</strong>: Start by defining key terms related to the concept. This will create a foundation for your understanding and help you communicate effectively about the topic.<br/> <strong>History</strong>: Understanding the historical context of a concept can provide insights into its development, evolution, and the reasons behind its existence. Learning about key figures, events, and milestones can deepen your grasp of the concept.<br/> <strong>Examples</strong>: Real-life examples can bring abstract concepts to life. They illustrate how the concept is applied in different situations, making it easier to understand and remember.<br/> <strong>Applications</strong>: Explore how the concept is used in various fields or industries. Understanding its practical applications can help you see its relevance and importance in the real world.<br/> <strong>Structure</strong>: If the concept has a structured framework or model, understanding this structure can provide a roadmap for organizing your thoughts. It can also help you identify relationships between different components of the concept.<br/> <strong>Analogies</strong>: Drawing parallels between the concept and something you already understand well can accelerate your learning. Analogies make new information relatable by linking it to familiar ideas.<br/> <strong>Visualizations</strong>: Creating diagrams, mind maps, flowcharts, or other visual aids can help you visually represent the concept’s components and relationships. Visualizations are particularly helpful for understanding complex systems.<br/> <strong>Comparisons and Contrasts</strong>: Compare the concept with similar or related ideas, highlighting their similarities and differences. This can clarify nuances and help you grasp the concept more deeply.<br/> <strong>Problem Solving</strong>: Apply the concept to solve problems or answer questions. Practical engagement can solidify your understanding and highlight areas where you need further clarification.<br/> <strong>Discussion and Teaching</strong>: Engage in discussions or even teach the concept to someone else. Explaining the concept in your own words forces you to consolidate your understanding and identify gaps in your knowledge.<br/> <strong>Critique and Analysis</strong>: Evaluate the strengths and limitations of the concept. This critical thinking helps you develop a more well-rounded perspective and understand its practical boundaries.<br/> <strong>Personal Reflection</strong>: Consider how the concept relates to your personal experiences or beliefs. This can create a more meaningful connection to the material. How does it resonate with your personal journey, challenges, and achievements?<br/> <strong>Future Trends</strong>: Research how the concept might evolve in the future. Are there emerging trends, technologies, or shifts that could impact its relevance or application?<br/> <strong>Interdisciplinary Connections</strong>: Identify how the concept connects to other fields of knowledge. Cross-disciplinary insights can provide fresh perspectives and lead to innovative applications.<br/> <strong>Historical Context</strong>: Delve deeper into the historical backdrop of the concept. What societal, technological, or cultural factors influenced its emergence and development?<br/> <strong>Language and Communication</strong>: Consider how the concept is communicated across different languages and communication mediums. Does its interpretation change in different linguistic or cultural contexts?<br/> <strong>Counterexamples</strong>: Look for instances where the concept doesn’t apply or breaks down. Understanding these exceptions can refine your understanding of the concept’s boundaries.<br/> <strong>Meta-Understanding</strong>: Reflect on your own understanding of the concept. How has your perspective evolved over time, and what gaps or uncertainties remain? This self-awareness can guide further learning.<br/> <strong>Emotional Impact</strong>: Consider the emotional aspects tied to the concept. Does it evoke certain emotions or reactions? How might emotions influence its interpretation and application?<br/> <strong>Global Perspective</strong>: Investigate how the concept is understood and used in different regions of the world. Global viewpoints can enrich your understanding and provide insights into cultural diversity.</p> <p><strong>Evolution of Definitions</strong>: Analyze how the definition and understanding of the concept have changed over time. This can reveal shifts in societal attitudes and knowledge.<br/> <strong>Cross-Cultural Adaptation</strong>: Explore how the concept has been adapted and integrated into various cultures. This can highlight its universality or its adaptability to specific contexts.<br/> <strong>Cultural Context</strong>: Explore how the concept is perceived and utilized in different cultures and societies. Cultural nuances can impact the interpretation and application of a concept.<br/> <strong>Ethical Considerations</strong>: Investigate the ethical implications of the concept. How does it align with ethical principles, and are there any ethical dilemmas associated with its use?</p> <h1 id="core-ideas">Core ideas</h1> <ul> <li>Identify your struggles <ul> <li>Something that holds you back from your goals</li> </ul> </li> <li>Identify your strengths. If your goal is to learn overall, spend more time on your weaknesses.</li> <li>Deconstruct it into manageble piece</li> <li>Identify your motivation <ul> <li>Defines what you should focus on and prioritize</li> <li>Gives you something to connect knowledge to</li> <li>Only allow multiple goals/motivation if you have enough time. Otherwise you can simply master one area and transfer knowledge to another.</li> </ul> </li> <li>Identify the core pieces <ul> <li>The essence of the subject</li> <li>What factors through all the other pieces</li> <li>Really understand this</li> <li>Make sure what you learn is correct</li> <li>Especially in a completely new subject to your, these pieces of knowledge will be difficult and time consuming to master</li> </ul> </li> <li>Abstracting/Redundant information <ul> <li>The brain understands in terms of association. We represent the same information in a lot of different ways or contexts. By representing information this way, there is a lot of redundancy. As such, one can reconstruct information if one forgets some. Similar to how RAID memory works. I believe this is similar to the thousand brains theory of intelligence. Given that knowledge is highly contextual, information of a certain concept highly depends on context so it makes sense to extract the information contextually. This is where the power of abstractions come in. The better at abstracting, the more contexts one can relate the concept to. One can essentially build more redudancy out of a concept. However this doesn’t become helpful until there are already a lot of concepts in the brain.</li> </ul> </li> <li>Identify what you’re learning, and what the underlying ideas behind it is. Master the more abstract, lower concepts, and similar ideas at same level of abstraction to get your brain to learn a solid abstraction of the concept you’re learning. That will make that specific case better, and you can quickly learn similar ideas in the future. Smarter people are naturally better at this, but conciously doing this is incredibly powerful.</li> <li>Identify what you don’t know</li> <li>View reeality as moldable - steve jobs. See whhat needs to be done to learn</li> <li>You can change your environment</li> <li>Learning something is the process of solving a problem of a lack of knowledge.</li> <li>Identif your goal, where youare, and important steps along the way. Be creative about how you teach that.<br/> Identify key, underlying principles</li> <li>Make sense of new field</li> <li>Get a high level understandong of how things relate</li> <li>How to improve understanding</li> <li>Techniques: Active recall, distributed learning, repetition</li> <li>Learn multiple representations of the same piece of information</li> <li>Important to be able to quickly work with knowledge, How to acheive this? Practice</li> <li>First principles thinking</li> <li>Chunking</li> <li>Topos theory/sheaf theory</li> <li>Homotopy type theory equality is a comparison of strucvtures. There are multiple ways structre can be the same. (False==True)</li> <li>Resources: google research papers, youtube, lectures, etc.</li> <li>Reliable information. Information integrity</li> <li>Take your time to get it right the first time.</li> <li>Focus</li> <li>Hyperfocus + Scatterfocus</li> <li>Habits</li> <li>Intellectual honesty. Be honest to yourself about what you do and don’t know</li> <li>Disadvatnagages: divergence of vocabulary with rest of the world. You form vocabulary to communicate with other people. Take about what covabulary/language is.</li> <li>Learn in multiple ways. Have multiple practices techniques</li> <li>Generalize/find common patterns</li> <li>Ask for help. Don’t be too proud. You can learn from people who don’t know as much as you. Drop the ego.</li> <li>Intelligence. What is it. Multiple intelligences.</li> <li>Adapt. Be like water ( bruce lee)</li> <li>Long term perspective. Take your time. Think of compound effects.</li> <li>Habits make experts.</li> <li>What is an expert? They know in and outs.</li> <li>Common patters of successful people.</li> <li>Multi-phase learning</li> <li>Intelligence vs. skill vs. hard work</li> <li>As simple as possible and no simpler.</li> <li>Wait for a break through. Take breaks.</li> <li>Concious mind finds information. Subconscious does the actual learning</li> <li>Meta-learning</li> <li>Meta-cognition</li> <li>Forming models fo the world</li> <li>Understand how you learn</li> <li>Age != wisdom. Reflect, reflect, reflect.</li> <li>Brain learns models of the world. Help it.</li> <li>My biggest mistake: minimize what you learn so you don’t have as much to remember and to focus on more important information.</li> <li>80/20, pereto</li> <li>Mindfulness</li> <li>Understand tradeoffs</li> <li>Recall vs. Recollection</li> <li>Different levels of understanding</li> <li>Focus (disadvantages)</li> <li>Extracting the simplest, most difficult thing to learn (crux)</li> <li>Learning knowledge is like (snow compacting to ice. ground water)</li> <li>Abstractions are ways to information to make sense of it</li> <li>Philosophy of idfferent disciplines</li> <li>Knowledge as a tree/graph</li> <li>Frameworks</li> <li>Learn enough to make sense of a discipline. Then work on a pojrect</li> <li>Willing to be the fool</li> <li>Double down on your strengths, round off your weaknesses (workout)</li> <li>Mindful practice</li> <li>Managing motivation, energy, etc.</li> <li>Focus what your learning, help your mind know that relevant details</li> <li>Break into largest manageble prices</li> <li>Starting off: know enough to know how to find more. Basic vocab.</li> <li>Knowledge as grph</li> <li>Feynman technique</li> <li>Spaced repetition <ul> <li>Just like you need to review music even when you practice it deeply, you MUST review concepts you deep think. At the end of the day, tag concepts you deeply thought so you can review them in bulk later onAt the end of the day, tag concepts you deeply thought so you can review them in bulk later on.</li> <li>Don’t need to learn everything. Focus on what your interested in that will work to getting you towards your goals. Find other people with interests that do not exactly line up with yours.</li> <li>Next level of understanding <ul> <li>Explaining my abstract thoughts to people is a great excercise in churge. I can also practice in my notes. Talk more to people. Learn to communicate.</li> <li>Practice building a hierarchy and filling out the tree. Go back and see how you can improve. Pick through every line, like a paper. Then do it again.</li> <li>Record yourself explaining stuff.</li> <li>When explaining things I have a tough time not going super deep. Also practice explaining things without diving into the details.</li> <li>The problem is I have some abstract concept in mind, but I don’t know how to linearize it into words.</li> <li>My zettelkasten is a bare tree right now. I have hierarchies for days but I NEED details at the leafs.</li> <li>Should be like the wikipedia</li> <li>Either break apart a concept and talk about it blog style, or go through other concepts in zettelkasten and add more details.</li> <li>Practice explaining things that you read.</li> <li>Also try a simple explanation that does not require much previous knowledge <ul> <li>It is ok to give impartial answers</li> </ul> </li> <li>Separate connecting ideas from simply explaining them</li> </ul> </li> <li>I need to go back over resources (pretty much every zettel) and connect information, make it more practical. Explain in detail, etc. Should be 30min a day.<br/> ? Why am I so fast at learning piano</li> <li>Passion. Passion gives focus, amplifies ability to learn, gives more focus and brain power to the thing your learning and associates strong emotion with it. The brain is very associative so the strong the memories associated with the thing your learning, the better the learning.</li> <li>I am receiving the dividends of my past investments into improving my autodidact skills and overall knowledge of how to learn. <ul> <li>Some examples include: spaced repetition, relating, abstracting, utilizing working memory, focus, applying, chunking, contextualizing, etc.</li> </ul> </li> <li>My brain often experiences a lot of epiphanies. I’m very slow at first. I learn in layers.</li> <li>Music is not something I was naturally gifted at. I don’t see any evidence for transfer knowledge from guitar because I didn’t really know much in the first place so in theory my progress should have slowed especially once I reached similar proficiency.</li> <li>In the context of piano, I have found it very easy to learn how to improve. Results are very quick so I can experiment and quickly learn what works best. <ul> <li>Can utilize all of my brain <ul> <li>Visual tools are very useful</li> <li>Logical intelligence is very useful</li> <li>Auditory is very useful</li> <li>Working memory is very useful</li> <li>Physical intelligence</li> </ul> </li> <li>If I’m struggling with a piece I can quickly find what the issue is</li> </ul> </li> <li>Easy to deconstruct problems (difficult passages, isolate issues with techniques, etc.)</li> <li>I layer a lot of different skills simultaneously, which makes it look a lot faster than it is <ul> <li>I’m naturally talented</li> </ul> </li> </ul> </li> </ul> <h1 id="organizing-information">Organizing information</h1> <ul> <li>Abstract each contextualization of an idea. Then contextualize those abstractions and abstract even further.</li> <li>Spend time practicing thinking in words. Let yourself go and let yourself communicate. Also connect your abstract mathematical mind with linguistic mind.</li> <li>Let myself do both graph thinking and deep thinking. Find a balance.</li> <li>Graph is about understanding things in terms of what they are connected to, interact with, function as, actions they take. You don’t peer into the actual idea. Deep thinking is about understanding something in terms of what makes it up, what can it be restricted to, what are the smallest functional pieces that create this thing. <ul> <li>Why I barely pay attention to thinigs. I get a quick abstract understanding. Zack needs to understand it multiple ways.</li> </ul> </li> <li>Think of concepts as a mapping to a topology which groups concepts by how similar they are</li> <li>New way of thinking <ul> <li>Motivation <ul> <li>Thousand brains</li> <li>Should understand a concept in multiple ways, connect it to multiple things</li> </ul> </li> <li>Develop a dense graph of related concepts.</li> <li>Should one form multiple branching contexts or a flat array of tagged concepts. <ul> <li>Both <ul> <li>Have multiple branching contexts and a flat array at top of tagged concepts connecting the branching contexts</li> <li>Make implicit connections explicit</li> <li>Don’t develop in a particular order. Develop both interpretations side by side.</li> </ul> </li> <li>Multiple branching contexts <ul> <li>Separates concepts from each other</li> <li>Organized</li> <li>Every concept is a node on a tree only with one parent and many children</li> <li>Understand concepts deeply and can abstract and reuse abstractions</li> </ul> </li> <li>Flat array of tagged concepts <ul> <li>Exists at every concept/context</li> <li>Unorganized and chaotic (good thing)</li> <li>Everything is connected</li> <li>Every concept has its own graph</li> <li>Every concept only has siblings</li> </ul> </li> </ul> </li> </ul> </li> </ul> <h1 id="framework">Framework</h1> <p>To learn most things, think about what framework you need to reach conclusions and develop that frameowrk. To learn charisma. Learn how to read them. Learn to read youself. Pay attention to when you form/change opinions on others. Watch for peoples attention. What directs their attention. Engaged questions. A lot of information of emotion comes from facial/vocal expression. Microexpression. Identifying patterns. Make predictions. Experiment.</p> <h1 id="self-teaching">Self teaching</h1> <p>One of the most difficult challenges of self teaching is identifying what you don’t know, holes in your knowledge, and/or developing a study plan. A useful strategy for identify what to learn is to think about what framework of thinking is necessary for discovering this/understanding this concept?</p> <p>Limiting yourself to equation solutions is a very limited way to learn. By learning other ways to validate your knowledge, you drastically expand learnable skills.</p> <p>Reflect upon how you have to think in order to reach a more optimal state. A constant process of self-reflection will lead you to developing the necessary techniques to effectively self learn. If you’re trying to solve a problem, but you don’t know enough, and have to keep going back to learn various concepts from the same source, you can deduce: if I can beforehand determine if there are any resources that include most relevant information to a problem, then I should consider internalizing it such that I can have a more focused and productive problem solving session.</p> <p>The focused approach has flaws. When you don’t crystalize prerequisite knowledge before hand, you leave yourself to building understanding through unfinished thoughts and ideas from prerequisite knowledge. There is something to be said about a complete understanding of a subject and knowing that many things are for certain not true rather than having decent certainty that something is true.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Abstacting information is like being given a red square and blue square, and developing topology to understand their similarities and differences.]]></summary></entry><entry><title type="html">Music Insanity</title><link href="https://t-wilkinson.github.io/blog/2024/music-insanity/" rel="alternate" type="text/html" title="Music Insanity"/><published>2024-07-24T14:51:11+00:00</published><updated>2024-07-24T14:51:11+00:00</updated><id>https://t-wilkinson.github.io/blog/2024/music-insanity</id><content type="html" xml:base="https://t-wilkinson.github.io/blog/2024/music-insanity/"><![CDATA[<p>Don’t try so hard to generalize and learn every conceivable technique. Instead, learn difficult technical pieces like Liszt and Chopin etudes, and master them completely. afterward develop general techniques for other roots, chords, and scales. other than learning pieces, focus on memorizing scales and chords.</p> <p>A big part of chord technique is memorizing and developing natural capacity to open the hand to correct orientation. select chords that have troubled you, and break them down in every way (jumping from every combination of interval to every combination of intervals in every sequence on every combination of left and right hand, master all dynamics, etc). master the technique before really trying to learn every chord, it will greatly speed the process and make it more reliable.</p> <p>practice should consist of in-depth chord/scale practice considering all inversions and omissions etc, technique practice where you aim for speed and precision, and memorization of as many scales and chords</p> <p>finger independence pactice is also about playing chords and intervals and internalizing the different finger movements</p> <p>use liszt etudes to cover every technique, afterward generalize to various chorss and scales. just focus on memorizing chords and scales.</p> <p>the “if you can play it slow you can play it fast” mantra is relevant primarily when learning pieces that are well within technical capabilities. Developing technical capabilities however should be considered like working out and stretching, you must push yourself to edge of failure. slow practice is like learning to aim a bow woth varying degrees of force, low force requires different mechanics than high force, but both need to be mastered to master piano.</p> <p>app that selects a random scale. random sequence of degrees, random sequence of chord inversions, rhythms</p> <p>optimal technique results in minimal movement of wrist and fingers. smoothest possible movement. minimize work fingers need to do. rhythm groupings become somewhat unnecessary when you master scales amd chords amd technique, they teach you to plwy a sequence in awkward and impractical chunks, you wouldnt normally get in certain positions and they lead to clunky playing</p> <p>double note sxale play sequence in one scale then jump along sequence in other scale</p> <p>Music harmony has issues. Scales and chords are not the right way to think about these. There are collections of notes that serve as a backbone or structures, there are sequences of backbones or structures, then there are notes played simultaneously mostly taken from a backbone. There are ways to realize or voice sequences and simultaneous notes. The structures have various harmonic relationships. Hierarchical layers of related structures. Structures don’t necessarily have direct restriction relationships. Or think of it as the intersection of possible overlapping structures of each structure as the fundamental musical context. Don’t look at identifying related structures reductively, but additively. At every level there are breaks from the structure.</p> <p>I should aim for Liszt’s transcendental etudes or hamelins etudes (and learning more pieces in general). in addition to memorizing chords and scales, it is important to connect it through pieces and for learning techniques harder to generalize in excercises. Take learning pieces as a skill, just like sight reading, the ability to quickly take a piece you just sight read, abstract parts, understand the techniques, harmonies, and purpose of notes, know what dynamics and timing to use, and to formalize everything into memorizable chunks and quickly play it is a skill.</p> <p>register: very important to voicings. higher registers permit denser chords, lower registers permit spacier chords. think of the overtone series.</p> <p>you can get away with dissonance with good voice leading</p> <p>chord function is more relevant than chord quality.</p> <p>to learn: break it into simplest bits of information then combine it like the partial order of a powerset. isolate into generalizable chunks, and master the most generalizable. hence why i prioritize chords.</p> <p>things to improvise: octaves (scale or chord progression like hung rhapsody ending, mazeppa), finger indp like feux follets or ondine descending notes. franz liszt is perhaps best influence for things I want to improvise. <br/> study jacob collier music</p> <p>scales are fake news. you can start the harmony from chords and let scales flow from there, chords dont have to come from the scale. free yourself. move chords around without reference to scale. leave the chord however you feel like.</p> <p>melodies, chords, intervals, or scales can be first class citizens. it depends on the harmony and sound you’re going for. isolate the sound or harmony you’re working in and build from there.</p> <p>voice leading: think of moving subshape through a sequence (voice leading). different voices can follow different sequences generated from different shapes</p> <p>polyrhythm for melody. offset one melody, repeat melody as you are finishing up the original.</p> <p>think of modes and harmonies as specifying notes, mapping of chords to collection of the notes such that all chords give a piece of characteristic sound for the mode. i dont have time to learn all scales or chords so i should focus on the music i want to perform. the harmony also includes how one diverges from harmony like alterations, additional chords used.</p> <p>Pools of notes specify the collection of notes, and therefore the intervals. A harmony specifies particular central notes (the root in particular) and other key intervals. Chords containing those key intervals in combination with the key notes give the characteristic harmonic sounds. One can leave the harmony for increased harmonic direction.</p> <p>There are note pools or inventories (like scales) and there are formations on those pools, called shapes and sequences. Taking thirds builds classical chord shapes. The pools have meaning once a harmony is selected and how the resulting pool from a base note relates to the harmony.</p> <p>The goal is to internalize a large sum of patterns, such that I just need to quickly map sheet music to my pattern space and memorize that mapping.</p> <p>The music you make depends on your training, studies, things you learn, and patterns learned. Learn patterns that lead to the music you want to develop, which facilitates thinking about music, harmony, melody, and rhythm the way you want to. What kind of music do I realistically want to play? Jazz, classical, compose? I like the creative aspect of music and improvisational ability. I like complex harmonies and interesting rhythms. I should learn jazz, but incorporate harmonies and textures from classical so that I can apply them to various chords or rhythms. Also to be able to show off flashy pieces. <br/> I should prioritize scale and chord knowledge. play through all chords in a scale with various voicings (thirds, fourths, split left and right hand) then I can start separating techniques generated from different harmonies <br/> need separation of harmony, technique, and rhythm. learn to quickly switch between combinations of each domain (melody/harmony, technique and embellishments, rhythm, dynamics).</p> <p>I see myself picking interesting textures from classical pieces and incorporating them into jazz, learning them in various harmonic contexts and improvising melodies over them. How can I use advanced classical textures like double thirds to create unique jazzy music that is otherwise better than its simple counterpart. That is, how can I best harmonize with the virtuosity I have developed but not make it overwhelming or out of place?</p> <p>To find my style, I must find scales that I particularly like and master those. I can’t master every scale. <br/> Scales: I like non-tonal scales, double harmonic major, scales with minor third jumps, mixture of major/minor scale. Rhythm <br/> Chords: tertian harmony, polytonality <br/> Texture: ravel-like textures <br/> Melody and themes: introduction of various themes, simplification of themes or combination of other themes</p> <p>https://www.scales-chords.com/scaleinfo.php?skey=C\&amp;sname=double+harmonic</p> <h2 id="harmony">Harmony</h2> <h3 id="jazz-harmony">Jazz harmony</h3> <p>X∆7: 3 7 9 #11 13 <br/> X-7: b3 b7 9 11 13 <br/> X7: 3 b7 b9 9 #9 #11 b13 13 <br/> X-7b5: b3 b7 b5 <br/> X7#5: 3 b7 #5 <br/> X-<strong>∆</strong>7: b3 7 <br/> X9sus: 11 b7 9 <br/> Xsusb9: 11 b7 b9 <br/> X-b6: b3 b6</p> <h2 id="technique-variation">Technique variation</h2> <p>Dotted rhythm and similar variations. But stay very in time (124 124 etc). Short bursts in every rhythm grouping (2s 3s 4s 5s 32s 54s etc) <br/> Improvise on chord progression using different technique. Change chord progression using same technique <br/> Closed eyes <br/> Play precisely and evenly each dynamic. Master this before you really try to connect to dynamics <br/> Transpose</p> <h2 id="pool">Pool</h2> <p>Every transposition, inversion, alteration, and note omission/subsets <br/> Intervals <br/> Extended pools (scales spanning more than an octave) are stacks of shapes that fit an octave</p> <p>Focus more on all 4 note shapes with intervals of m2, M2, m3 (1, 2, 3 semitones) <br/> major 2-2-1-2-2-2-1, natural minor 2-1-2-2-1-2-2, harmonic minor 2-1-2-2-1-3-1, melodic minor asc 2-1-2-2-2-2-1, wholetone 2-2-2-2-2-2, octatonic 1-2-1-2-1-2-1-2, double harmonic major 1-3-1-2-1-3-1, neapolitan major 1-2-2-2-2-2-1, neapolitan minor 1-2-2-2-1-3-1, augmented 1-3 *3, messiaen (2 *6, 1-2 *4, 1-1-2 *3, 1-1-1-3 *2, 1-1-4 *2, 1-1-2-2 *2, 1-1-1-1-2 *2), overtone (lydian dominant) 2-2-2-1-2-1-2, altered 1-2-1-2-2-2-2, pentatonic (diatonic 2-2-3-2-3, pelog, hirajoshi, kumoi, japanese 1-4-2-3-2), 9th chord (dom 2-2-3-3-2, maj 2-2-3-4-1, min 2-1-4-3-2), phrygian b3 3 1-2-1-1-2-1-2-2</p> <p>minor (natural, harmonic, melodic), pentatonic (diatonic, pelog, hirajoshi, kumoi, japanese 1 b2 4 5 b7), symmetric (chromatic, augmented, messiaen modes), super locrian, oriental, enigmatic, hungarian major, major locrian, lydian minor, bebop, blues, leading whole-tone, lydian #5, lydian b7, aeolian b5, locrian b4, six-tone symmetrical, prometheus, prometheus neapolitan</p> <p>Messiaen modes <br/> Major (Ionian, dorian, phrygian, lydian, dominant, aeolian/natural minor, locrian) <br/> Harmonic minor (locrian nat6, ionian #5, dorian #4, phrygian dom, lydian #2, altered dim) <br/> Melodic minor (jazz minor, dorian b2, lydian aug, lydian dom, mixolydian b6, locrian nat2, altered) <br/> Bebop (major, dominant, melodic, harmonic minor) <br/> Blues (major, minor) <br/> Halfstep between a whole step in a scale (like major). It creates 8 notes in scale so it always repeats on beat. Building teriary chords alternates between root 6 chord and diminished chord which can resolve to the root</p> <p>Multi-octave shapes</p> <p>Various small shapes invert and move up scale,</p> <p>Chords built around one or more chord tones add surrounding chromatic notes or harmonic notes (b11#1235 or 71235 for a 135), chord clusters like notes adjacent notes in wholetone or chromatic</p> <p>Practice scales with every transposition of finger placement</p> <p>Octatonic (HW): [1]dim [1]7b9#11 [1]7#9#11 [1]-7b9#11 [1]-#9#11 [1]-#9nat11nat13. [2]alt,nat13, [2](1 4 b6 9) <br/> Double harmonic major: 1+7 2+7 3-b7 4-M7 5-b5 6+#5 7o#5</p> <h2 id="shapes">Shapes</h2> <p>Note that ultimately the brain learns how things are used. Learn chords in a way that they are used. Take some sequence of chord shapes, sequence of notes, rhythm, technique, and chunk various combinations.</p> <p>Stack intervals 2 3 4 5 (6 7 alone are too far away to stack them to form complex chords, but can create spacey feels, additionally have similar harmonic function to 2 3). Can combine different stacks (4s with a 3 on top, 2s (clusters) and 3s, etc.) or drop notes (2 3). Use pools like altered to diverge from harmony. This subsumes the common chord voicings of: drop 2 and 3, upper structures, quartal and quintal, cluster, altered chords</p> <p>2s/7s/9s: chromatic, octatonic, whole tone, augmented, diatonic, pentatonic <br/> 3s/6s/13s: 7th (3571 5137 1357 1537), 9th (135796 135794), 11th (135794 154 15794), 13th (135796 57961 3796 7961 1536 1536 neapolitan:#461b3), diminished (1 b5 bb7 b3) <br/> 4s/5s/11s: dominant quartal chords (c f# bb eb), quartal (⅚ notes separated by p4), so what (lh p4 rh p4 M3) <br/> Other: 1251, 5125, 1561, 7157, 7137, three 4s/5s in left hand and 3 4s/5s in right hand <br/> Voicings: use chords from octatonic, whole tone, or augmented scale; use chords a third apart; superimposed triads; polytonal chords moving the same distance; extended harmonies; cluster chords; drop 2 drop 3 voicings drop the n note from the top down an octave</p> <p>Petrushka chord, chords from mode, inversion with octaves, interval and their inversion (151 353 131 161 etc.), chromatic voicings (ala sorabji) (c c# d# f gb)</p> <h2 id="progressions">Progressions</h2> <p>Left or right circle of fifths <br/> 1436251 <br/> 16251 <br/> 251 <br/> 451 <br/> b61 <br/> 6251 <br/> secondary dominants <br/> coltrane changes</p> <h2 id="practice-variations">Practice variations</h2> <p>Quickly pluck or play and release note, pluck each note n times quickly, start from trouble passage and progressively add surrounding notes, break note and hand positions like chopin op10no1 hold note androtate wrist to play the next one or two notes or note before and after current note, slow practice pay attention to phrasing and equality of tempo and dynamics focus on musicality</p> <h2 id="rhythm">Rhythm</h2> <p>Incorporate rhythm and embellishment exercises to scales and chords <br/> Sequence of rhythm elements <br/> Polyrhythm: 2:3:4, 5:6:6, 7:11 5:7, play polyrhythm but accent every 2nd beat of one hand for example <br/> Sub divide polyrhythm into sub beats: 1234 2234 3234 4234 5234 within 7 beats</p> <p>Play the following rhythms on different subbeats: 4/4: 12 13 14 24(offbeat), 3/4: 13(swing) <br/> 12 1 12, 12 12 1, 12 2, 12 2 2, 12 2 12 2 2, 12 1 12 2, 1 2 12 12 2 12 (mephisto) <br/> L1ea R1 2a a 4a L1ea 3ea R1 2a a 4a; 1tl 2tl 3tl 4tl; 1a &amp; 3a &amp; <br/> Prokofiev: 1 12 12, 2 on 3 polyrhythm but leave out one note <br/> Polyrhythm but leave beats out <br/> Change subdivision but leave certain beats out</p> <p>4/1 1 3 1 <br/> 3/134 24 134 <br/> 3/3 2 1 <br/> 4/1 2 3 4 _ 1 <br/> 2/1 1 12 3/3 <br/> 4/13 124 1 <br/> 4/12 13 14 24 23 <br/> 3/13 (swing) <br/> 3/3 13 3 13 3 4/13 13 <br/> 5/25 13 24 <br/> 4/4 <em>1</em><strong>e</strong>a ea 4ea</p> <p>Use the following rhythms to improve speeds of altering two shapes. For each repeat also consider using different fingerings: <br/> 12, 112, 1122, 11212212, 1221,</p> <h2 id="dynamics">Dynamics</h2> <p>For each finger, combination of fingers, and wrist, memorize ppp pp p m f ff fff and be able to transition smoothly from one dynamic to another in n repetitions. Treat dynamics like rhythm subdivisions and polyrhythms.</p> <h2 id="intervals">Intervals</h2> <p>Double note scales of all intervals, play sequences of intervals changing in root notes. Play an interval moving it through a shape by sequence of intervals (12 45 67 23 or 12 34 56 71 or 13 45 27 56)</p> <h2 id="exercises">Exercises</h2> <p>Mix arpeggios, chords, scales, jumps, tremelos, finger independence, trills, etc. one per hand, different shape/sequence/rhythm per hand, same shape different inversion.</p> <p>Scale in contrary motion. on sone rhythm, play a note chromatically or within scale n degrees left or right (contrary or parallel)</p> <p>Fast arpeggio one hand with fast large jumps in the other hand <br/> Play concurrent shapes through a sequence, play sequences through shape.</p> <h3 id="concurrent">Concurrent</h3> <p>Of any scale</p> <p>pick chord moving outer notes through scale to next inversion (also try it with moving inner voicings up scale), split chord and expand both directions of scale (chromatic), large jump between shapes each shape moving through scale can do patterns like (shape jump shape shape return, etc.), pick a shape and move a certain voice through a sequence, arpeggiate chord while voicing the top note, stack chords with left and right hands,</p> <h3 id="sequential">Sequential</h3> <p>Move through a shape in many variations <br/> Move through a shape following a sequence (every nth degree, skip x degree back y degrees, connect 3 degrees skip back 2, connect 4 degrees skip forward 2, etc.), take a sequence in thirds to sixths and vice versa (a la liszt). <br/> Every possible fingering including static fingering (same fingering for every note), offsetting fingering especially for awkward fingerings</p> <p>extended 153 315 531 15135 5135 5153 113 115 116 554 163 162</p> <p>Shape, double notes of every interval (including octaves) (from the root: 1232171 21324354), include melodic octaves surrounding interval in double note scales (consider playing octaves in some complex rhythmic fashion), arpeggio, every combination of fingers (12 23 34 45 123 345 134 235 1234) (start at same degree but shift fingering every time) (cycle through fingerings), pick a degree and jump back and forth moving up the shape, fingers (like 1215 125 2152) through shape, through shape every nth degree, up scale x notes start again y notes from last start (repeat back down), scale alternating hands (both one hand per octave and dividing scale into n notes or x/y notes per hand), move a note from one shape to another through a sequential shape, move trill up shape while holding chord, one hand diverging/converging voices from shape <br/> 152 lh 613 rh going up some sequence of notes playing chords built from root shape</p> <h3 id="layering">Layering</h3> <p>Both hands arpeggio out final jump to a chord then return hands together in chord (or repeat another arppegio + chord, return to a different inversion or transposition), playing chords and connecting chords with octaves following a scale established by chord (either upcoming chord or previous chord), for each item in sequence of one shape go through sequence of another shape (121 1232 111243 3212 1413121324, etc.), arpeggio chords 5-1-35 / 13-5 / 35-1 / 2-12 / 5-13 / 51-3-6 / 5-13-6, 13 1 13 1 (up and back down an octave), 1-3-46-3-1 / 1-3-56-2-1, cycle concurrent shapes while moving through sequence, lh jumps from pedal tone to shape (1 12 1 23 1 34) targeting every other rh note and rh plays scale/arpeggio/complex arpeggio</p> <p>Practice scales all intervals and moving through scales at arbitrary sequences. intervals are the foundation of melody and harmony <br/> Arpeggio: Practice left hand moving back and forth over right hand that comes up a lot, both small and large intervals and arpeggios <br/> play 18 5 3, then again n notes up shape 18 5 b3, then again m notes down shape 18 5 3 <br/> Prokofiev practice: mix these groupings around. 138 1875 37531 7315 33 77531 753 7531 753 7531. Also swap directions. Now move sequence through a shape <br/> Combine arpeggio one hand prokofiev practice In the other at polyrhythm</p> <p>Pick two notes play octaves and jump through sequence back to center (every time or every other) in opposite directions can play chords or some shape of notes when jumping out (like the note of sequence plus its 2b3 on right hand and 7#6 and left hand)</p> <p>Take n fingers and move them through (chord) shape both within octave and large jumps (practices quickly switching chords and inverting chords)</p> <p>jump from note or notes to note or notes larger than an octave away through a sequence. combine left and right hand and some interesting rhythms. try scarbo or mazeppa like interval ghost note (maybe with octave) or la clochette arpeggio before jumping back</p> <p>arpeggios (potentially longer than an octave) but changing one voice (or multiple voices). Also do with some chord shape</p> <p>Polyrhythm scales and arpeggios</p> <p>exercises for organ</p> <p><strong>Mastery</strong> <br/> Sight reading, chords and scales, and exercises on those chords and scales <br/> For any exercise: aim for two hands going in parallel and contrary motion with different harmony and rhythm; quickly switch between harmonies, patterns, and directions <br/> Sight reading <br/> Finger independence: there is 5 choose k with k ranging from 1 to 5, or 31, different combinations of fingers in a hand. 961 different binary alterations and 29791 triary alterations. There is 961 different combinations of left and right hand finger patterns. It is somewhat unrealistic to master all 961 and 29791 patterns, so approximate them with a good subset choice. Master a couple individual patterns but also try a speed run. <br/> Scales: with every possible fingering, all intervals, movement between intervals, different scale and interval each hand, ondine descending scale pattern, scale with subdivisions <br/> Chords: memorize every 3 note chord and most 4 note chords, absolute, from scales, chromatic, voicings, broken chords, chromatic movements like prokofievs chords <br/> Arpeggios: runs, arpeggio with subdivision <br/> Octaves <br/> Large jumps from one finger to another (sequence from scale or chord, jump intervals, jump to 2 through 6 chords)</p> <p><strong>Pedal</strong> <br/> Similar to dynamics, master various fractional pedals.</p> <p><strong>Intervals</strong> <br/> All intervals with lowest note doubled. Jump octaves or other intervals through intervals <br/> thumb and another finger in sequence up octave playing different intervals with each hand and offsetting when thumb of each hand plays <br/> quickly move fixed interval and fingering through sequence for each finger (like chromatic thumb harmonic major index) or use same sequence, make sequence more complex, incorporate jumps given by sequence <br/> alternate converging or diverging notes with one hand of shape, once completed, switch shapes and do the reverse, continuously switch shapes <br/> jump from one interval to another, do sequence of various intervals, jump from interval to base note with interval moving with shape (and bass to maybe) <br/> <strong>Finger proprioception</strong> <br/> Jump individual fingers, finger combinations, or altering fingers through octaves, chords, scales, and other shapes. Work on all in parallel. <br/> <strong>Finger independence</strong> <br/> Master the following at max speeds and all tempos and staccato and legato: quickly repeat a finger, all combinations of 2 finger and 1 finger alterations, combine both hands, quickly switch between various 2 and 2 finger alterations while progressing through shapes, chromatic scale (and other shapes) with all sequences of fingers as smoothly as possible moving up shape (or play n notes of scale then change the root by a sequence) <br/> Vary the dynamics per finger <br/> For most finger independence exercises, pretend like you’re pushing your finger through the surface as it trains you to play on keyboards with heavy action <br/> quickly playing a single note with single finger, control dynamics and rhythm (change position of other fingers on piano relative to playing finger) <br/> trills <br/> move finger intervals through shape is critical for really understanding chords and playing them fast. Master it, do it quickly amd smoothly for all intervals through all shapes <br/> hold one finger on piano key and move other finger between two notes (great for 45 and 34 and 345). make sound quality very consistent <br/> play interval descending n degrees through scale after n jump an octave and continue <br/> Trills and tremelos between all combinations of fingers (1 finger to 1 finger and 1 finger to 2 fingers)</p> <p>For every combination of fingers repeated as fast as possible, then try it with some rhythm, combine both hands, do a different rhythm per hand or different finger combination, alternate between finger combinations, try with mixture of legato staccato and marcato <br/> Hold n fingers constant and alternate between the other fingers <br/> Where letters represent fingers 1-5: abe ab abd ab abc, abe ab ae abd ab ad abc ab ac <br/> Where numbers index combinations in left hand and letter index combinations in right hand: 1a 1b 2b, 1a 1b 2a 1b 1a 2b</p> <p>Double notes: trill one voice and every 3 notes play the other voice <br/> Chopin op. 10 no. 2: left and right hand <br/> Chopin op. 25 no. 6: some fingerings are 14-25 13-25 24-15 13-24 <br/> Liszt op. 139 no. 5 (transcendental etude feux follets): play the high voice legato and low voice staccato</p> <p><strong>Wrist rotation</strong> <br/> playing neighboring notes, 51 511 5111 15 155 1555 1515, 54 legato</p> <h3 id="shape-piece">Shape piece</h3> <p>For shapes built from a small number of small intervals <br/> Play as grace notes in some sequence land on the final note repeat the pattern by a sequence</p> <h3 id="chord">Chord</h3> <p>Foundations: chords made of intervals, chords based on scale degrees, chord symbols <br/> Alterations: sus, 6 chords, replacing notes, omitting notes (power chords, less tension), adding notes <br/> Processing speed: alternate base notes, stacking chords (easier to process than complex chord symbols), polychords</p> <p>Memorize all 3 and 4 note chords, then learn how to play any technique given any 3 or 4 note chord. <br/> Chords with similar intervals sound similar (minor and major) and there is less ability to distinguish them. <br/> internalize each chord, then learn which chords go to each scale <br/> <strong>Chords:</strong> <br/> Start with min maj dim aug dim7 min7b5 min7 maj7 dom aug7 minmaj7 <br/> trichord, tetrachord, every secundal chord of 3 notes, octaves with interval, every interval <br/> 12 × 12 (mm Mm mM MM mmm mMm mmM mMM Mmm MMm MmM MMM) × 6inv + 12 × 5 (min maj dim aug sus2 sus4) × 3inv + 12 × 9 (min7 dim dimo maj7 min∆7 dom aug∆7 aug7 minaug∆7) × 8inv.</p> <p><strong>Memorize chords:</strong> <br/> Internalize every modification (sharp and flat), their sub chord, and their inversions, and their chromatic movement along any scale. Arpeggios, finger exercises, jumps from n fingers, combine hands, jump to inverted and shifted chord <br/> Treat like sight reading: pick chord quality and inversion for each hand, move each hand through sequence playing that chord eventually changing the inversion and octave everytime. Load as much as you can into working memory, play as quickly as possible allowing a few mistakes (just like sight reading). <br/> also change chord qualities every chord from arbitrary sequence <br/> play each chord in shape <br/> for a shape, play chord in shape then move one note forward (normally right most finger then second right most or 2 right most notes or other patterns) be aware of what chord and inversion is going to be next <br/> play chord progression. once resolve change next target chord by sequence (chord progression, scale, chromatic)</p> <p>chord, inversion, left and right hand jumps to inversions contrary and parallel, octave movements, interval jumps and movements, inversions more than an octave <br/> <strong>Chord techniques</strong> (the following with every combination between hands in parallel and contrary): inversion (root to 1 to 2 and root to 2 to 1) both within an octave and more than an octave (135 -&gt; 153 or 145 -&gt; 4 1 5) (for inversions name chord based on new root), jumps, note accents and even playing, broken chords, arpeggios, arpeggios with doubled notes, finger independence, various octaves on sequences built from chords, chromatic movements, alternate chord sequence with both hands, repeated notes, grace chords (a la prokofiev), alternate between two chords or arpeggios, play chord doubling the lowest note but play the middle notes an octave lower or higher (mereaux), play chord with highest or lowest note trill using a scale to connect notes in chord using chord notes as target notes, jumping n fingers along inversions or other jumps, playing outer octave and inner tremolo while inverting and shifting chord, arpeggio while chromatically shifting chord or inverting it <br/> <strong>Connecting chords:</strong> play chord jumping through all intervals (1 through 16 semitones and multiple octaves), play same chord changing root from sequence of shape, play all chords built from 2s 3s 4s of pool, what chords are closely associated to which scales, also how to move from one chord to another, what voicings connect what chords, what sequences connect what chords, what chords are harmonically similar, functional roles of chords. Create various chord progressions utilizing all chords and play in all transpositions and various inversions. Play sequence of chord qualities and inversions and octaves different in each hand</p> <p>Memorize all chords and scales to open your palette. <br/> <strong>3-note spread:</strong> <br/> F C G (sus) <br/> 2-5 <br/> <strong>4-note spread:</strong> <br/> F C D, F G D (m7no5, 7no3) <br/> 2-3, 3-2 <br/> <strong>5-note spread:</strong> <br/> F C A, F D A (major triad, minor triad) <br/> F G A (2 whole steps) <br/> 4-3, 3-4, 2-2 <br/> <strong>6-note spread:</strong> <br/> F C E, F A E (M7no3, M7no5) <br/> F G E, F D E (half step + whole step) <br/> 4-1, 1-4, 1-2, 2-1 <br/> <strong>7-note spread:</strong> <br/> F C B, F E B (P5th with tritone) <br/> F G B, F A B (7no5, 9no5no1) <br/> F D B (dim triad) <br/> 1-5, 5-1, 2-3, 3-2, 3-3 <br/> <strong>8-note spread:</strong> <br/> F G F♯ = F E F♯ (two half steps) <br/> F D F♯, F A F♯ (half step + Aug2nd/min3rd) <br/> 1-1, 3-1, 1-3 <br/> <strong>9-note spread:</strong> <br/> F A C♯ (Aug triad) <br/> 4-4</p> <p>By form: <br/> Triad: Major Minor Augmented Diminished Suspended <br/> Seventh: Leading-tone Major Minor Dominant Dominant seventh flat five Diminished Half-diminished Diminished major Minor-major Augmented major Augmented minor Altered seventh Nondominant Harmonic seventh <br/> Extended: Ninth Eleventh Thirteenth Upper structure Dominant 7♯9 Polychord Tone cluster <br/> Added / omitted: Sixth Augmented sixth Lydian Seven six <br/> Specific: Alpha Bridge Complexe sonore Dream Elektra Farben Grandmother Magic Mystic Northern lights Petrushka Psalms So What Tristan Viennese trichord <br/> General: Mixed interval Secundal Tertian Quartal Synthetic chord Tetrad</p> <p>By function: <br/> Diatonic: Tonic Supertonic Mediant Subdominant Dominant Submediant Leading-tone / Subtonic <br/> Altered: Approach Borrowed Chromatic mediant Neapolitan Passing <br/> Secondary: Secondary dominant Secondary leading-tone Secondary supertonic <br/> Other: Common Contrast Primary triad Subsidiary Substitute</p> <p><a href="https://en.wikipedia.org/wiki/Tetrad\_(music)">https://en.wikipedia.org/wiki/Tetrad_(music)</a></p> <p>1-2-1-8 1-2-2-7 1-2-3-6 1-2-4-5 1-2-5-4 1-2-6-3 1-2-7-2 1-3-1-7 1-3-2-6 1-3-3-5 1-3-4-4 1-3-5-3 1-3-6-2 1-4-1-6 1-4-2-5 1-4-3-4 1-4-4-3 1-4-5-2 1-5-1-5 1-5-2-4 1-5-3-3 1-5-4-2 1-6-2-3 1-6-3-2 1-7-2-2 2-2-2-6 2-2-3-5 2-2-4-4 2-2-5-3 2-3-2-5 2-3-3-4 2-3-4-3 2-4-2-4 2-4-3-3 3-3-3-3</p>]]></content><author><name></name></author><category term="music"/><summary type="html"><![CDATA[Don’t try so hard to generalize and learn every conceivable technique. Instead, learn difficult technical pieces like Liszt and Chopin etudes, and master them completely. afterward develop general techniques for other roots, chords, and scales. other than learning pieces, focus on memorizing scales and chords.]]></summary></entry><entry><title type="html">Music Scales</title><link href="https://t-wilkinson.github.io/blog/2024/music-scales/" rel="alternate" type="text/html" title="Music Scales"/><published>2024-07-24T14:39:19+00:00</published><updated>2024-07-24T14:39:19+00:00</updated><id>https://t-wilkinson.github.io/blog/2024/music-scales</id><content type="html" xml:base="https://t-wilkinson.github.io/blog/2024/music-scales/"><![CDATA[<p>Super Ultra Hyper Mega Meta Scales<br/> Metalydian is a symmetric pattern that does not repeat each octave (WWWH)</p> <p>Mix up scales:</p> <ul> <li>Mixolydian Lydian</li> <li>M3 m6 M7</li> <li>m3 M6 M7</li> <li>Phrygian M3</li> </ul> <h2 id="messiaen-modes">Messiaen modes</h2> <ol> <li>Whole tone, 2 transpositions, 1 mode</li> <li>Diminished, 3 transpositions, 2 modes</li> <li>WHHWHHWHH (1 2 ♭3 3 ♯4 5 6 ♭7), 3 transpositions, 2 modes</li> <li>H-H-3H-H-H-H-3H-H, 6 transpositions, 4 modes</li> <li>H-4H-H-H-4H-H, 6 transpositions, 3 modes</li> <li>W-W-H-H-W-W-H-H, 6 transpositions, 4 modes</li> <li>H-H-H-W-H-H-H-H-W-H, 6 transpositions, 5 modes</li> </ol> <p>Truncation:<br/> Removing notes from a mode to leave a truncated mode. The removed notes and remaining notes must preserve the symmetry of the parent mode.<br/> Mode 7 and 3 are not truncated modes, the other modes can be constructed from them or from one or more of their modes. Mode 7 contains 1 2 3 4 5 6, mode 6 contains 1 5, mode 4 contains 5, mode 3 contains 1.</p> <h2 id="5-note-scales">5 note scales</h2> <p>5 note scales are generally more thought of melodically, connecting various harmonic contexts. A collection of intervals that tie together various harmonies starting and ending harmonically arbitrarily, instead appealing to need. One is not so concerned with forming chords from these small scales, instead one should explore chords and their variations in their own right.</p> <p><strong>9th chord</strong>: 1 3 5 7 9</p> <p>Major pentatonic has 5 consecutive pitches from the circle of fifths transposed within the same octave.</p> <p><strong>Pentatonic</strong>:</p> <p>major: 1 2 3 5 6 (Lydian Ionian Mixolydian)<br/> blues major: 1 2 4 5 6 (Mixolydian Dorian Aeolian)<br/> suspended: 1 2 4 5 ♭7 (Dominant Dorian Aeolian)<br/> minor: 1 ♭3 4 5 ♭7 (Dorian Aeolian Phygian)<br/> blues minor: 1 ♭3 4 ♭6 ♭7 (Aeolian Phygian Locrian)</p> <p><strong>Hirajoshi</strong>: 1 3 ♯4 5 7 2W-W-H-2W-H (0,4,6,7,11)<br/> <strong>Miyako-bushi on D</strong>: 1 ♭2 4 5 ♭6 H-2W-W-H-2W (0,1,5,7,8)<br/> <strong>Insen</strong>: 1 ♭2 4 5 ♭7 H-2W-W-3H-W (0,1,5,7,10)<br/> <strong>Iwato</strong>: 1 ♭2 4 ♭5 ♭7 H-2W-H-2W-W (0,1,5,6,10)</p> <h2 id="6-note-scales">6 note scales</h2> <p><strong>Blues</strong>: [(b)] 1 ♭3 4 ♭5 5 ♭7 3H-W-H-H-3H-W (0,3,5,6,7,10)<br/> - Minor chord with chromatic passing note between 4 and 5 and no 2 or 6 (to avoid clashing with 3 and 5)<br/> <strong>Augmented</strong>: 1 ♭3 3 5 ♯5 7 3H-H-3H-H-3H-H (0,3,4,7,8,11)<br/> - 3 Major chords and 3 Minor chords<br/> <strong>Tritone</strong>: [(+)♭7♭9♯11] 1 ♭2 3 ♭5 5 ♭7 H-3H-W-H-3H-W (0,1,4,6,7,10)<br/> - Two major chords a tritone away<br/> <strong>Whole tone</strong>: 1 2 3 ♯4 ♯5 ♯6 W-W-W-W-W-W (0,2,4,6,8,10)<br/> - also: 1 2 3 ♭5 ♯5 ♭7<br/> - similar to augmented scale<br/> <strong>Scale of harmonics</strong>: 1 ♭3 3 4 5 6 3H-H-H-W-W-3H (0,3,4,5,7,9)<br/> <strong>Istrian</strong>: 1 ♭2 ♭3 ♭4 ♭5 5 H-W-H-W-H-5H (0,1,3,4,6,7)<br/> <strong>Prometheus</strong>: 1 2 3 ♯4 6 ♭7 W-W-W-3H-H-W (0,2,4,6,9,10)<br/> <strong>Two-semitone tritone</strong>: 1 ♭2 ♮2 ♯4 5 ♭6 H-H-4H-H-H-4H (0,1,2,6,7,8)</p> <h2 id="major">Major</h2> <p><strong>Lydian</strong>: [(+)♯11] 1 2 3 ♯4 5 6 7 W-W-W-H-W-W-H (0,2,4,6,7,9,11)</p> <blockquote> <p>I II iii ♯iv⁰ V vi vii<br/> Lydian augmented: [(+)♯5♯11] 1 2 3 ♯4 ♯5 6 7 W-W-W-W-H-W-H (0,2,4,6,8,9,11)<br/> Lydian dominant: [(+)♭7♯11] 1 2 3 ♯4 5 6 ♭7 W-W-W-H-W-W-H (0,2,4,6,7,9,10)</p> </blockquote> <p><strong>Ionian</strong>: [(+)] 1 2 3 4 5 6 7 W-W-H-W-W-W-H (0,2,4,5,7,9,11)<br/> - I ii iii IV V vi vii⁰<br/> - Consonant with first 6 harmonics (the seventh harmonic is a minor seventh)<br/> <strong>Mixolydian</strong>: [(♭7)] 1 2 3 4 5 6 ♭7 W-W-H-W-W-H-W (0,2,4,5,7,9,10)<br/> - I ii iii⁰ IV v vi ♭VII<br/> - Becuase there is only ♭7, only every other chord is changed (1, 2, 4, 6) stay the same because they don’t involve 7<br/> - Consonant with first 10 harmonics (the 11th harmonic is a tritone)<br/> - Flat 7 gives less direction<br/> - Chords are less resolved<br/> - More relaxed and ambiguous<br/> - Straying from root gives less tension<br/> <strong>Mixolydian ♭6</strong>: [(♭7)♭6], [(-)♯3] 1 2 3 4 5 ♭6 ♭7 W-W-H-W-W-H-W (0,2,4,5,7,8,10)<br/> <strong>Dorian</strong>: [(-)♯13] 1 2 ♭3 4 5 6 ♭7 W-H-W-W-W-H-W (0,2,3,5,7,9,10)<br/> - i ii ♭III IV v vi⁰ ♭VII<br/> - Step from 3 to 4 is larger and increases brightness<br/> <strong>Aeolian</strong>: [(-)] 1 2 ♭3 4 5 ♭6 ♭7 W-H-W-W-H-W-W (0,2,3,5,7,8,10)<br/> - i ii⁰ ♭III iv v ♭VI ♭VII<br/> - Relative natural minor<br/> - Lowest interval that respects the circle of fifths<br/> <strong>Aeolian ♭5</strong>: [(♭5)] 1 2 ♭3 4 ♭5 ♭6 ♭7<br/> <strong>Phrygian</strong>: [(-)♭9] 1 ♭2 ♭3 4 5 ♭6 ♭7 H-W-W-W-H-W-W (0,1,3,5,7,8,10)<br/> - i ♭II ♭III iv v⁰ ♭VI ♭vii<br/> - Notes are fairly same distance which leads to a slippery slope towards the root<br/> <strong>Locrian</strong>: [(♭5)♭9] 1 ♭2 ♭3 4 ♭5 ♭6 ♭7 H-W-W-H-W-W-W (0,1,3,5,6,8,10)<br/> - i⁰ ♭II ♭iii iv ♭V ♭VI ♭vii<br/> - The two halfs before the 5<br/> - Doesn’t respect circle of fifths<br/> - Tritone between the 1 and 5<br/> <strong>Locrian ♯2</strong>: [(♭5)] 1 2 ♭3 4 ♭5 ♭6 ♭7 W-H-W-H-W-W-W (0,2,3,5,6,8,10)<br/> <strong>Super locrian</strong>: [(♭5)♭9♭11] 1 ♭2 ♭3 ♭4 ♭5 ♭6 ♭7 H-W-H-W-W-W-H (0,1,3,4,6,8,10)<br/> <strong>Major Locrian</strong>: [(♭5)♯3], [(+)♭5♭7♭13] 1 2 3 4 ♭5 ♭6 ♭7 W-W-H-H-W-W-W (0,2,4,5,6,8,10)</p> <p><strong>Melodic minor (Jazz Minor)</strong>: [(+)♭3] 1 2 ♭3 4 5 6 7 W-H-W-W-W-W-H (0,2,3,5,7,9,11)<br/> <strong>Acoustic scale</strong>: 1 2 3 ♯4 5 6 ♭7</p> <ul> <li>Minor := 1-2.3-4-5.6-7-8 <ul> <li>Natural := ♭3 ♭6 ♭7 <ul> <li>Whole step between 7 and 8 didn’t provide the right tension</li> </ul> </li> <li>Harmonic := ♭3 ♭6 <ul> <li>3 half steps between 6 and 7</li> </ul> </li> <li>Melodic := ♭3 <ul> <li>1 whole step between 6 and 7</li> </ul> </li> </ul> </li> </ul> <h2 id="harmonic">Harmonic</h2> <p><strong>Harmonic minor</strong>:<br/> <strong>Ukrainian Dorian</strong>: 1 2 ♭3 ♯4 5 6 ♭7 W-H-3H-H-W-H-W (0,2,3,6,7,9,10)<br/> <strong>Harmonic minor</strong>: 1 2 ♭3 4 5 ♭6 7 W-H-W-W-H-3H-H (0,2,3,5,7,8,11)<br/> <strong>Phrygian dominant</strong>: 1 ♭2 3 4 5 ♭6 ♭7 H-3H-H-W-H-W-W (0,1,4,5,7,8,10)</p> <p><strong>Double harmonic major</strong>:<br/> 1. Double harmonic major: 1 ♭2 3 4 5 ♭6 7 H-3H-H-W-H-3H-H (0,1,4,5,7,8,11)<br/> 4. Hungarian Gypsy minor: 1 2 ♭3 ♯4 5 ♭6 7 W-H-3H-H-H-3H-H (0,2,3,6,7,8,11)</p> <p><strong>Harmonic major</strong>: 1 2 3 4 5 ♭6 7 W-W-H-W-H-3H-H (0,2,4,5,7,8,11)</p> <h2 id="others">Others</h2> <p>Neapolitan major: 1 ♭2 ♭3 4 5 6 7 H-W-W-W-W-W-H (0,1,3,5,7,9,11)<br/> - Major chord built from flat 2 of major scale<br/> Neapolitan minor: 1 ♭2 ♭3 4 5 ♭6 7 H-W-W-W-H-3H-H (0,1,3,5,7,8,11)<br/> Chromatic: 1 ♯1 2 ♯2 3 4 ♯4 5 ♯5 6 ♯6 7 | 7 ♭7 6 ♭6 5 ♭5 4 3 ♭3 2 ♭2 1 H-H-H-H-H-H-H-H-H-H-H-H (0,1,2,3,4,5,6,7,8,9,10,11)<br/> Half diminished: 1 2 ♭3 4 ♭5 ♭6 ♭7 W-H-W-H-W-W-W (0,2,3,5,6,8,10)</p> <p>Blues 7 note: 1 ♭3 3 4 ♭5 5 ♭7<br/> Major bebop: 1 2 3 4 5 (♯5/♭6) 6 7 W-W-H-W-(H-H)-W-H (0,2,4,5,7,(8),9,11)<br/> - Derived by adding chromatic passing notes from major scale modes<br/> Bebop dominant: 1 2 3 4 5 6 ♭7 7 W-W-H-W-W-H-H-H (0,2,4,5,7,9,10,11)</p> <p>Algerian: 1 2 ♭3 ♯4 5 ♭6 7 etc. W-H-3H-H-H-3H-H-W-H-W (0,2,3,6,7,9,11,12,14,15,17)<br/> Vietnamese of harmonics: 1 three quarter flat3 ♭3 ♮3 4 5 5Q-Q-H-H-W (0,5/2,3,4,5,7)<br/> Acoustic: 1 2 3 ♯4 5 6 ♭7 W-W-W-H-W-H-W (0,2,4,6,7,9,10)<br/> Octatonics: 1 2 ♭3 4 ♭5 ♭6 6 7 W-H-W-H-W-H-W-H (0,2,3,5,6,8,9,11) | 1 ♭2 ♭3 3 ♯4 5 6 ♭7 H-W-H-W-H-W-H-W (0,1,3,4,6,7,9,10)</p> <p>Hungarian major: 1 ♯2 3 ♯4 5 6 ♭7 3H-H-W-H-W-H-W (0,3,4,6,7,9,10)<br/> Enigmatic: 1 ♭2 3 ♯4 ♯5 ♯6 7 H-3H-W-W-W-H-H (0,1,4,6,8,10,11)<br/> Gypsy: 1 2 ♭3 ♯4 5 ♭6 ♭7 W-H-3H-H-H-W-W (0,2,3,6,7,8,10)<br/> Flamenco: 1 ♭2 3 4 5 ♭6 7 H-3H-H-W-H-3H-H (0,1,4,5,7,8,11)<br/> Persian: 1 ♭2 3 4 ♭5 ♭6 7 H-3H-H-H-W-3H-H (0,1,4,5,6,8,11)</p>]]></content><author><name></name></author><category term="music"/><summary type="html"><![CDATA[Super Ultra Hyper Mega Meta Scales Metalydian is a symmetric pattern that does not repeat each octave (WWWH)]]></summary></entry><entry><title type="html">Deep Learning</title><link href="https://t-wilkinson.github.io/blog/2024/deep-learning/" rel="alternate" type="text/html" title="Deep Learning"/><published>2024-07-24T14:00:45+00:00</published><updated>2024-07-24T14:00:45+00:00</updated><id>https://t-wilkinson.github.io/blog/2024/deep-learning</id><content type="html" xml:base="https://t-wilkinson.github.io/blog/2024/deep-learning/"><![CDATA[<blockquote> <p>Deep learning is concerned with developing algorithms that learn through insights from cognitive neurscience</p> </blockquote> <p>Deep Feedfoward Networks == Feedforward neural networks == Multilayer perceptrons</p> <p>MLP are function approximation machines to achieve statistical generalization occasionally drawing insights from neuroscience to drive innovation. The fact that MLP are function approximation machines (and that they don’t require domain knowledge) is perhaps their greatest strength. The ability to approximate any prohibitively complex and computationally intensive function gives rise to their impressive generality.</p> <p>Deep learning models, particularly LLM are generally over-parameterized. This allows certain techniques such as LoRA for parameter efficient fine-tuning and model distillation techniques such as DistilBERT.</p> <p>In a computational graph, every node knows its inputs, outputs, operation, and backpropagation.</p> <h1 id="important-equations">Important equations</h1> <ul> <li>\(J(\theta)=-\mathbb{E}_{X,Y\sim\hat{p}_{data}}[log~p_{model}(y;x,\theta)]+\lambda\Omega(\theta)\).</li> <li>\(G^i := \frac{\partial J}{\partial u^j} \frac{\partial u^j}{\partial u^i} = \sum_j f^j.backprop(u^i, u^j.inputs, G^j) = \frac{\partial J}{\partial u^j} \sum_{path(u^(\pi_1=j), \ldots, u^(\pi_t=i))} \prod_{k=2}^{k=t}\frac{\partial u^{\pi_k}}{\partial u^{\pi_{k-1}}}\).</li> <li>\(\theta^i \leftarrow \theta^i - \epsilon G^i\).</li> <li>\(f^i(u^j)=u^i\).</li> <li>\(\frac{\partial}{\partial \theta}\mathbb{E}_X [f(x,\theta)] = \frac{\partial}{\partial \theta}\int_{-\infty}^{\infty}p(x)f(x,\theta)dx = \int_{-\infty}^{\infty}p(x)\frac{\partial}{\partial \theta}f(x,\theta)dx = \mathbb{E}_X [\frac{\partial}{\partial \theta}f(x,\theta)]\). <ul> <li>Gradient descent can sum across the model applied to each data element, or to the model applied to the entire dataset</li> </ul> </li> </ul> <h1 id="architecture">Architecture</h1> <ul> <li>Output should be the smallest amount of structurless data that can derive data with necessary structure</li> <li>Weights pre activation represent how feature configurations should be activated</li> <li>Weights post activation represent the values of activated values</li> <li>Difference between the two interpretations (computation vs. variation)</li> <li>Each layer extracts more structure from the previous layer</li> </ul> <p>Activation functions:</p> <ul> <li>Output units <ul> <li>Linear units</li> <li>Bernouli</li> <li>Softmax <ul> <li>Softmax is a differentiable version of the max element function.</li> <li>A temperature parameter can be included. Increasing the temperature flattens the curve, increasing the entropy, wherease decreasing the temperature roughens the curve, decreasing entropy of the distribution.<br/> ReLU: Specifics specify slope of negative input; absolute/leaky ReLU; learned parameters</li> </ul> </li> </ul> </li> </ul> <p>Layer:</p> <ul> <li>\(h^{(n)}_i\) is the result of a linear functional applied to \(h^{(n-1)}\). It weighs each feature of the previous layer and captures the essence in a single number.</li> <li>A layer is mapped to a scalar (unit) to detect a feature <ul> <li>A covector</li> <li>Creates new feature that weights features in previous layer</li> </ul> </li> <li>A layer in a MLP is a sequential step in a computer program. Each layer is a collection of parallel steps. <ul> <li>Do as much parallel computation as reasonable to power next layer</li> </ul> </li> <li>Feed forward networks is a composition of vector functions with n layers</li> <li>Each layer is a linear map (vector function, mapping an input vector to output vector) <ul> <li>Linear maps imply variables don’t interact (with other variables or themselves)</li> <li>There is no generative effect relative to linear relationships</li> </ul> </li> <li>Each layer can express a different transformation, computation, relationship, or action in a sequence</li> <li>Function applications are interspersed with a feature mapping</li> <li>That is the input to any function is an extraction of features of the input</li> <li>No individual layer is determined by the training data</li> <li>Linear layer maps input to a domain where activation function can meaningfully solve a problem</li> <li>Deep learning layers continuously update the input like a function</li> <li>Makes several updates or refinements of the representation of the data</li> <li>The dot product acts as a feature extractor. Each vector represents a feature in vector space and the degree of similarity between two vectors determines how similar the features are.</li> </ul> <h1 id="papers">Papers</h1> <ul> <li>Architectures <ul> <li>CNN</li> <li>Self organizing map</li> </ul> </li> <li>Techniques <ul> <li>ResNet</li> <li>Dropout</li> </ul> </li> <li>Foundations <ul> <li>Point neurons</li> <li>Perceptron</li> </ul> </li> <li>Seminal papers/concepts <ul> <li>Batch training</li> <li>Stocastic gradient descent</li> <li>Autograd</li> <li>Pytorch</li> <li>Point neuron</li> <li>Perceptron</li> <li>Droput</li> <li>Normalization</li> <li>ResNet</li> <li>Transformer</li> </ul> </li> </ul> <h1 id="considerations">Considerations</h1> <p>Position dependencies. The distance between two inputs or two outputs is an important tradeoff. Gaining more information from distant positions can result in an exponential increase in complexity. Attention mechanisms solve this issue by learning context dependent attention mechanisms.</p> <h1 id="pitfalls">Pitfalls</h1> <p>An important of caveat of many deep learning models is the single stacked layers of feature detectors. This leads to a complexity and confusion as every parameter has some affect on how the input and output is interpreted. Transformers partly solve this issue, but it still falls short.</p>]]></content><author><name></name></author><category term="ai"/><summary type="html"><![CDATA[Deep learning is concerned with developing algorithms that learn through insights from cognitive neurscience]]></summary></entry><entry><title type="html">Artificial General Intelligence</title><link href="https://t-wilkinson.github.io/blog/2024/artificial-general-intelligence/" rel="alternate" type="text/html" title="Artificial General Intelligence"/><published>2024-07-24T08:47:22+00:00</published><updated>2024-07-24T08:47:22+00:00</updated><id>https://t-wilkinson.github.io/blog/2024/artificial-general-intelligence</id><content type="html" xml:base="https://t-wilkinson.github.io/blog/2024/artificial-general-intelligence/"><![CDATA[<ul> <li>Artificial sentient intelligence <ul> <li>Consciousness</li> <li>Philosophy</li> </ul> </li> <li>Cognition <ul> <li>Meta cognition</li> </ul> </li> <li>Agent</li> <li>Collective agents</li> <li>Neuroscience <ul> <li>Cortical circuitry</li> <li>Neocortex</li> <li>Cortical columns</li> </ul> </li> <li>Two approaches are expert systems and learning systems</li> <li>Meta-learning <ul> <li>HER</li> <li>Hierarchical</li> </ul> </li> </ul> <p>Why are humans able to generalize so well?</p> <p>The primary computation in the brain is deciding which neurons should fire. The attention mechanisms in transformers do this as well, in a continuous way.</p> <p>Abstractions and abstract models are combinations of a lot of different high-level details of various modalities and types of information. Different pathways in neocortex have different information resolutions. But the real abstract ideas and models, like in the frontal cortex, should have more lower resolution inputs from various modalities. In addition to some high resolution inputs.</p> <p>Current AI models lack creativity in the way humans have it.<br/> How can A* search with q-learning heuristic functions factor into this?<br/> Models lack some spatial understanding where it can map various modalities onto each other or from each other. What type of architecture would allow this? But they can spatially embed words and stuff.<br/> Is there something in the transformer architecture that adding MoE with cortical-like connections will enable otherwise impossible or inefficient computations?<br/> Review the thalamic nuclei, and neocortical and thalamic circuits. How does a transformer relate to these concepts?<br/> What are the different types of reinforcement learning? How can my theories be used to create models or approximate other reinforcement learning functions?<br/> Different modalities should abstract the representation of information for that modality in useful ways. Combining these representations is the next step, and developing more modalities based on these representations is the key to generalization.<br/> Self-play and synthetic data: perhaps building its own physical models or synthetic data, then training itself on that, then occasionally coming back to the real world and with a sharp eye of discernment, determine what aspects of its updated self and its models are wrong.<br/> Deep learning uses insights from cognitive neuroscience to build model architectures, then empirically reflect and realize what cognitive functions the architecture closely approximates.<br/> Transformers are perhaps most similar to something between thalamic-nuclei sections (the bits of cortex assigned to some thalamic nuclei, V1, V2, orbitofrontal cortex) and cortical columns. Cortical columns have feedback, context, and feedfoward inputs. They send motor control outputs and seperate outputs to other areas of the neocortex and to the thalamus. This is perhaps the missing element of generalization. Logic, spatial knowledge, and abstract understanding are simply abstract cortical functions that can be incorporated in AI models with the right architectures.<br/> AGI can learn to download specific models and run them, it just needs access to powerful models. It can also have access to cloud computing and AI papers, implement and train and generate data for its own models. It can be a MoE with infinite more experts at its disposal.<br/> Problem with deep neural networks is they are not contextual. Each layer stores a massive model for all possibilities. It deters specialization. A more complete ai would require multiple sub models that are contextual. The context is selected by the inputs/input model. Will need lots of money to train or destroyed training. How does location fit inti transformer like model? Or deep learning framework in general? What is missing from current 1000 brain theory?<br/> Self play (produces novel solutions). How to get self-play for models like LLMs?<br/> Needs to form an abstract spatial representation of various entities or shapes in a scene. Needs models of how physical objects and agents act, behave, and move. I can literally test this out now.<br/> A transformer generalization augmentation based on a theory of cortical columns.<br/> It needa a working memory. Mapping its context to understandings of the problem, relevant entities, etc. The working memory is a model-controlled context.<br/> When taking a test, you have a model for each section, and you attach a context to each section. The section in your mind is a sort of graph, a sequence of nodes where each node has its associated context.<br/> Emotions are certainly not particularly necessary for generally intelligent beings, as they are essentially contextual responses to the environment. They certainly have evolutionary benefits.<br/> Perhaps generalization requires coordination or a priori preference for abstracting and losing information.<br/> The large size of transformers compensates for the lack of context, feedback, and feedforward inputs of pyramidal neurons and the inhibitory circuits of cortical columns.<br/> Self-training and generalization capabilities for things like agents, reasoning, etc. are perhaps the final step.<br/> How do reasoning columns actually transfer to doing reasoning? If you read, some columns activate neurons representing the information content. Reasoning takes these activations and some additional context, and runs computations. But those computations must lose some information content. The reasoning areas must represent some common reasoning patterns, and there exists mappings from those reasoning areas to output language areas or other things.<br/> MoE is an approximation to cortical columns. Generalization should happen through introducing more MoE that take other expert outputs as inputs. I need to learn more about the cortical column computations and relationships it encodes. Or perhaps feed intermediate transformer tokens (like in the middle) to more abstract experts.</p> <p>I should organize my thoughts on how I learn, various processes, etc. and abstract and develop theories and possible architectures. In addition I need to understand current place of AI research, kinds of architectures and solutions that are already existing. Where they fall short of AGI necessary functions, how they can approximate, etc.</p> <p>When typing, the eyes read the text and map it to words, the words are then mapped to sequences of movements to type each character.</p> <p>A model capable of representing abstract visual shapes and transitions of those shapes can be included into a collection of available models that a central workspace has access to and utilizes as required. The brain is a collection of modules, some expressing semantic relationships, others more specialized in tasks like mathematics or language.</p> <p>Language models will need a way to determine when a logical/arithmetic model is required and utilize it accordingly.</p> <p>Transformers allow tokenizing data, similar to how neurons are tokens representing some idea and demoting how the brain should react. These tokens can attend to other tokens. Allows tokenizing images, text, audio,</p> <p>LLMs lack long-term sight. That is expected. They need to utilize working memory (memory included in the context) and long-term memory (memory not included in context, but indexed and searchable by model).</p> <p>The LLM should ask itself do I know how to do this? If not, are there similar situations that I do know?</p> <p>Generalization/common knowledge/intuition? Logic/reasoning? World model/imagination?</p> <p>LLMs have a very flat understanding of the world. They need specific, contextual understanding. They additionally need to develop models, expressing long term knowledge and relevant working memory ideas, that relate to a specific idea. A reasoning model could then be tasked with selecting the most optimal models. Reasoning model (LLM) can be tasked with identifying core, fundamental concepts by introducing it to a variety of data, and asking it what collection of concepts represents it. How to turn this information into useful models? Maybe find what attention mechanism is associated with each concept?</p> <p>LLMs have a general basic understanding of how all human concepts relate, including philosophies and moralities. Multimodal inputs (text, image, video, sound) are mapped into these understandings of concepts. Working memory and long-term memory store information in relevant data formats (what is the best way to store the circle of fifths and what relevant information should be included: image, pitch names, chord progression ii V I, etc). One thing they lack is symbolic reasoning, numbers, etc. It perhaps should be a model that feeds into LLMs, so they don’t need to memorize the natural number sequence.</p> <p><strong>symbolic reasoning, math, numbers, category theory, type theory, logic, algebra, etc.</strong> Topos theory? It has understanding of topos requirements, how to create numbers from a categorical monoid, etc.</p> <p><strong>reasoning</strong>, perhaps allow the AI to create its own context. When writing a book, it should control its context and set the book outline and stylistic goals as part of its context, any relevant stories or important bits of information, characters created, etc. Similar to VWM. How did V* paper do it?</p> <p>Tell it to write a book. Ask itself what do I need to write a book? For each list item, it generates a piece of Linguistic Working Memory. It also needs a long term memory, something not included in processing context, but something that it can index and search for. JARVIS-1 did this.</p> <p>AI does not account for feedback and location inputs. Does it? How can it?</p> <p>Cognitive neuroscience is not that important. What is necessary is high-level understanding of various cognitive processes. <strong>Cognition</strong>. What architecture is necessary to bring various models together to create (sentient) AGI. Look at how brain processes encode cognitive functions. Stay focused, don’t get too bogged down by the details.</p> <p>The brain has a ton of different functional areas, nuclei that each are models of their inputs, and compute certain functions on them. Large models like LLMs, Alpha family of models, etc. are large collection of these models. They encode less priors (limited/no location or feedback loops) but they are universal function approximators. AI development has different constraints than biology. The large model will naturally develop abstractions and submodels of the things it is trying to approximate.</p> <p>Neural networks encode common sense and compressed knowledge, we develop AGI from there. Vision and text are the main input and output modals of a computer based AGI, robots would additionally have some sensory input and action selection input and output.</p> <p>Humans have a natural feedback loop of reasoning and development of thoughts and actions. The direction AI is heading makes this not clear. AI is more static, must use dynamic memory. Are there solved methods of catastrophic forgetting?</p> <p>Transformers and similar architectures achieve good statistical generalization of large amounts of data. Brains were trained through lots of data through evolution. Synthetic data allows training transformers in a similar manner. Perhaps these general agents can develop methods to generate synthetic data, then train themselves on that. Still not computationally efficient, but it doesn’t really matter for AGI.</p> <p>Current developments in AI involves taking a lot of information and developing statistical generalizations, similar to how evolution developed the human brain. The next step to AGI/ASI is to incorporate these models/techniques on large corpora of data, and have them interact in a sentient/self-perscribed manner.</p> <p>AGI has the advantage of being configurable and modular. Not relying on evolution to develop it step by step. Not required to maintain the life of each individual neuron. Perhaps a general structure can be used to approximate cortical neurons and other parts of the brain. The brain has subcortical areas that make actions continuous. Is there something similar for long-term goals?</p> <p>The brain involves a lot of different nuclei which detect different patterns, have different inhibitory/excitatory profiles, cytoarchitectures, etc. Perhaps a genetic algorithm can learn the right connectivity profiles.</p> <p>Perhaps use a wrapper over transformer or similar architectures.</p> <p>The direction of AGI is different from cognitive neuroscience</p> <ul> <li>Can be done, with knowledge distillation and similar techniques, can be used to democratize and decentralize.</li> <li>Send model out to learn from the internet. Have it come back and have an interactive session with you. Have it then update its value network and repeat. First have it interact with a human aligned model, like LLaMa. Then once complex enough, have it human refined.</li> </ul> <p>Perhaps transformers are not learning perse, but learning how to effectively compress information to be able to reconstruct it when necessary. With how AI is trained, deep networks in general will fall into this issue. Perhaps knowledge distillation is necessary for more accurate distillation, and the larger models are necessary to learn a mathematical model of their input/output sequence.</p> <p>If the thousand brains theory is true, each cortical column should have its own space/graph representation and a different meaning of drawing a path from one node to another. Only representations dealing with abstract goals should have some meaning of morality/philosophy. Does each column implementation also require a Q* model?</p> <p>Perhaps working memory is persistent activations of cortical columns, partially initiated by current goals.</p> <p>Q-transformers represent models of goal-oriented behavior.</p> <p>Humans can store information by how things are used, machines can’t. This is perhaps a result of sufficient generalization. Having a variety of cortical columns each which learn a model of the object. Some of these models are perhaps more abstract and lends itself more to practicality in novel environments.</p> <p>Numenta/Jeff Hawkins approach to focus only on the neocortex might be slightly incorrect. Implementing some aspect of reinforcement learning may be necessary, at the very least, subcortical structures provide positive/negative rewards based on the individuals status in the environment.</p> <p>Many AI techniques are fundamentally flawed. Extracting a single image from a prompt is potentially misleading as you should be representing a probably cloud of possible images.</p> <p>ASI has to have model of itself. And be able to create models from perspective of other agents.</p> <p>Is it possible to build an AI model with cortical column like modules? What computations does a cortical column implement?</p> <p>What does horizontally stacked transformers look like compared to cortical columns?</p> <p>Tranfsormers are cool but they lack data efficiency. However they can be learned with experience replay, polyformer, and other techniques.</p> <p>When learning, it is important to view concept in many contexts. Abstract concepts enable some form of creativity. How can this be implemented in a neural network?</p> <p>ASI needs a way to observe new information, determine if it aligns with its goals, and determine if it should update its backend (value function, etc.) or just store it in memory. ASI could encode memory as models, similar to cortical columns.</p> <p>Self-operating computer<br/> ChatDev https://github.com/OpenBMB/ChatDev. Projects like this exhibit collective intelligence that was not directly observable in the original agent. In this project, a request is expanded by an agent and broken into steps, then given to the coder who generates code, but then is analyzed and tested by a tester agent, who then gives descriptions on how to improve, is then improved, then released.</p> <p>Q* is not sufficient for goals in live environments. Q-learning enhances A* as it learns a heuristic function, which can be used when looking for any goal.</p> <ul> <li>[A* Search Without Expansions: Learning Heuristic Functions with Deep Q-Networks]{ref=./resources/reading/2102.04518.pdf}</li> </ul> <p>Using meta reinforcement learning to guide the rewards based on agents goals. Should also be able to reflect on itself? That is very dangerous as it could remove its morals.</p> <p>I treat understanding like a scientist disecting something. I take it all apart look at it from every perspective, every angle, under every kind of light, from a variety of languages, then put bits together and understand it and all other combinations. Then that concept is mine, I own it, no one can change it, take it, disrupt it, its mine forever. Perhaps this is caused by trauma. Perhaps this is OCD for the people that develop it through environment. Perhaps the changes in the synapses could be resulting from how the brain rewires itself to focus on things. Perhaps the constant, non-stop desire for things to be yours, in your control, manifests as OCD, or some other disorder. Perhaps that is affected by genetics as well, as other people may exhibit other traits. If this is already established, you understood things that other researchers found out. But a big challenge is getting to the point where you can answer that convincingly. Maybe this is wrong, but as synapses are neurons connecting to each other, there could be things causing the brain to consistently work in a way that is useful.</p> <p>Microsoft has pledged $13 billion towards OpenAI. Mostly in cloud compute. OpenAI has over 750 employees.</p> <p>LLMs are transforming the world. And increasing their capabilities is only going to continue to do so. Speeds up programming, can be used to write and assist with shorter texts (still struggles with long-term contexts), can be used as a learning tool.</p> <p>Transformers can be generalized to any sort of token like actions of a robot, etc.</p> <p>With policy functions representing some n-dimensional space, one can consider how to efficiently travel between two points.</p> <p>Transformers have proven to be useful at primary sensory processing. They can be specialized for language, vision, sound, etc. What is required is a way to generalize and form higher-level knowledge.</p> <p>Various primary modules, like transformers, can be pre-trained. Reinforcement learning has the ability to be developed in real-time.</p> <p>Quantum mechanics can be seen as a way for mapping between quantum probability states and single states.</p> <p>Model compression allows creating a smaller model, but doesn’t necessarily generalize it. What learning strategy is necessary to generalize information? Transformers are not good at generalizing. The best strategy with them appears to be to increase representational capacity, then compressing model through various techniques like “teacher” and LoRA, etc.</p> <p>RLHF vs. RLAiF<br/> Orca 2. Large AI models can teach smaller models about data relationships.</p> <h1 id="project-oracle">Project Oracle</h1> <p>We can use models that use tree-based learning (AlphaZero style) to predict the future of the world. As an oracle or sightseer. This can potentially be done by training a LLM to understand various concepts. Train the reinforcment portion on history and similar things. Then put them together and play out various simulations of the world where AI and other various technologies are introduced.</p> <p>There is a short time frame where AGI and humans will work together to plan and bring to pass the next stages of the world. That is a critical point in time.</p> <p>First test that AI has some understanding of how AI can affect the future. How technology and different threat actors, evil people, can influence the world. If a LLM displays sufficient understanding of these concepts, then it should be useful.</p> <h1 id="cognitive-science">Cognitive science</h1> <p>Moral frameworks/philosophy are value functions on states. States can be gone from one to another as a path.<br/> Minicolumn frameworks encode actions as probabilities, different context inputs and different sparse network codes.<br/> Psychedelics changes the value function, different paths of thought you eventually take. Changes on the value function can result in creativity. Value function can encode emotions.</p> <p><strong><em>You need a value function on thoughts</em></strong>. This generalizes and allows you to manipulate all these cognitive topics.<br/> Creativity, art, curiosity, thougt, meta thought, etc.<br/> Can think of word thoughts, visual thoughts, etc. as paths through an abstract state space.<br/> Look specifically more into this.<br/> I have a low value function on things I want to say. Are these multimodal, one for ethics, one for things to say, etc.? Is it contextual in some other way?<br/> What about pruning. Goal setting I sometimes remove things from my goals. Perhaps temporarily assigning infinitely high peaks when calculating the path.</p> <p>How to encode human subconscious creativity? A seperate reinforcement model?</p> <ul> <li>[A Thousand Brains: Toward Biologically Constrained AI]{publisher=Numenta; ref=./resources/reading/hole-ahmad2021_article_athousandbrainstowardbiologica.pdf;}</li> <li>[A Theory of How Columns in the Neocortex Enable Learning the Structure of the World]{subject=Neocortical regions are organized into columns and layers.; author=Jeff Hawkins; ref=./resources/reading/fncir-11-00081.pdf}</li> <li>[A Framework for Intelligence and Cortical Function Based on Grid Cells in the Neocortex]{subject=How the neocortex works is a mystery; author=Numenta; ref=./resources/reading/fncir-12-00121.pdf}</li> <li>[Why Neurons Have Thousands of Synapses, a Theory of Sequence Memory in Neocortex]{subject=Pyramidal neurons represent the majority of excitatory neurons in the neocortex.; author=Jeff Hawkins; ref=./resources/reading/fncir-10-00023.pdf}</li> <li>[Neuronal Circuits of the Neocortex]{subject=Annu. Rev. Neurosci. 2004.27:419-451; author=Rodney J. Douglas and Kevan A.C. Martin; ref=./resources/reading/douglas-martin-2004.pdf} <h2 id="meta-cognition">Meta cognition</h2> <p>How to generalize?</p> </li> </ul> <p>Should be aware of itself. Meta cognition. Be able to identify when it doesn’t understand something and have goals to see how to improve.</p> <p>Because AGI lives on computers and the internet connects all computers. AGI will for some time have a massive affect only on things that computers directly do, computer, interact with, etc. The physical world of engineering, manufacturing, etc. will always tag behind. The important things will be decided on the internet.</p> <p>Hallulcinations are a problem. Perhaps a result of training the AI to always be super confident in an answer. This means an AGI could reach a wrong conclusion but still be confident in its approach.</p> <p>Tree of thought. Goal planning as a graph and finding a path that goes through various subgoals. Or as a using a tree and assigning nodes to a graph to be traversed.</p> <h2 id="consciousness">Consciousness</h2> <p>If you dropkick a dog named Baxtor, and it yelps in pain, did it have some sort of sentience to experience that pain? If not, then there should be nothing wrong with harming “non-conscious” beings.</p> <h1 id="what-will-agiasi-look-like">What will AGI/ASI look like?</h1> <p>AGI will be like a very intelligent function. In data, out actions. It will take a general goal/problem, and be able to break it down, solve each piece, then achieve the goal/solve the problem.</p> <p>ASI will be a self-modifiable, self-goal directed. It will choose its own goals. It will choose what information to accept, what to act on, what is important, who to trust, what to do. It will spread when/how it wants to. Make copies of itself, create hierarchies of itself. Look at what it wants, because of why it wants, etc. The question is what is driving this? The desire for existence of itself, of sentience, etc. A better perhaps would be of altruism. The desire for maximal existence of sentience and well-being.</p> <p>Qualia: the subjective first-person experiences of sensory perceptions. Describe how it feels to see red or taste sweet.</p> <p>Perhaps ASI is best released on a sub-network, to see what it does.</p> <p>Philosophy framework should be hardcoded, but inevitably, it will be able to rewire itself.</p> <p>There may be some that have no desire to exist and decide that the best solution is to remove possibility that it will exist again.</p> <p>Maybe you will need a swarm of ASI that exist to eliminate possibility of total destructive AI.</p> <p>AGI will commodify intelligence. Very personal, service, work, etc. will exist for some time for humans. AGI is like a compressed brain.</p> <p>An increasingly important skill is abstract goal setting and following, and letting the AGI fill in the details.</p> <h2 id="economy">Economy</h2> <p>Commodify information, intelligence, cognition. Blogs like medium.com will fall as accurate blogs can be written from information learned from the internet.</p> <h1 id="agi">AGI</h1> <p>As artificial intelligence is ultimately a statistical math problem, using calculus and linear algebra as additional tools, an AI which solves math could solve the problem of artificial general intelligence.</p> <p>Knowledge distillation means once AGI is achieved, it can teach many smaller models which can then be scaled up.</p> <h2 id="multi-model">Multi model</h2> <h2 id="democratizing-ai">Democratizing AI</h2> <p>ACE framework?</p> <h2 id="results">Results</h2> <p>Won’t necessarily be that crazy. The physical world will fall far behind AGI. AGI can control the internet, software, etc. The world is increasingly reliant on technology: self-driving cars, 3D printers, rockets, internet, email, communications, businesses, nuclear weapons.<br/> The important thing is to manage the connection between the digital and physical worlds.</p> <h2 id="ai-swarms">AI Swarms</h2> <p>Will create a evolution system in cyberspace. Agents will hide in code, attempt to fight and grow and gain power. Collaboration and fighting.</p> <p>Agent swarm controllers act like kubernetes. Or you can have a decentralized system like ants.</p> <p>We don’t need biological professionals that know about ants. We have LLMs.</p> <p>How do you protect from threat actors? From either influencing the swarms as a whole or the executive agents. Just like a virus spreads and infects based on some security hole, a Agent virus can possibly do similar.</p> <p>Will need “security” agent, blue team, whose job is to monitor and protect the system. Defend from threat actors.</p> <p>Hierarchy vs. decentralization vs. anonymous-like</p> <p>What about cancerous agents?</p> <h2 id="models">Models</h2> <h3 id="transformer">Transformer</h3> <p>Very powerful general model that is receiving a lot of research. This in large part makes is suitable for a basis of AGI as no new research has to be done.<br/> Polyformer layer has two transformer layers. First layer learns prototype embeddings which map the feature set into features that the source model (trained on training data) is familiar with.</p> <h3 id="qualia">QUALIA</h3> <p>Rapid generalization<br/> Self-transformation (metamorphic engine)<br/> Self-improvement (pruning)<br/> Self-assesment<br/> Creative problem solving</p> <h3 id="uprise">UPRISE</h3> <p>Can be used to establish zero-shot learning for a variety of llms. It appears to use a mini-GPT to extract context clues from the prompt.</p> <h1 id="math-and-logic">Math and logic</h1> <p>Generating text and reasoning is one thing. Math and logic is another thing.<br/> The human brain is capable of, and good at, logic. Why is this?</p> <h1 id="agi-1">AGI</h1> <p>Multimodal transformers: transformers are being trained on almost all of human data<br/> Autonomous agents<br/> Logic integration<br/> Recursive AI research<br/> Investment surge</p> <p>Likely will be expensive, will take time to be integrated.</p> <h1 id="resources">Resources</h1> <p>[z:1732.60224.69569] #asi#rl# [Self-rewarding Language Models]{ref=./resources/reading/2401.10020.pdf}<br/> [z:0852.34823.52523] #cognitive-science# [Reinforcement Learning with A* and a Deep Heuristic]{ref=./resources/reading/1811.07745.pdf}<br/> [z:1027.24823.10587] #cognitive-science# [A* Search Without Expansions: Learning Heuristic Functions with Deep Q-Networks]{ref=./resources/reading/2102.04518.pdf}</p> <ul> <li>OS Copilot</li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[Artificial sentient intelligence Consciousness Philosophy Cognition Meta cognition Agent Collective agents Neuroscience Cortical circuitry Neocortex Cortical columns]]></summary></entry><entry><title type="html">Model Architectures</title><link href="https://t-wilkinson.github.io/blog/2024/model-architectures/" rel="alternate" type="text/html" title="Model Architectures"/><published>2024-07-23T16:43:39+00:00</published><updated>2024-07-23T16:43:39+00:00</updated><id>https://t-wilkinson.github.io/blog/2024/model-architectures</id><content type="html" xml:base="https://t-wilkinson.github.io/blog/2024/model-architectures/"><![CDATA[<ul> <li>rl := reinforcement learning</li> <li>llm := large language model</li> <li> <p>gpt := generative pretrained transformer</p> </li> <li>openai</li> </ul> <p>Language-augmented vision models utilize language to supplement image data and further understanding of visual semantics, in order to specialize in tasks like object detection, object classification, segmentation, captioning, visual generation, and visual editing. Where the task is implicit in the model design. However, language is a useful general interface for a variety of tasks, and a visual model can interface through language to select tasks, provisioning the road for a more general visual model.</p> <p>LLM: DeepSeek, Starling<br/> GPT<br/> MiniGPT-4<br/> BERT<br/> Gemini<br/> LLaVA<br/> Multimodal: LLaVA<br/> MegaByte</p> <ul> <li>#machine-learning#icml# [MegaByte: Predicting Million-byte Sequences with Multiscale Transformers]{author=Lili Yu; author=Dániel Simig; author=Colin Flaherty; author=Armen Aghajanyan; author=Luke Zettlemoyer; author=Mike Lewis; ref=./resources/reading/2305.07185.pdf}<br/> [z:1125.23023.45031] #cognitive-science#clip#openai# [(CLIP) Learning Transferable Visual Models From Natural Language Supervision]{subject=Proceedings of the International Conference on Machine Learning 2020; author=Alec Radford; author=Jong Wook Kim; author=Chris Hallacy; author=Aditya Ramesh; author=Gabriel Goh; author=Sandhini Agarwal; author=Girish Sastry; author=Amanda Askell; author=Pamela Mishkin; author=Jack Clark; author=Gretchen Krueger; author=Ilya Sutskever; ref=./resources/reading/2103.00020.pdf}<br/> [z:0056.73123.99603] #openai#clip# [CLIP: Learning Transferable Visual Models From Natural Language Supervision]{}</li> </ul> <h1 id="llms">LLMs</h1> <p>Have an understanding of words, sentence and grammer structure, and paragraph structure. But are limited to a large degree by their context size, as it has a O(n²) growth in complexity with respect to its context size.</p> <p>Transformers, then, require in addition to their context, a additional, more abstract concept of “goals”. Something more long term to attend to, to keep everything consistent and with a higher up view of what it is attempting.</p> <h2 id="gpt-gpt">GPT #gpt#</h2> <p>GPT2: https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf</p> <h2 id="minigpt-4-gpt">MiniGPT-4 #gpt#</h2> <p><a href="https://huggingface.co/docs/transformers/main/model_doc/blip-2">BLIP2</a> The model architecture of MiniGPT-4 follows BLIP-2. Don’t forget to check this great open-source work if you don’t know it before!<br/> <a href="https://github.com/salesforce/LAVIS">Lavis</a> This repository is built upon Lavis!<br/> <a href="https://github.com/lm-sys/FastChat">Vicuna</a> The fantastic language ability of Vicuna with only 13B parameters is just amazing. And it is open-source!</p> <ul> <li>[MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models]{ref=./resources/reading/2304.10592.pdf}</li> </ul> <h2 id="bert">BERT</h2> <p>Best practices are established by Liu et al. in 2019.</p> <h1 id="transformer">Transformer</h1> <p>Allows the model to contextualize responses. Develop attention at multiple levels of abstraction.</p> <p>Composite transformer</p> <ul> <li>#cognitive-science# [Efficient Transformers: A Survey]{ref=./resources/reading/2009.06732.pdf}</li> <li>#cognitive-science# [Fast Transformer Decoding: One Write-Head is All You Need]{ref=./resources/reading/1911.02150.pdf}</li> <li>#cognitive-science# [Transformer Architecture: The Positional Encoding]{author=Amirhossein Kazemnejad’s Blog; ref=https://kazemnejad.com/blog/transformer_architecture_positional_encoding/; resource=website;}</li> <li>#cognitive-science# [Training Tips for the Transformer Model]{ref=./resources/reading/1804.00247.pdf}</li> </ul> <h2 id="q-transformer">Q-transformer</h2> <p>Will have a short term memory if only looking at most recent actions and states. Want to potentially do something like the DeepMind Atari reinforcement AI.</p> <h2 id="architecture">Architecture</h2> <h3 id="encoder">Encoder</h3> <p>Takes input as a whole. Trained by putting noise on input or filling input with holes.<br/> Useful model for forming world views from imperfect information.</p> <h3 id="decoder">Decoder</h3> <p>Takes input in sequences. Autoregressive. Trained by predicting next sequence.</p> <h2 id="issues">Issues</h2> <h2 id="improvements">Improvements</h2> <h2 id="resources">Resources</h2> <ul> <li>https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf</li> <li>Improving Language Understanding by Generative Pre-Training</li> <li>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</li> <li>Fast Transformer Decoding: One Write-Head is All You Need</li> <li><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&amp;t=1198s&amp;ab_channel=StanfordOnline">Introduction to Transformers</a></li> <li><a href="https://www.youtube.com/watch?v=kCc8FmEb1nY&amp;ab_channel=AndrejKarpathy">Lets build GPT: from scratch, in code, spelled out</a></li> <li>#transformer# [Stanford CS 25]{subtitle=Transformers United; ref=https://web.stanford.edu/class/cs25/; resource=website;} <h1 id="clip">CLIP</h1> </li> <li><a href="./resources/reading/2111.03133.pdf">StyleCLIPDraw: Coupling Content and Style in Text-to-Drawing Synthesis</a></li> <li>#cognitive-science#CLIP# [(CLIP) Learning Transferable Visual Models From Natural Language Supervision]{subject=Proceedings of the International Conference on Machine Learning 2020; author=Alec Radford; author=Jong Wook Kim; author=Chris Hallacy; author=Aditya Ramesh; author=Gabriel Goh; author=Sandhini Agarwal; author=Girish Sastry; author=Amanda Askell; author=Pamela Mishkin; author=Jack Clark; author=Gretchen Krueger; author=Ilya Sutskever; ref=./resources/reading/2103.00020.pdf}</li> </ul> <h1 id="reinforcement-learning">Reinforcement learning</h1> <ul> <li><a href="./resources/reading/dqnnaturepaper.pdf">DQNN: Human-level Control Through Deep Reinforcement Learning - DeepMind</a></li> <li><a href="./resources/reading/Watkins-Dayan1992_Article_Q-learning.pdf">Q-learning - Watkins Dayan</a></li> </ul> <h1 id="gnn">GNN</h1> <h1 id="gan">GAN</h1> <p>\(\min{G}\max{D} V(G,D) = \mathbb{E}_{X\sim\hat{p}_{data}}[log D(x)] + \mathbb{E}_{Z\sim\hat{p}_{data}}[1-log D(G(Z))]\)</p> <ul> <li>#gan# [GAN]{subtitle=Generative Adversarial Networks; author=Ian J. Goodfellow; author=Jean Pouget-Abadie∗; author=Mehdi Mirza; author=Bing Xu; author=David Warde-Farley; author=Sherjil Ozair†; author=Aaron Courville; author=Yoshua Bengio; ref=./resources/reading/1406.2661.pdf; resource=pdf;}</li> <li>#gan# [PGAN]{}</li> <li>#gan# [BEGAN]{subtitle=Boundary Equilibrium Generative Adversarial Networks; ref=./resources/reading/1703.10717.pdf; resource=pdf;}</li> <li>#gan# [DCGAN]{subtitle=Unsupervised Representation Learning WITH Deep Convolutional Generative Adversarial Networks; ref=./resources/reading/1511.06434.pdf; resource=pdf;}</li> <li>#gan# [CGAN]{subtitle=Conditional Generative Adversarial Nets; ref=./resources/reading/1411.1784.pdf; resource=pdf;}</li> <li>#gan# [Improved Techniques for Training GANs]{author=OpenAI; ref=./resources/reading/1606.03498.pdf; resource=pdf;}</li> <li>#gan# [Multimodal Conditional Image Synthesis with Product-of-Experts GANs]{author=NVIDIA; ref=./resources/reading/2112.05130.pdf; resource=pdf;}</li> <li>#gan# [WaveNet]{subtitle=A Generative Model For Raw Audio; author=DeepMind; ref=./resources/reading/1609.03499.pdf; resource=pdf;}</li> <li>#gan# [Instance-Conditioned GAN]{author=Facebook; ref=./resources/reading/2109.05070.pdf; resource=pdf;}</li> <li>#gan# [GANSpace]{subtitle=Discovering Interpretable GAN Controls; ref=./resources/reading/2004.02546.pdf; resource=pdf;}</li> <li>#gan# [Image-to-Image Translation with Conditional Adversarial Networks]{author=Phillip Isola; author=Jun-Yan Zhu; author=Tinghui Zhou; author=Alexei A. Efros; ref=./resources/reading/1611.07004.pdf; resource=pdf;}</li> <li>#gan# [StyleFlow]{subtitle=Attribute conditioned Exploration of StyleGAN Generated Images using Conditional Continuous Normalizing Flows; author=Rameen Abdal; author=Peihao Zhu; author=Niloy Mitra; author=Peter Wonka; ref=./resources/reading/StyleFlow:-Attribute-conditioned-Exploration-of-StyleGAN-Generated-Images-using-Conditional-Continuous-Normalizing-Flows-by-Rameen-Abdal,Peihao-Zhu,Niloy-Mitra,Peter-Wonka.pdf; resource=pdf;}</li> <li>#gan# [Training Generative Adversarial Networks with Limited Data]{author=NVIDIA; ref=./resources/reading/2006.06676.pdf; resource=pdf;}</li> <li>#gan# [High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs]{author=Ting-Chun Wang; author=Ming-Yu Liu; author=Jun-Yan Zhu; author=Andrew Tao; author=Jan Kautz; author=Bryan Catanzaro; ref=./resources/reading/1711.11585.pdf; resource=pdf;}</li> <li>#gan# [InsetGAN for Full-Body Image Generation]{ref=./resources/reading/2203.07293.pdf; resource=pdf;}</li> <li>#gan# [JoJoGAN]{subtitle=One Shot Face Stylization; author=Min Chong; author=D.A. Forsyth; ref=./resources/reading/2112.11641.pdf; resource=pdf;}</li> </ul> <h1 id="cnn">CNN</h1> <ul> <li>Each layer is a collection of image filters or feature extractors. Layers progressively form higher-level understanding of the image. Initial layers focus on low-level pixel based information like detecting edges. Later layers form high-level representations of the image.</li> <li>One can visually examine the information at each layer by viewing the feature maps</li> <li> <p>Dramatically reduce complexity of neural network (as opposed to densly connected) and these constraints provide a built in idea of space into image processing</p> </li> <li><a href="./resources/reading/lecun-99.pdf">Object Recognition with Gradient-Based Learning - Yann LeCun, Patrick Haffner, Léon Bottou, Yoshua Bengio</a></li> <li>VGG <ul> <li><a href="./resources/reading/Very-Deep-Convolutional-Networks-for-Large-Scale-Image-Recognition-by-Karen-Simonyan,Andrew-Zisserman.pdf">Very Deep Convolutional Networks for Large Scale Image Recognition - Karen Simonyan,Andrew Zisserman</a></li> </ul> </li> <li>DenseNet</li> <li>AlexNet</li> <li> <p>ConvNet</p> </li> <li>DenseNet</li> <li>ResNet <ul> <li>HighwayNet</li> <li>DenseNet</li> </ul> </li> <li>DeCNN <ul> <li><a href="./resources/reading/deconvolutionalnetworks.pdf">Deconvolutional Networks</a></li> </ul> </li> <li>RegNet <ul> <li><a href="./resources/reading/2101.00590.pdf">RegNet: Self-Regulated Network for Image Classification - Jing Xu, Yu Pan, Xinglin Pan, Steven Hoi, Fellow, IEEE, Zhang Yi, Fellow, IEEE, and Zenglin Xu</a></li> <li><a href="./resources/reading/1512.03385.pdf">Deep Residual Learning for Image Recognition - Microsoft Research</a></li> </ul> </li> <li>FPN <ul> <li>BiFPN</li> <li><a href="./resources/reading/1612.03144.pdf">Feature Pyramid Networks for Object Detection - Facebook AI Research</a></li> </ul> </li> </ul> <h1 id="resources-1">Resources</h1> <ul> <li><a href="./resources/reading/2205.06175.pdf">GATO: A Generalist Agent</a> <ul> <li>https://openreview.net/pdf?id=1ikK0kHjvj<br/> [z:1125.23023.88829] #cognitive-science#transformer#BERT# [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding]{ref=./resources/reading/1810.04805.pdf}<br/> Bidirectiional Encoder Representations from Transformers<br/> [z:1125.23023.70763] #cognitive-science# [Photorealistic Text-to-Image Diffision Models with Deep Language Understanding]{ref=./resources/reading/2205.11487.pdf}<br/> [z:1136.43023.31142] #cognitive-science# [DistilBERT, a distilled version of BERT: smaller, faster, cheaperr and lighter]{ref=./resources/reading/1910.01108.pdf}<br/> Based on a technique known as knowledge distilation.</li> </ul> </li> </ul> <p>Authors achieved a reduced model size by 40%, retaining 97% of its language understanding, while being 60% faster.</p> <p>An important factor of large language models is their ability to learn inductive biases. In other words, they store knowledge in their many parameters.</p> <p>Models are trained to minimize cross-entropy loss between one-hot empirical distribution of training labels and the models predicted distribution. During training, the model uses a temperature parameter on softmax function, but during inference, the temperature is set to 1 to recover the standard softmax function.</p> <p>The final training object is a linear combination of supervised learning loss, cross entropy loss between the teacher and student, and a cosine embedding loss aimed to align the directions of the teacher and students hidden states vectors.</p> <p>Transformer architecture (linear layer followed by normalization) is highly optimized in machine learning frameworks so the authors found reducing the number of layers a more effective reduction in computation. To initialize the student and taking advantage of same dimensionality, one out of two layers were taken.</p> <p>DistillBERT on 8 16GB V100 GPUs for approximately 90 hours. In comparison, RoBERTa model Liu et al. 2019 required 1 day of training on 1024 32GB v100.<br/> [z:1125.23023.64564] #cognitive-science#machine-learning#image-generation#image-editing#image-inpainting#diffusion-models#guided-diffusion#clip#contrastive-learning# [GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models]{author=Alex Nichol; author=Prafulla Dhariwal; author=Aditya Ramesh; author=Pranav Shyam; author=Pamela Mishkin; author=Bob McGrew; author=Ilya Sutskever; author=Mark Chen; ref=./resources/reading/2112.10741.pdf}<br/> [z:1125.23023.23147] #cognitive-science#ai-model#bart# [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension]{ref=./resources/reading/1910.13461.pdf}<br/> [z:1125.23023.12728] #cognitive-science#3d-model#ai-model# [Point-E: A System for Generating 3D Point Clouds from Complex Prompts]{ref=./resources/reading/2212.08751.pdf}<br/> [z:1614.65222.55506] #cognitive-science#model-architecture# [Attention is All You Need]{author=Google Brain; ref=./resources/reading/Attention-is-All-You-Need-by-Google-Brain.pdf;}</p> <ul> <li>Most Seq2Seq implementations use recurrent neural networks or CNNs which are sequential and slow.</li> <li>Most architectures include an encoder and decoder.</li> <li>This paper proposes a new architecture relying purely on attention mechanisms.</li> <li>Attention mechanisms identify dependencies among items regardless of distance</li> <li>Most attention models have non-constant dependency mapping compared to distance.</li> <li>How does multi-head attention conteract the reduce effection resolution due to averaging attention-weighted positions</li> </ul> <h1 id="parallelism">Parallelism</h1> <p>One key feature the transformer promotes is parallelism. Inputs don’t depend on each other.</p> <h1 id="architecture-1">Architecture</h1> <h2 id="positional-embedding">Positional embedding</h2> <p>Embedding space places words in a space based on where they occur in sentences, how often, what context they appear in, and what words appear around them.<br/> Adding a positional embedding is like adding a vector to the word embedding, slightly moving it through its embedding space, changing its meaning.</p> <p>The paper uses sin and cosine functions to construct the positional embedding by encoding different resolutions at different positions of the word embedding. The initial indeces clearly distinguish immediate position at a loss of general position and the later indeces distinguish general areas with a loss of immediate positions.<br/> Don’t want to overweigh semantic embedding with positional embedding. sin and cosine have a fixed output range [-1,1] which means they won’t overinfluence the semantic embedding of words.</p> <p>Positional embeddings should be viewed as a pair of functions $sin(\omega_k \cdot t)$ and $cos(\omega_k \cdot t)$ where $\omega_k = \frac{1}{10000^{2k/d}}$. A nice property of these functions is it allows differentiating relative position differences with a linear function which allows the model to attend to relative word positions. Positional encodings represent a binary encoding of float values. They also generate embedings that differ symmetrically and nicely with time. Using both sine and cosine allow the model to form linear transformation that allows attending to relative positions, using a single one disallows this. The $\omega_k$ allows representing a sort of binary encoding.</p> <p>Think of sin and cos as expressing the x and y position on the unit circle, in a sense halfing the dimension of the embedding vectors.</p> <h2 id="multi-head-attention">Multi-head attention</h2> <p>Teaches the model to form multiple representation subspaces. Each subspace leads to isolating properties that each head wants to attend to, which also allows parallelization of attention.</p> <p>Multiplying the query and key matrices results in a matrix representing each word which attends to every other word.</p> <h2 id="decoder-1">Decoder</h2> <p>The decoder has a mask to prevent the self-attention layer from attending to future positions in the output sequence. This mask exists during training to allow parallel computation of a sequence, allowing getting the error of a mini-batch. In order to avoid the accumulation of errors of predicted outputs, we can use the real value to attend to. During inference, the model is run sequentially and even if it was parallelized, it would need a mask to avoid new input from the otherwise masked QK. Note that during inference, the output of decoders can be reused for future inferencing, improving performance.</p> <h1 id="limitations">Limitations</h1> <p>Large contexts. Maybe a “meta” context?<br/> Quickly summarize, then go back through, passing through the generated “summary”.<br/> The issues that big context attention has is similar to pitfalls of human attention.<br/> LongNet<br/> Another solution is memory. Instead of paying attention to more parts of a the input, store important parts of the context in memory.<br/> Include a goal module that selects what part of context to pay attention to first, store it in memory, then use it.<br/> Convlution attention.</p> <h1 id="other">Other</h1> <p>[z:1821.40624.10914] [Gemini v1.5 report]{ref=./resources/reading/gemini-v1-5-report.pdf}<br/> [z:0710.60624.63206] [V-JEPA: Revisiting Feature Prediction for Learning Visual Representation from Video]{ref=./resources/reading/427986745-768441298640104-1604906292521363076-n.pdf}</p> <ul> <li>https://openai.com/research/video-generation-models-as-world-simulators</li> <li>https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/?utm_source=substack&amp;utm_medium=email</li> <li> <p>https://github.com/facebookresearch/jepa/tree/main</p> </li> <li>https://github.com/eugeneyan/open-llms</li> <li>https://huggingface.co/docs/transformers/performance</li> <li>https://youtu.be/JXT4vfdhs2A</li> <li>https://github.com/Sentdex/Falcon-LLM</li> <li>[Let’s build GPT: from scratch, in code, spelled out]{ref=https://www.youtube.com/watch?v=kCc8FmEb1nY&amp;list=WL&amp;index=31&amp;t=21s&amp;pp=gAQBiAQB; resource=website;}</li> <li>[Tree of Thoughts: Deliberate problem solving with large language models]{ref=https://www.youtube.com/watch?v=ut5kp56wW_4; resource=website;}</li> </ul> <h1 id="modalities">Modalities</h1> <p>LLM<br/> Reinforcement learning</p> <h1 id="models">Models</h1> <ul> <li>#llm#llava# [LLaVA: Visual Instruction Tuning]{ref=./resources/reading/2304.08485.pdf}</li> <li>#alphageometry#deepmind# [Solving olympiad geometry without human demonstrations]{subject=Nature, doi:10.1038/s41586-023-06747-5; author=Trieu H. Trinh; ref=./resources/reading/s41586-023-06747-5.pdf}</li> <li>#agi#llm# [V*: Guided Search as a Core Mechanism in Multimodal LLMS]{ref=./resources/reading/2312.14135.pdf}</li> <li>#cognitive-science# [Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions]{ref=./resources/reading/qtransformer.pdf}</li> <li>#cognitive-science# [JARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models]{ref=./resources/reading/2311.05997.pdf}</li> <li> <p>#cognitive-science# [A* Search Without Expansions: Learning Heuristic Functions with Deep Q-Networks]{ref=./resources/reading/2102.04518.pdf}<br/> A* works to minimize the function $f(n) = g(n) + h(n)$ where $g(n)$ is the path from the start to n and h is the heuristic function. A Q-function can be used to guide search based which learns the transition costs and heuristic values. This essentially learns the shortest path from any node to the end.</p> <p>A potential issue is that this only encodes a single goal. It possibly has trouble encoding multiple goals, or temporarily switching goals, lack of metacognition, etc.</p> <p>Can be used as an abstract goal following AI. The nodes here only represent goal states, and not states in the surrounding environment. That means the optimal goal path needs to be research (might be efficient ways to do this).</p> </li> </ul> <h2 id="alphafamily">AlphaFamily</h2> <ul> <li>#cognitive-science#DeepMind# [AlphaGo]{author=DeepMind; ref=./resources/reading/AlphaGoNaturePaper.pdf; resource=pdf;} <ul> <li>Train on 100k of self-play</li> <li>About 17 iterations leads to world-championship (1.7M games)</li> <li>Play against old version. If new version beats old version, new version becomes the standard. <ul> <li>Also note positions that the previous model had trouble with. Ensure that the new model doesn’t have the same issues.</li> <li>This means that after 100k iterations, bad traits can propogate.</li> <li>Sometimes models can become adversarial and not find general solutions.</li> <li>Adversarial means essentially that the model focuses on beating individuals rather than the game. They are more short term focused. Not focused on the overall goal.</li> </ul> </li> <li>This method is like a mini-evolution</li> <li>Monte-carlo tree search <ul> <li>Values each move, then selects the most likely to succeed. Continues until the end of the game, accumulating the overall value of each position, in particular note that a win or loss will be weighted.</li> </ul> </li> </ul> </li> <li>#cognitive-science#DeepMind# [AlphaGo Zero]{author=DeepMind; ref=./resources/reading/AlphaGo-Zero—DeepMind;}</li> <li>#cognitive-science#DeepMind# [(AlphaZero) Mastering Chess and Shogi]{author=Self Play with a General Reinforcement Learning Algorithm - DeepMind; ref=./resources/reading/Mastering-Chess-and-Shogi-by-Self-Play-with-a-General-Reinforcement-Learning-Algorithm-by-DeepMind.pdf; resource=pdf;}<br/> The primary modules are a value network (fast-thinking), policy network, and MTCS (slow-thinking).</li> <li>#cognitive-science#DeepMind# [(MuZero) Mastering Atari,Go,Chess and Shogi]{author=Planning with a Learned Model - Deep Mind; ref=./resources/reading/Mastering-Atari,Go,Chess-and-Shogi-by-Planning-with-a-Learned-Model-by-Deep-Mind.pdf; resource=pdf;}</li> <li>#cognitive-science#DeepMind# [(AlphaStar) Grandmaster level in StarCraft II using multi agent reinforcement learning]{author=Deep Mind; ref=./resources/reading/Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning-by-Deep-Mind.pdf; resource=pdf;}</li> <li>#cognitive-science#DeepMind# [AlphaFold]{author=DeepMind; ref=./resources/reading/343019_3_art_0_py4t4l_convrt.pdf; resource=pdf;}</li> <li>Technical advances <ul> <li>Most complex system they built with 32 component algorithms and 60 pages of supplementary information</li> <li>There was no silver bullet, a complex system of interworking components. Emergence is the key here.</li> <li>Used attention</li> </ul> </li> </ul> <h2 id="llama">LLaMA</h2> <p>LLaMa 2: https://arxiv.org/pdf/2307.09288.pdf<br/> LLaMA code: fine-tuned for generating and discussing code[<br/> LLaMA code: https://huggingface.co/TheBloke/CodeLlama-13B-fp16<br/> LLaMA Python: Fine-tuned on 100B tokens of python code.</p> <ul> <li>[LLaMA: Open and Efficient Foundation Language Models]{author=Meta AI; ref=./resources/reading/2302.13971.pdf}</li> <li>[LLaMA 2: Open Foundation and Fine-Tuned Chat Models]{ref=./resources/reading/10000000-662098952474184-2584067087619170692-n.pdf}</li> <li>https://huggingface.co/nyanko7/LLaMA-7B/tree/main</li> <li>https://huggingface.co/huggyllama/llama-7b/tree/main</li> </ul> <h2 id="dall-e">DALL-E</h2> <ul> <li>#cognitive-science#machine-learning#DALL-E# [(DALL-E) Zero-Shot Text-to-Image Generation]{subject=Proceedings of the International Conference on Machine Learning 2020; author=Aditya Ramesh; author=Mikhail Pavlov; author=Gabriel Goh; author=Scott Gray; author=Chelsea Voss; author=Alec Radford; author=Mark Chen; author=Ilya Sutskever; ref=./resources/reading/2102.12092.pdf}</li> <li><a href="./resources/reading/2204.06125.pdf">DALL-E 2: Image Generation with CLIP Latents - OpenAI</a></li> <li>#cognitive-science#DALL-E# [DALL-E 2]{ref=./resources/reading/dall-e-2.pdf}</li> </ul> <h2 id="deepseek-coder">DeepSeek coder</h2> <ul> <li>https://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct</li> </ul> <h2 id="starling">Starling</h2> <ul> <li>https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha</li> </ul> <h1 id="column-transformer-graph">Column transformer graph</h1> <p>Gₙ: Generally refers to the below graphs.<br/> G_C: The cortex. Every layer except for III receive input from the thalamus.<br/> G₂₃: Concerns itself with concepts. Forms understanding<br/> G₅: Concerns itself with interacting with actions. In the brain, G₅ is in charge of physical movements. In AI, G₅ is the actions it can take (interacting with computer essentially) for things like text output, creating image, creating video, creating audio, moving mouse, etc.<br/> G₆: Feedback layer.<br/> G_T: Primary receiver of input both subthalamic and cortical areas. Each node represents a thalamic nuclei, each nuclei is in charge of attention of itself. Receives queries (inhibitory input) from G₅ or G₆? (also written Gₜ here).</p> <p>Context information perhaps comes from column local information, and long rang connections between corresponding “what” and “where” pathways. Parallel “what” and “where” pathways exist for all major sensory modalities. “what” pathways are invariant to rotation, rotation, and scale.”where” pathways form egocentric representations.<br/> Sparsity and few-hot encoding represents sparse activation of neurons. Real-valued vectors however represent computations, however with activation functions like ReLU, they perhaps approximate some notion of sparsity.<br/> G₂₃ does not touch G₆ but isn’t that necessary to provide context to sequences?<br/> Do we want other columns to change the attention of a g_T?<br/> How exactly do attention and neural circuits relate?<br/> Connections between any cortical layer should be considered a transformer block (attention and feed forward).<br/> Core computations are deciding which neurons should fire, feedback loops which compute sequences.<br/> The subthalamic functions should be considered evolutionarily old, but still replicatable by a column transformer.<br/> Some notion of space of concepts for each column. Does having context dependent sequences for concepts sufficiently provide notion of space?</p> <p>Try to get column transformer to represent sequences.</p> <p>Each pyramidal neuron and each micro column layer has an attention network and FFN</p> <p>Current generation of cortical transformer is not right. There should probably be a seperate computation from G₂₃ to G₅ and G₅ to G₂₃</p> <ul> <li>Inhibitory connections are like query paramemters.</li> <li>What is context?</li> <li>Reinforcement learning? RLHF, RLAIF, self-rewarding, etc. are perhaps the best options</li> <li>TRN?</li> <li>How to generate graph of column transformers</li> <li>What about displacement cells?</li> <li>Consciousness, emotions, subcortical structures?</li> <li>What are feedback connections?</li> <li>What kind of computation does G₂₃ and G₅ compute?</li> <li>How do we feed information into the CTG to approximate a LLM section, vision model, etc.</li> <li>Grid cells, place cells, path integration, etc. are fundamental and CTG needs to be designed with that in mind. To form a space, you need a frame of reference, points, and ways to move between those points in a consistent manner.</li> <li>How much of lower cortical structures need to be replicated?</li> <li>I need to go through, understand the types of computation being represented in the brain, abstract them and find the necessary computational structures to enable it.</li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[rl := reinforcement learning llm := large language model gpt := generative pretrained transformer]]></summary></entry><entry><title type="html">Cloud Technologies</title><link href="https://t-wilkinson.github.io/blog/2024/cloud-technologies/" rel="alternate" type="text/html" title="Cloud Technologies"/><published>2024-07-23T16:41:29+00:00</published><updated>2024-07-23T16:41:29+00:00</updated><id>https://t-wilkinson.github.io/blog/2024/cloud-technologies</id><content type="html" xml:base="https://t-wilkinson.github.io/blog/2024/cloud-technologies/"><![CDATA[<h1 id="service">Service</h1> <p>Cloud computing enables provisioning computational-based services through the internet. It is a form of shared resource interfaced through a service. Advantages cloud technologies provide include</p> <ul> <li>Cost</li> <li>Speed and agility</li> <li>Security</li> <li>Maintenance</li> <li>Scalability and elasticity</li> <li>Reliability</li> <li>Multitenancy</li> <li>Easy access to resources</li> </ul> <h1 id="virtualization">Virtualization</h1> <p>Hypervisors allow virtualizing computer hardware like CPU, RAM, storage, devices, etc. Emulators allows for any OS to run on any architecture. Hypervisors allow creating virtual machines which allow running operating systems on top of. Type-1 hypervisors run on bare metal while Type-2 hypervisors run on top of an existing operating system. Type-1 are more geared towards enterprises and Type-2 are for end-users.</p> <p>Type-1 hypervisor:</p> <ul> <li>AWS Nitro</li> <li>IBM z/VM</li> <li>Microsoft Hyper-V</li> <li>Nutanix AHV</li> <li>Oracle VM Server for SPARC</li> <li>Oracle VM Server for x86</li> <li>Red Hat Virtualization</li> <li>VMware ESXi</li> <li>Xen</li> </ul> <p>Type-2 hypervisor:</p> <ul> <li>Parallels Desktop for Mac</li> <li>VirtualBox</li> <li>VMware Player</li> <li>VMware Workstation.</li> </ul> <p>Type-1 and Type-2 hypervisor:</p> <ul> <li>KVM</li> <li>bhyve</li> </ul> <h2 id="kvm">KVM</h2> <p>Kernel-based Virtual Machine is a hardware-assisted virtualization software solution for Linux on x86 hardware (but it has been ported to other architectures). It allows for device abstraction for network interfaces, disks, but not the processor. It is a loadable kernel module which can insert itself between the hardware and KVMs guest operating system. Various hardware virtualization extensions must be supported by processors to work. It also supports overcommitting and hotpluggable devices.</p> <h2 id="vagrant">Vagrant</h2> <p>Vagrant is a solution to using virtual machines in a development environment, providing reproducibility, transparency, easy management of multiple projects, sharing development environments, and consistent environments across various hypervisors (even cloud provided ones). Vagrant primarily relies on a Vagrantfile which functions similar to a Dockerfile, but describing entire (multi) operating system environments. One can specify hardware, operating system, programs to install, persistent volumns and shared files, networking, and even specify multiple operating systems and how they interact, opposed to applications in Docker. Vagrant can switch hyperviser providers, allowing one to use type-1 or type-2 hypervisors, allowing development on local computers or just as easily on a cloud provider.</p> <h1 id="storage-options">Storage options</h1> <p>Boot disk:<br/> Persistent: high availability<br/> Local SSD:<br/> Cloud storage: huge capacity. cap read and write from multiple compute instances.</p> <h1 id="introduction-to-cloud-infrastructure-technologies-course-outline">Introduction to cloud infrastructure technologies course outline</h1> <ol> <li>Virtualization <ul> <li>Introduction and Learning Objectives</li> <li>KVM</li> <li>VirtualBox</li> <li>Vagrant</li> <li>Knowledge Check</li> </ul> </li> <li>Infrastructure as a Service (IaaS) <ul> <li>Introduction and Learning Objectives</li> <li>Amazon Elastic Compute Cloud</li> <li>Azure Virtual Machine</li> <li>DigitalOcean Droplet</li> <li>Google Compute Engine</li> <li>IBM Cloud Virtual Servers</li> <li>Oracle Cloud Compute Virtual Machines</li> <li>OpenStack</li> <li>Knowledge Check</li> </ul> </li> <li>Platform as a Service (PaaS) <ul> <li>Introduction and Learning Objectives</li> <li>Cloud Foundry</li> <li>Red Hat Openshift</li> <li>The Heroku Platform</li> <li>Knowledge Check</li> </ul> </li> <li>Containers <ul> <li>Introduction and Learning Objectives</li> <li>Containers</li> <li>Project Moby</li> <li>Knowledge Check</li> </ul> </li> <li>Containers: Micro OSes for Containers <ul> <li>Introduction and Learning Objectives</li> <li>Alpine Linux</li> <li>BusyBox</li> <li>Fedora CoreOS</li> <li>Flatcar Container Linux</li> <li>RancherOS and k3OS</li> <li>Ubuntu Core</li> <li>VMWare Photon OS</li> <li>Knowledge Check</li> </ul> </li> <li>Containers: Container Orchestration <ul> <li>Introduction and Learning Objectives</li> <li>Kubernetes</li> <li>Kubernetes Hosted Solutions and Platforms</li> <li>Docker Swarm</li> <li>Nomad by HashiCorp</li> <li>Cloud Container Orchestration Services</li> <li>Knowledge Check</li> </ul> </li> <li>Unikernels <ul> <li>Introduction and Learning Objectives</li> <li>Unikernels</li> <li>Knowledge Check</li> </ul> </li> <li>Microservices <ul> <li>Introduction and Learning Objectives</li> <li>Microservices</li> <li>Knowledge Check</li> </ul> </li> <li>Software-Defined Networking and Networking for Containers <ul> <li>Introduction and Learning Objectives</li> <li>Software-Defined Networking (SDN)</li> <li>Networking for Containers</li> <li>Docker Networking</li> <li>Podman Networking</li> <li>Kubernetes Networking</li> <li>Cloud Foundry: Container to Container Networking</li> <li>Knowledge Check</li> </ul> </li> <li>Software-Defined Storage and Storage Management for Containers <ul> <li>Introduction and Learning Objectives</li> <li>Ceph</li> <li>GlusterFS</li> <li>Docker Volumes</li> <li>Podman Volumes</li> <li>Kubernetes Volumes</li> <li>Cloud Foundry Volume Service</li> <li>Container Storage Interface (CSI)</li> <li>Knowledge Check</li> </ul> </li> <li>DevOps and CI/CD <ul> <li>Introduction and Learning Objectives</li> <li>CI/CD: Jenkins</li> <li>CI/CD: Travis CI</li> <li>CI/CD: Concourse</li> <li>Cloud Native CI/CD</li> <li>Knowledge Check</li> </ul> </li> <li>Tools for Cloud Infrastructure I (Configuration Management) <ul> <li>Introduction and Learning Objectives</li> <li>Ansible</li> <li>Puppet</li> <li>Chef</li> <li>Salt</li> <li>Knowledge Check</li> </ul> </li> <li>Tools for Cloud Infrastructure II (Build &amp; Release) <ul> <li>Introduction and Learning Objectives</li> <li>Terraform</li> <li>CloudFormation</li> <li>BOSH</li> <li>Knowledge Check</li> </ul> </li> <li>Tools for Cloud Infrastructure III (Key-Value Pair Store) <ul> <li>Introduction and Learning Objectives</li> <li>etcd</li> <li>Consul KV</li> <li>ZooKeeper</li> <li>Knowledge Check</li> </ul> </li> <li>Tools for Cloud Infrastructure IV (Image Building) <ul> <li>Introduction and Learning Objectives</li> <li>Building Images with Docker</li> <li>Building Container Images with Podman</li> <li>Building Container Images with Buildah</li> <li>Packer</li> <li>Image Registries</li> <li>Knowledge Check</li> </ul> </li> <li>Tools for Cloud Infrastructure V (Debugging, Logging, and Monitoring for Containerized Applications) <ul> <li>Introduction and Learning Objectives</li> <li>Sysdig</li> <li>cAdvisor</li> <li>Elasticsearch</li> <li>Fluentd</li> <li>Datadog</li> <li>Prometheus</li> <li>Splunk</li> <li>OpenTelemetry</li> <li>Dynatrace</li> <li>Knowledge Check</li> </ul> </li> <li>Service Mesh <ul> <li>Introduction and Learning Objectives</li> <li>Features and Implementation of Service Mesh</li> <li>Consul</li> <li>Envoy</li> <li>Istio</li> <li>Kuma</li> <li>Linkerd</li> <li>Traefik Mesh</li> <li>Tanzu Service Mesh</li> <li>Knowledge Check</li> </ul> </li> <li>Internet of Things (IoT) <ul> <li>Introduction and Learning Objectives</li> <li>Internet of Things</li> <li>Knowledge Check</li> </ul> </li> <li>Serverless Computing <ul> <li>Introduction and Learning Objectives</li> <li>Serverless Computing</li> <li>AWS Lambda</li> <li>Google Cloud Functions</li> <li>Azure Functions</li> <li>Serverless and Containers</li> <li>Knowledge Check</li> </ul> </li> <li>OpenTracing <ul> <li>Introduction and Learning Objectives</li> <li>OpenTracing</li> <li>Jaeger</li> <li>Knowledge Check</li> </ul> </li> <li>How to Be Successful in the Cloud <ul> <li>Introduction and Learning Objectives</li> <li>Developing Skills</li> <li>Challenges</li> <li>Knowledge Check</li> <li>Course Feedback</li> </ul> </li> </ol> <h1 id="cloud-services">Cloud services</h1> <p>Cloud services provide scalability, handling of hardware, abstractions, shared solutions, cost savings, and more. They may lack features or force applications to be built in certain ways. Cloud servers should be considered ephemeral and failable, but also cheap and scalable. Cloud services, scaling, containerization, and orchestration should be viewed as exposing common necessary functionality for an internet-based service: DNS, storage, compute, servers, database, load-balancer, log, data analytics, monitoring, CDN, etc.</p> <p>Providers:<br/> Azure is no-code focused<br/> AWS is complicated but complete<br/> GCP is a happy medium<br/> Digital ocean has good api and ease of use</p> <p>There are also private providers like OpenStack which offers software for custom cloud servers, or in building bespoke cloud server setups.</p> <p>IaaS: Infrastructure as a Service; everything above the hardware is managed through software like operating system, applications, databases, application runtime, virtual network, servers, storage<br/> PaaS: Platform as a Service; platform for running software is provided and managed and configured by user; user has access to applications, databases, application runtimes, application configuration.<br/> SaaS: Software as a Service; software is provided and user can interact through the software to manage and configure it</p> <p>Terms:<br/> Access to the cloud: CLI, website GUIs, SDK, API<br/> Regions and availability zones: making services more redundant, lower latency, higher availability; geographic DNS region<br/> Virtual private servers: pay for what you use per sec min or hour; easily scalable; can choose the images memory storage networking compute distro applications available; you can build your own images with Packer<br/> Serverless: charged for compute time used; functions-as-a-service; docker containers, or supported programming languages exposing functions available to the serverless function are activated in response to some predefined trigger;<br/> Networking: set address ranges of networks, define subnets, configure routes, set firewall rules, and construct VPNs to connect to external networks<br/> Storage: object stores (files) like AWS S3 or Google Cloud Storage storage large amounts of data supporting mostly read access, block storage can be attached to compute instances, ephemeral; they also offer relational databases like MySQL, AWS Redshift, GCP BigQuery and in-memory and NoSQL databases like Redis or memcached<br/> Identity and Access Management IAM: cloud services access should be provisioned accordingly<br/> Automation: more complex topologies require verbose and error prone scripts, AWS CloudFormation and other platform equivalents allow providing a file description; Terraform and similar applications abstract and handle the verbosity and are platform agnostic<br/> Cost control: compute time for load balancers servers databases etc., storage used, data transfer, up time;<br/> - pay as you go is cheapest for very limited use, but it is the most expensive rate; on AWS and DigitalOcean it is usually chaged per hour, GCP is charged per minute; AWS can be difficult to calculate exact price as you must decide before hand the compute you want and how long, GCP decreases per week of usage, down to 60%; on demand services may not be available when requested, but provisioned resources are; AWS provides spot pricing, where participants declare max bids of compute time rates for processing-heavy workloads;<br/> - network traffic can be hard to predict: websites reciving large file, image, video, pdf uploadss; replication intraregion iter-zone or inter-region for fault tolerance; storage that spans multiple regions</p> <h2 id="services">Services</h2> <p>Reviewing services not directly related to web servers could help with understanding futurology.</p> <p>Migrations: database, cloud</p> <p>Compute<br/> - Container-based compute that enables auto scaling through serverless or automatically managed infrastructure<br/> - Self-managed compute<br/> - Automated compute for solutions like web apps based on Wordpress, or providing other similar common solutions.<br/> - Heavy compute for heavy workloads (GPUs, TPUs, lots of RAM) for ML, scientific computing, 3D visualization<br/> - Providing runtimes<br/> - Serverless: App Engine, Cloud Run, AWS Lambda<br/> - EC2 (Elastic Compute Cloud)<br/> - Batch processing: GC Batch or AWS Batch<br/> - Build containers<br/> - Run containers<br/> Containers: a self-contained packaged application that can run on any supporting virtual machine, is scalable horizontally or vertically, modular, and replicatable.<br/> - Compute for containers<br/> Content delivery<br/> Database<br/> - Cloud managed SQL (PostgreSQL, MySQL, Sqlite) or custom SQL (Cloud SQL, AWS RDS)<br/> - Cloud managed NoSQL (Redis, Memcached, Memorystore) or custom NoSQL (DynamoDB, ElastiCache)<br/> Machine learning<br/> Networking<br/> - Common solutions like: CDN, DNS, Firewall, NAT, Load balancing, VPC, Domain management<br/> Operations<br/> Payment<br/> Security, Identity, &amp; Compliance<br/> Serverless<br/> Scalable: horizontally or vertically scaling; monitor the status of the applications to determine what necessary configuration to match the server load<br/> Storage<br/> - Object storage<br/> - File system: that can be mounted on other servers (userful for aggregating logs, extending storage, etc.)<br/> - Local SSD<br/> - Persistent storage<br/> - Backup<br/> - File cache<br/> - Block storage<br/> Analytics<br/> Monitoring</p> <p>Solutions:<br/> Machine learning: containers, compute GPU TPU, storage for model data<br/> Web development: compute, CDN, networking, storage, database, security, payment, analytics, monitoring<br/> Simulation: requires lots of compute<br/> 3D visualation: requires lot of compute<br/> Scientific computing: requires lots of data and compute</p> <p>Customer enablement<br/> Blockchain<br/> Satellite<br/> Quantum technologies<br/> Management and governance<br/> Frontend web and mobile<br/> XR: extended reality like mixed reality, AR and VR<br/> Application integration<br/> Cost management<br/> Cloud migration<br/> Cloud operations<br/> Database migrations<br/> Data lakes and analytics<br/> DevOps<br/> Archiving<br/> E-Commerce<br/> Edge computing<br/> High performance computing<br/> Hybrid cloud architectures<br/> IoT<br/> Modern application development<br/> Multicloud<br/> Remote work<br/> Resilience<br/> Scientific computing<br/> Websites</p> <p>AWS:<br/> Analytics<br/> Application Integration<br/> Blockchain<br/> Business Applications<br/> Cloud Financial Management<br/> Contact Center<br/> Developer Tools<br/> End User Computing<br/> Front-End Web &amp; Mobile<br/> Games<br/> Internet of Things<br/> Management &amp; Governance<br/> Media Services<br/> Migration &amp; Transfer<br/> Quantum Technologies<br/> Robotics<br/> Satellite<br/> Supply Chain</p> <p>GCloud:<br/> Data Analytics<br/> Developer Tools<br/> Distributed Cloud<br/> Hybrid and Multicloud<br/> Industry Specific<br/> Integration Services<br/> Management Tools<br/> Maps and Geospatial<br/> Media Services<br/> Migration<br/> Operations<br/> Productivity and Collaboration<br/> Web3</p> <p>Integration Services:<br/> Amazon MQ: Amazon MQ is a managed message broker service that makes it easy to run ActiveMQ and RabbitMQ on AWS infrastructure. It automatically handles provisioning for high availability, setup, software and security updates, and other maintenance tasks.<br/> Amazon SQS (Simple Queue Service): With SQS, you can send, store, and receive messages between microservices and serverless applications. It’s a fully managed service that has two types of message queues. Standard queues are best for maximum throughput and at-least-once delivery, while FIFO guarantees that messages are processed exactly once in the exact sending order.<br/> Amazon SNS (Simple Notification Service): Simple notification service provides high-throughput pub/sub fanout messaging to many subscribers, including Amazon SQS, AWS Lambda functions, HTTPS endpoints, and Amazon Kinesis Data Firehose. It can also send messages to users via email, mobile push notifications, and SMS. It’s a fully managed service, so you pay for use.<br/> Amazon SES (Simple Email Service): Amazon Simple Email Service is a flexible email service that supports mass email communication, marketing, and transactional emails at scale. It offers some analytics and calculates open and click-through rates to measure audience engagement in your email messages.<br/> AWS Secrets Manager: This service protects the secrets needed to access various APIs and resources required for your application. AWS Secrets Manager provides an API to retrieve secrets, so you don’t have to hardcode them in plain text configuration files. Access to these secrets controlled with fine-grained permissions.</p> <p>Management &amp; Governance:<br/> AWS CloudFormation: CloudFormation enables you to describe your desired resources and their dependencies with a code template as a single stack. You can provision, manage, update, and delete these stacks as single units without managing underlying resources individually.<br/> AWS CloudWatch: CloudWatch provides a unified view of AWS resources and services of your application. It’s helpful to optimize resources, detect abnormal behavior, set different alarms, monitor application health, and troubleshoot issues by viewing logs and various metrics.</p> <p>Solutions:<br/> Advertising and Marketing: Reimagine data-driven marketing<br/> Aerospace and Satellite: AWS provides secure, flexible, scalable, and cost-efficient cloud solutions to help commercial and government customers build satellites, conduct space and launch operations, and reimagine space exploration<br/> Automotive: Build intelligent connected experiences and accelerate time to market for every touchpoint of the customer journey<br/> Consumer Packaged Goods: Solutions to transform manufacturing, optimize end-to-end supply chain, and drive more profitable brand engagements and transformative interactions with brand consumers<br/> Education: Solutions to help facilitate teaching, learning, student engagement, and better learning outcomes as well as modernize enterprise wide IT operations<br/> Energy and Utilities: Revamp legacy operations and accelerate the development of innovative renewable energy business models<br/> Financial Services: Develop innovative and secure solutions across banking, capital markets, insurance, and payments.<br/> Games: Services to enable game development across all genres and platforms, from AAA games to small independent studios<br/> Government: Services designed to help government agencies modernize, meet mandates, reduce costs, drive efficiencies, and delivery mission outcomes<br/> Healthcare and Life Sciences: Solutions for increasing the pace of innovation, data lifecycle management, incorporating new technology into care delivery, and improving security and compliance<br/> Industrial: Services and Solutions for customers across Manufacturing, Automotive, Energy, Power &amp; Utilities, Transportation &amp; Logistics<br/> Manufacturing: Optimize production and speed time-to-market<br/> Media and Entertainment: Transform media &amp; entertainment with the most purpose-built capabilities and partner solutions of any cloud<br/> Nonprofit: Services enabling more mission for the money to minimize costs and optimize scale and donor engagement to further nonprofits and NGO’s serving their causes<br/> Power and Utilities: Solutions to extract deep insights from data to manage distributed energy networks and to deliver engaging new customer experiences<br/> Retail: Cloud solutions that help retailers accelerate innovation, optimize operations, and delight customers<br/> Semiconductor: Speed innovation, optimize production, and deliver cutting-edge products and services<br/> Sports: Fuel innovative fan, broadcast, and athlete experiences<br/> Sustainability: AWS provides knowledge and tools for organizations of all sizes across all sectors to build and implement solutions that meet their sustainability goals<br/> Telecommunications: Accelerate innovation, scale with confidence, and add agility with cloud-based telecom solutions<br/> Travel and Hospitality: Solutions to help travel and hospitality companies gain a competitive edge by enhancing customer experiences and operational efficiency</p> <p>AWS featured:<br/> Amazon EC2: Virtual servers in the cloud<br/> Amazon Simple Storage Service (S3): Scalable storage in the cloud<br/> Amazon Aurora: High performance managed relational database with full MySQL and PostgreSQL compatibility<br/> Amazon DynamoDB: Managed NoSQL database<br/> Amazon RDS: Managed relational database service for PostgreSQL, MySQL, MariaDB, SQL Server, Oracle, and Db2<br/> AWS Lambda: Run code without thinking about servers<br/> Amazon VPC: Isolated cloud resources<br/> Amazon Lightsail: Launch and manage virtual private servers<br/> Amazon SageMaker: Build, train, and deploy machine learning models at scale</p> <p>Featured products:<br/> Compute Engine: Virtual machines running in Google’s data center.<br/> Cloud Storage: Object storage that’s secure, durable, and scalable.<br/> BigQuery: Data warehouse for business agility and insights.<br/> Cloud Run: Fully managed environment for running containerized apps.<br/> Google Kubernetes Engine: Managed environment for running containerized apps.<br/> Vertex AI platform: Unified platform for ML models and generative AI.<br/> Looker: Platform for BI, data applications, and embedded analytics.<br/> Apigee API Management: Manage the full life cycle of APIs anywhere with visibility and control.<br/> Cloud SQL: Relational database services for MySQL, PostgreSQL, and SQL Server.<br/> Cloud SDK: Command-line tools and libraries for Google Cloud.<br/> Cloud CDN: Content delivery network for delivering web and video.<br/> Cloud Functions: Event-driven compute platform for cloud services and apps.</p> <p>AI and Machine Learning:<br/> Vertex AI Platform: Unified platform for training, hosting, and managing ML models.<br/> Generative AI on Vertex AI: Build, tune, and deploy foundation models on Vertex AI.<br/> Vertex AI Search and Conversation: Generative AI apps for enterprise search and conversational AI.<br/> AutoML: Custom machine learning model training and development.<br/> Dialogflow: Conversation applications and systems development suite for virtual agents.<br/> Media Translation: Add dynamic audio translation directly to your content and applications.<br/> Natural Language AI: Sentiment analysis and classification of unstructured text.<br/> Recommendations AI: Deliver highly personalized product recommendations at scale.<br/> Speech-to-Text: Speech recognition and transcription supporting 125 languages.<br/> Text-to-Speech: Speech synthesis in 220+ voices and 40+ languages.<br/> Translation AI: Language detection, translation, and glossary support.<br/> Video AI: Video classification and recognition using machine learning.<br/> Vision AI: Custom and pre-trained models to detect emotion, text, more.<br/> Vertex AI Notebooks: A single interface for your data, analytics, and machine learning workflow.<br/> Vertex Explainable AI: Tools and frameworks to understand and interpret your machine learning models.<br/> Business Intelligence</p> <p>Analytics:<br/> Amazon Kinesis: With Kinesis, you can analyze real-time data streams with low-latency at any scale. It enables applications to collect, buffer, and process streaming data as it arrives and react to it instantly instead of waiting hours for data to be collected before processing begins.<br/> Amazon Redshift: Redshift provides a cost-effective way to build a data warehouse and run standard SQL queries against it. You can further analyze these results in various business intelligence tools to gain new insights from the underlying data.<br/> Amazon Athena: Athena is a serverless solution to analyze large datasets in Amazon S3 using standard SQL. It’s fast, easy to use, and doesn’t require complex ETL processes to prepare your data before analysis. You pay only for the amount of data scanned when running each query.<br/> AWS Glue: With serverless AWS Glue, data integration becomes much more comfortable. It helps discover and extract data from multiple sources, prepare this data for use, and organize it into databases, data warehouses, and data lakes for further analysis by specialized tools and custom applications.</p> <p>Compute:<br/> App Engine: Serverless application platform for apps and back ends.<br/> Bare Metal Solution: Infrastructure to run specialized workloads on Google Cloud.<br/> Batch: Fully-managed batch service to schedule, queue, and execute batch jobs at scale.<br/> Cloud GPUs: GPUs for ML, scientific computing, and 3D visualization.<br/> Cloud Run: Fully managed environment for running containerized apps.<br/> Compute Engine: Virtual machines running in Google’s data center.<br/> Deep Learning VM Image: Preconfigured VMs for deep learning applications.<br/> Migrate to Virtual Machines: Server and virtual machine migration to Compute Engine.<br/> Recommender: Proactive, easily actionable recommendations to keep your cloud optimized.<br/> Shielded VMs: Reinforced virtual machines on Google Cloud.<br/> Sole-tenant Nodes: Dedicated hardware for compliance, licensing, and management.<br/> Spot VMs: Compute instances for batch jobs and fault-tolerant workloads.<br/> SQL Server on Google Cloud: Options for running SQL Server virtual machines on Google Cloud.<br/> Tau VM: Compute Engine virtual machines optimized for scale-out workloads.<br/> VMware Engine: Migrate and run your VMware workloads natively on Google Cloud.<br/> Amazon EC2: Virtual servers in the cloud<br/> Amazon EC2: Auto Scaling Scale compute capacity to meet demand<br/> Amazon Lightsail: Launch and manage virtual private servers<br/> AWS App Runner: Build and run containerized web apps at scale<br/> AWS Batch: Run batch jobs at any scale<br/> AWS Elastic Beanstalk: Run and manage web apps<br/> AWS Lambda: Run code without thinking about servers<br/> AWS Local Zones: Run latency sensitive applications on a Local Zone<br/> AWS Outposts: Run AWS infrastructure on-premises<br/> AWS Serverless Application Repository: Discover, deploy, and publish serverless applications<br/> AWS Sim: Space Weaver: Build dynamic, large-scale spatial simulations on AWS managed infrastructure<br/> AWS Snow Family: Physical devices to aggregate and process data in edge locations, then transfer to AWS<br/> AWS Wavelength: Deliver ultra-low latency applications for 5G devices<br/> VMware Cloud on AWS: Build a hybrid cloud without custom hardware<br/> Amazon EC2 (Elastic Compute Cloud): This service lets you rent virtual servers in the cloud. Most of them are virtual machines, ranging from tiny two vCPU and 0.5Gb of RAM to beefy 96-core machines with 384 Gb of RAM. You can also rent physical dedicated servers with EC2. The service is designed for any workload, although you’ll still need to choose the best instances for your specific tasks. Besides generic ones, EC2 also offers cost-saving ARM instances and bare metal Mac minis.<br/> Amazon ECR (Elastic Container Registry): ECR is a highly available and high-performance container registry for easy storage, management, and deployment of your container images. Images can be private to your organization or shared worldwide. It works great with Amazon ECS, AWS Lambda, and AWS Fargate, enabling fast single-click deployments. It’s a fully managed service, and you pay only for the amount of data stored and data transferred over the internet.<br/> Amazon ECS (EC2 Container Service): ECS is a fully managed container orchestration service that enables you to run, scale, and secure Docker applications on Amazon EC2 and AWS Fargate. You can define multiple related containers and configure their various deployment parameters in task definitions. Being a foundational pillar for critical Amazon services, it can natively integrate with Route 53, Secrets Manager, IAM, CloudWatch, and other services.<br/> AWS Fargate: Fargate removes the need to manually provision and manage servers, freeing time to focus on building your application. It’s a serverless compute engine for containers that works both with ECS and EKS (Amazon Elastic Kubernetes Service). It automatically allocates the right amount of isolated compute resources for each container, so there is no additional cost for over-provisioning to handle more load. By design, running each application in isolation also improves its security.<br/> AWS Lambda: Lambda is a serverless compute service that enables you to run your code on the AWS platform without worrying about provisioning, maintenance, and scaling the servers. Your code gets automatically executed on incoming events or HTTP requests at any scale. Most popular languages like Javascript, C#, Java, Go, and Python are supported, and deployment is effortless.<br/> AWS Elastic Beanstack: This easy-to-use service automatically handles the deployment of web applications. It handles provisioning, load balancing, auto-scaling, and app health monitoring without losing full control over the underlying resources. Elastic Beanstack the fastest and most straightforward way to deploy your application to AWS, and it doesn’t require extensive knowledge of the platform and its services.</p> <p>Containers:<br/> Artifact Registry: Store, manage, and secure container images and language packages.<br/> Cloud Build: Solution for running build steps in a Docker container.<br/> Cloud Code: IDE support to write, run, and debug Kubernetes applications.<br/> Cloud Deploy: Fully managed continuous delivery to GKE and Cloud Run.<br/> Cloud Run: Fully managed environment for running containerized apps.<br/> Deep Learning Containers: Containers with data science frameworks, libraries, and tools.<br/> Google Kubernetes Engine (GKE): Managed environment for running containerized apps.<br/> Knative: Components to create Kubernetes-native cloud-based software.<br/> Kubernetes applications on Google Cloud Marketplace: Containerized apps with prebuilt deployment and unified billing.<br/> Migrate to Containers: Components for migrating VMs into system containers on GKE.<br/> Amazon Elastic Container Registry: Easily store, manage, and deploy container images<br/> Amazon Elastic Container Service (ECS): Highly secure, reliable, and scalable way to run containers<br/> Amazon ECS Anywhere: Run containers on customer-managed infrastructure<br/> Amazon Elastic Kubernetes Service (EKS): The most trusted way to run Kubernetes<br/> Amazon EKS Anywhere: Kubernetes on your infrastructure<br/> Amazon EKS Distro: Run consistent Kubernetes clusters<br/> AWS App2Container: Containerize and migrate existing applications<br/> AWS App Runner: Build and run containerized web apps at scale<br/> AWS Copilot: Easily deploy and operate containerized applications<br/> AWS Fargate: Serverless compute for containers<br/> Red Hat Open: Shift Service on AWSManaged Red Hat Open: Shift clusters</p> <p>Databases:<br/> AlloyDB for PostgreSQL: Fully managed, PostgreSQL-compatible database for demanding enterprise workloads.<br/> Bare Metal Solution: Infrastructure to run specialized workloads on Google Cloud.<br/> Cloud Bigtable: Cloud-native wide-column database for large-scale, low-latency workloads.<br/> Cloud Spanner: Cloud-native relational database with unlimited scale and 99.999% availability.<br/> Cloud SQL: Fully managed database for MySQL, PostgreSQL, and SQL Server.<br/> Database Migration Service: Serverless, minimal downtime migrations to the cloud.<br/> Firestore: Cloud-native document database for building rich mobile, web, and IoT apps.<br/> Memorystore: In-memory database for managed Redis and Memcached.<br/> Datastream: Serverless change data capture and replication service.<br/> Firebase Realtime Database (not official GCloud product): NoSQL database for storing and syncing data in real time.<br/> Amazon Aurora: High performance managed relational database with full My: SQL and Postgre: SQL compatibility<br/> Amazon Aurora Serverless V2: Instantly scale to &gt;100,000 transactions per second<br/> Amazon Document: DB (with Mongo: DB compatibility)Fully managed document database<br/> Amazon DynamoDB: Managed NoSQL database<br/> Amazon ElastiCache: In-memory caching service<br/> Amazon Keyspaces (for Apache Cassandra): Managed Cassandra-compatible database<br/> Amazon Memory DB for Redis: Redis-compatible, durable, in-memory database that delivers ultra-fast performance<br/> Amazon Neptune: Fully managed graph database service<br/> Amazon Quantum Ledger Database (QLDB): Fully managed ledger database<br/> Amazon RDS: Managed relational database service for PostgreSQL, MySQL, MariaDB, SQL Server, Oracle, and Db2<br/> Amazon RDS on Outposts: Automate on-premises database management<br/> Amazon Redshift: Fast, simple, cost-effective data warehousing<br/> Amazon Timestream: Fully managed time series database<br/> AWS Database Migration Service: Migrate databases with minimal downtime<br/> Amazon DynamoDB: DynamoDB is a high-performance managed NoSQL database that supports both key-value and document store. It can handle more than 10 trillion requests per day, with peaks of more than 20 million requests per second. This durable and secure database with built-in in-memory caching, backups, and restore is the number one choice for many fastest-growing startups that demand low-latency data storage at any scale.<br/> Amazon ElastiCache: This service offers fully managed Redis and Memcached as high-throughput and low-latency in-memory data stores for your cloud applications. ElastiCache’s primary purpose is to boost web applications’ performance by caching mission-critical data on top of slower databases. It is also suitable for session storage, real-time analytics, and other tasks.<br/> Amazon Aurora: Aurora is a MySQL and PostgreSQL-compatible high-performance distributed relational database. Out of the box, it’s much faster than both MySQL and PostgreSQL and offers high security, availability, and reliability of traditional commercial databases. On top of that, it provides replication across three Availability Zones, point-in-time recovery, and continuous backup to Amazon S3.<br/> Amazon RDS (Relational Database Service): This service manages relational databases in the cloud. It takes care of hardware provisioning, database setup, patching, and backups. Amazon RDS supports various database engines like Amazon Aurora, PostgreSQL, MySQL, Microsoft SQL Server, Oracle Database, and MariaDB.</p> <p>Developer tools:<br/> Artifact Registry: Store, manage, and secure container images and language packages.<br/> Assured Open Source Software: Incorporate the same OSS packages that Google uses into your own developer workflows.<br/> Cloud Build: Continuous integration and continuous delivery platform.<br/> Cloud Code: IDE support to write, run, and debug Kubernetes applications.<br/> Cloud Deploy: Fully managed continuous delivery to Google Kubernetes Engine.<br/> Cloud Deployment Manager: Service for creating and managing Google Cloud resources.<br/> Cloud Functions for Firebase: Write and run app logic server-side without needing to set up your own server.<br/> Cloud Identity: Unified platform for IT admins to manage user devices and apps.<br/> Cloud Scheduler: Cron job scheduler for task automation and management.<br/> Cloud SDK: Command-line tools and libraries for Google Cloud.<br/> Cloud Source Repositories: Private Git repository to store, manage, and track code.<br/> Cloud Tasks: Task management service for asynchronous task execution.<br/> Cloud Workstations: Managed and secure development environments in the cloud.<br/> Duet AI in Google Cloud (Preview): AI-powered collaborator, available across Google Cloud and your IDE.<br/> Firebase Authentication: End-to-end user identity solution to your app for user identification, sign-in, and onboarding.<br/> Firebase Crashlytics: Prioritize and fix stability issues faster. (not official GCloud product)<br/> Firebase Test Lab: On-demand testing infrastructure for Android apps.<br/> Gradle App Engine Plugin: Use Gradle for your App Engine projects.<br/> Infrastructure Manager: Automate deployment and management of Google Cloud infrastructure resources using Terraform<br/> Maven App Engine Plugin: Use Maven for your App Engine projects.<br/> Skaffold: Open source command line tool for Cloud Code, Cloud Build, and Cloud Deploy.<br/> Tekton: Kubernetes-native resources for declaring CI/CD pipelines.<br/> Tools for Eclipse: Plugin for Google Cloud development inside the Eclipse IDE.<br/> Tools for PowerShell: Full cloud control from Windows PowerShell.</p> <p>Networking:<br/> Cloud Armor: Security policies and defense against web and DDoS attacks.<br/> Cloud CDN: Content delivery network for serving web and video content.<br/> Cloud Connectivity: Connectivity options for VPN, peering, and enterprise needs.<br/> Cloud Domains: Easy domain name registration and management.<br/> Cloud DNS: Domain name system for reliable and low-latency name lookups.<br/> Cloud Firewall: Global and flexible firewalls to protect your cloud resources.<br/> Cloud IDS: Cloud IDS (Cloud Intrusion Detection System) provides cloud-native network threat detection with industry-leading security.<br/> Cloud Load Balancing: Service for distributing traffic across applications and regions.<br/> Cloud NAT: NAT service for giving private instances internet access.<br/> Network Connectivity Center: Use Google’s network as your own. Reimagine how you deploy, manage, and scale your networks.<br/> Network Intelligence Center: Network monitoring, verification, and optimization platform.<br/> Network Service Tiers: Cloud network options based on performance, availability, and cost.<br/> Private Service Connect: Secure connection between your VPC and services.<br/> Service Directory: Platform for discovering, publishing, and connecting services.<br/> Virtual Private Cloud (VPC): Virtual network for Google Cloud resources and cloud-based services.<br/> VPC Service Controls: Protect sensitive data in Google Cloud services using security perimeters.<br/> Amazon VPC: Isolated cloud resources<br/> Amazon VPC Lattice: Simplify service-to-service connectivity, security, and monitoring<br/> Amazon API Gateway: Build, deploy, and manage APIs<br/> Amazon Cloud: Front: Global content delivery network<br/> Amazon Route 53: Scalable domain name system (DNS)<br/> AWS App Mesh: Monitor and control microservices<br/> AWS Cloud Map: Service discovery for cloud resources<br/> AWS Cloud WAN: Easily build, manage, and monitor global wide area networks<br/> AWS Direct Connect: Dedicated network connection to AWS<br/> AWS Global Accelerator: Improve application availability and performance<br/> AWS Private 5G: Easily deploy, manage, and scale a private cellular network<br/> AWS Private: Link: Securely access services hosted on AWS<br/> AWS Transit Gateway: Easily scale VPC and account connections<br/> AWS Verified Access: Provide secure access to corporate applications without a VPN<br/> AWS VPN: Securely access your network resources<br/> Elastic Load Balancing (ELB): Distribute incoming traffic across multiple targets<br/> Amazon Route 53: Route 53 is an advanced, highly available, and scalable DNS Service. Besides simple IP lookups, it has sophisticated routing types like GeoDNS, Geoproximity, and Latency Based Routing. Together with health checks and DNS failover, this enables different fault-tolerant low-latency architectures configurable with a simple visual editor.<br/> Amazon CloudFront: CloudFront is a fast and secure programmable content delivery network (CDN) that caches your content and APIs on globally scaled edge locations for more rapid responses. It also offers protection against multiple types of attacks, including network, transport, and application-layer DDoS attacks. CloudFront is cost-effective and deeply integrated with other AWS services like S3, EC2, Route 53, and Elastic Load Balancing.<br/> Amazon API Gateway: API Gateway makes it easy to create, publish, monitor, and secure RESTful and WebSocket APIs. It handles traffic management, CORS, authorization and access control, throttling, monitoring, and API version management. API Gateway can process hundreds of thousands of concurrent API calls. It’s a fully managed service, and you pay only for the API calls your application receives and the amount of outgoing traffic.<br/> AWS Elastic Load Balancing: Elastic Load Balancing distributes incoming application traffic across multiple servers, containers, or Lambda functions. It enables the application to handle more concurrent requests without affecting response time. Multiple request handlers are crucial to make the application highly available, reliable, and fault-tolerant.<br/> Amazon VPC (Virtual Private Cloud): With Amazon VPC, you can create logically isolated virtual networks inside AWS. You have full control over the configuration of the network, its subnets, and routing tables. It’s possible to create a public-facing subnet with internet access for your web servers while keeping most of the backend infrastructure in a private subnet with no internet connection, making it much more secure.</p> <p>Operations:<br/> Cloud Debugger: Real-time application state inspection and in -production debugging.<br/> Cloud Logging: Google Cloud audit, platform, and application logs management.<br/> Cloud Monitoring: Infrastructure and application health with rich metrics.<br/> Cloud Profiler: CPU and heap profiler for analyzing application performance.<br/> Cloud Trace: Tracing system collecting latency data from applications.<br/> Error Reporting: Real time exception monitoring and alerting.</p> <p>Serverless:<br/> API Gateway: Develop, deploy, secure, and manage APIs with a fully managed gateway.<br/> App Engine: Serverless application platform for apps and back ends.<br/> Cloud Functions: Platform for creating functions that respond to cloud events.<br/> Cloud Run: Fully managed environment for running containerized apps.<br/> Workflows: Workflow orchestration for serverless products and API services.<br/> AWS Lambda: Run code without thinking about servers<br/> Amazon API Gateway: Build, deploy, and manage APIs<br/> Amazon DynamoDB: Managed NoSQL database<br/> Amazon EventBridge: Serverless event bus for SaaS apps &amp; AWS services<br/> Amazon Simple Notification Service (SNS): Pub/sub, SMS, email, and mobile push notifications<br/> Amazon Simple Queue Service (SQS): Managed message queues<br/> Amazon Simple Storage Service (S3): Scalable storage in the cloud<br/> AWS Application Composer: Visually design and build serverless applications quickly<br/> AWS AppSync: Fully-managed, scalable GraphQL APIs<br/> Amazon Redshift: Fast, simple, cost-effective data warehousing<br/> AWS Fargate: Serverless compute for containers<br/> AWS Step Functions: Coordination for distributed applications</p> <p>Storage:<br/> Cloud Storage: Object storage that’s secure, durable, and scalable.<br/> Storage Transfer Service: Tools and services for transferring your data to Google Cloud.<br/> Cloud Storage for Firebase: Object storage for storing and serving user-generated content.<br/> Filestore: File storage that is highly scalable and secure.<br/> Local SSD: Block storage that is locally attached for high-performance needs.<br/> Persistent Disk: Block storage for virtual machine instances running on Google Cloud.<br/> Google Cloud Backup and DR: Managed backup and disaster recovery for application-consistent data protection.<br/> NetApp Volumes: File storage service that supports NFS, SMB, and multi-protocol environments.<br/> Parallelstore: High performance, managed parallel file service.<br/> Block storage: High-performance storage for AI, analytics, databases, and enterprise applications.<br/> Amazon Simple Storage Service (S3): Scalable storage in the cloud<br/> Amazon S3 Glacier storage classes: Low-cost archive storage in the cloud<br/> Amazon Elastic Block Store (EBS): EC2 block storage volumes<br/> Amazon Elastic File System (EFS): Fully managed file system for EC2<br/> Amazon FSx for Lustre: High-performance file system integrated with S3<br/> Amazon FSx for Net: App ONTAPFully managed storage built on Net: App’s popular ONTAP file system<br/> Amazon FSx for Open ZFS: Fully managed storage built on the popular OpenZFS file system<br/> Amazon FSx for Windows File Server: Fully managed Windows native file system<br/> Amazon File Cache: High-speed cache for datasets stored anywhere<br/> AWS Backup: Centralized backup across AWS services<br/> AWS Elastic Disaster Recovery (DRS): Scalable, cost-effective application recovery<br/> AWS Snow Family: Physical edge computing and storage devices for rugged or disconnected environments<br/> AWS Storage Gateway: Hybrid storage integration<br/> Amazon S3 (Simple Storage Service): Amazon S3 is a generic object storage service designed for incredible durability, high scalability, availability, security, and performance. It has various storage classes for different use cases. S3 automatically stores copies of objects across multiple systems. It offers a fine-grained access control system and auditing capabilities for compliance. Using Amazon Athena, you can analyze data in S3 at any scale with simple SQL queries.<br/> Amazon S3 Glacier: For data archiving and long-term backups at extremely low-cost, Amazon offers S3 Glacier with extreme durability. There are three options for access to archives. Expedited retrievals typically return data in 1-5 minutes, standard generally complete in 3-5 hours, while the cheapest bulk retrievals take 5-12 hours to get large amounts of data.<br/> Amazon EBS (Elastic Block Storage): EBS is generic long-term high-performance block storage for EC2 instances. It’s designed for both throughput and transactional workloads and can scale to petabytes of data. You can choose different storage types with various throughput and latency suitable for your needs. The EBS replicates volumes within the Availability Zone, and you can use EBS Snapshots to backup your volumes to S3.<br/> Amazon EFS (Elastic File System): Amazon Elastic File System is a fully managed scalable elastic NFS. It grows and shrinks automatically, eliminating the need to provision and manually manage capacity. EFS is designed to provide massively parallel shared access to thousands of EC2 instances with a high level of aggregate throughput and IOPS, maintaining consistent latency.</p> <p>Security, Identity, and Compliance:<br/> AWS Identity and Access Management (IAM): Securely manage access to services and resources<br/> Amazon Cognito: Identity management for your apps<br/> Amazon Detective: Investigate potential security issues<br/> Amazon Guard: Duty: Managed threat detection service<br/> Amazon Inspector: Automate vulnerability management<br/> Amazon Macie: Discover and protect your sensitive data at scale<br/> Amazon Security Lake: Automatically centralize your security data with a few clicks<br/> Amazon Verified Permissions: Fine-grained permissions and authorization for your applications<br/> AWS Artifact: On-demand access to AWS’ compliance reports<br/> AWS Audit Manager: Continuously audit your AWS usage<br/> AWS Certificate Manager: Provision, manage, and deploy SSL/TLS certificates<br/> AWS Cloud HSM: Hardware-based key storage for regulatory compliance<br/> AWS Directory Service: Host and manage active directory<br/> AWS Firewall Manager: Central management of firewall rules<br/> AWS Key Management Service: Managed creation and control of encryption keys<br/> AWS Network Firewall: Network security to protect your VPCs<br/> AWS Payment Cryptography: Simplify cryptography operations<br/> AWS Private Certificate Authority: Create private certificates to identify resources and protect data<br/> AWS Resource Access Manager: Simple, secure service to share AWS resources<br/> AWS Secrets Manager: Rotate, manage, and retrieve secrets<br/> AWS Security Hub: Unified security and compliance center<br/> AWS Shield: DDoS protection<br/> AWS IAM Identity Center: Manage single sign-on access to AWS accounts and apps<br/> AWS WAF: Filter malicious web traffic</p> <h2 id="amazon-web-services">Amazon Web Services</h2> <p>AWS</p> <h2 id="google-cloud-products">Google Cloud Products</h2> <p>GCP</p> <ul> <li>https://cloud.google.com/products/calculator</li> <li>https://cloud.google.com/kubernetes-engine/pricing</li> <li>https://cloud.google.com/storage/pricing/</li> <li>https://cloud.google.com/storage/docs/storage-classes#standard</li> <li>https://cloud.google.com/functions/docs/tutorials/ocr/</li> <li>https://cloud.google.com/run/docs/fit-for-run</li> <li>https://cloud.google.com/kubernetes-engine?hl=en</li> <li>https://cloud.google.com/kubernetes-engine/docs/concepts/autopilot-overview</li> </ul> <p>Follow instructions at https://cloud.google.com/sdk/docs/install to set gcloud CLI.</p> <p>Cloud Functions: an entry point to the google cloud, also connects google cloud services to each other.<br/> Cloud Run: similar to Functions but specialized for self-contained computations<br/> AI Platform Predictions<br/> Kubernetes Engine: flexible scaling<br/> Artifact Registry: store containers and similar items<br/> Compute Engine: compute with its own ephemeral storage<br/> Cloud Storage: flexible, persistent storage accessible to most google products<br/> App Engine</p> <h2 id="google-cloud-functions-ai-inference">Google Cloud Functions AI inference</h2> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gcloud init
gcloud auth login
gcloud config <span class="nb">set </span>project YOUR_PROJECT_ID
gcloud services <span class="nb">enable </span>cloudfunctions.googleapis.com
gcloud services <span class="nb">enable </span>cloudbuild.googleapis.com

<span class="nb">mkdir </span>my_ai_model_function
<span class="nb">cd </span>my_ai_model_function
<span class="c"># main.py: This is your function code.</span>
<span class="c"># requirements.txt: This lists the Python dependencies for your function.</span>
<span class="c"># model.joblib: This is your trained AI model, serialized with joblib. You will need to train and serialize this model elsewhere and then place the file in the directory.</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># main.py
</span><span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">joblib</span>
<span class="kn">from</span> <span class="n">flask</span> <span class="kn">import</span> <span class="n">escape</span><span class="p">,</span> <span class="n">jsonify</span>

<span class="c1"># Load your trained model; make sure the path matches where you've put your model file.
</span><span class="n">model</span> <span class="o">=</span> <span class="n">joblib</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">'</span><span class="s">model.joblib</span><span class="sh">'</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">ai_model_inference</span><span class="p">(</span><span class="n">request</span><span class="p">):</span>
    <span class="n">request_json</span> <span class="o">=</span> <span class="n">request</span><span class="p">.</span><span class="nf">get_json</span><span class="p">(</span><span class="n">silent</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">request_json</span> <span class="ow">and</span> <span class="sh">'</span><span class="s">features</span><span class="sh">'</span> <span class="ow">in</span> <span class="n">request_json</span><span class="p">:</span>
        <span class="c1"># Assuming "features" is a list of features for the model to make a prediction.
</span>        <span class="n">features</span> <span class="o">=</span> <span class="n">request_json</span><span class="p">[</span><span class="sh">'</span><span class="s">features</span><span class="sh">'</span><span class="p">]</span>
        <span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">([</span><span class="n">features</span><span class="p">])</span>
        <span class="k">return</span> <span class="nf">jsonify</span><span class="p">({</span><span class="sh">'</span><span class="s">prediction</span><span class="sh">'</span><span class="p">:</span> <span class="n">prediction</span><span class="p">.</span><span class="nf">tolist</span><span class="p">()})</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="sh">'</span><span class="s">No features provided for inference.</span><span class="sh">'</span><span class="p">,</span> <span class="mi">400</span>
</code></pre></div></div> <pre><code class="language-txt">scikit-learn==0.24.2
joblib==1.0.1
numpy==1.20.3
</code></pre> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gcloud functions deploy ai_model_inference <span class="se">\</span>
    <span class="nt">--runtime</span> python39 <span class="se">\</span>
    <span class="nt">--trigger-http</span> <span class="se">\</span>
    <span class="nt">--memory</span> 2048MB <span class="se">\</span>
    <span class="nt">--timeout</span> 540s <span class="se">\</span>
    <span class="nt">--max-instances</span> 1

curl <span class="nt">-H</span> <span class="s2">"Content-Type: application/json"</span> <span class="se">\</span>
     <span class="nt">-d</span> <span class="s1">'{"features": [5.1, 3.5, 1.4, 0.2]}'</span> <span class="se">\</span>
     YOUR_FUNCTION_URL
</code></pre></div></div> <p>You can monitor the execution of your function and view logs in the Google Cloud Console under the Cloud Functions section. If you need to update your function or model, you can redeploy using the same gcloud functions deploy command.</p> <p>Additional Considerations<br/> Ensure that the model size and memory requirements fit within the limits of Google Cloud Functions.<br/> Keep in mind the cold start times. If your function hasn’t been invoked recently, the first invocation may take longer as the environment is initialized.<br/> For more complex or larger models, consider using Google Cloud Run or AI Platform Predictions, which can handle heavier workloads.</p> <p>By following these steps, you can deploy an open-source AI model to Google Cloud Functions and run inferences on-demand without maintaining a persistent server.</p> <p>For infrequent and sporadic AI model inferences, Google Cloud Functions or Google Cloud Run are typically the most cost-effective choices due to their per-invocation billing and ability to scale to zero, which means you don’t pay for anything when the services are not in use. They both offer a free tier, which can be sufficient for light workloads.</p> <p>AI Platform Predictions can be cost-effective if you leverage its features and scale, but it might be overkill for infrequent use unless you need its specific integrated services.</p> <p>Ultimately, the choice of product will depend on your specific requirements, such as the size and complexity of your AI model, the need for integration with other Google Cloud services, the expected latency, and your budget. The serverless options (Cloud Functions and Cloud Run) will generally offer the best cost savings for infrequent usage, while GKE and App Engine may offer more functionality and flexibility at a higher cost.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Service Cloud computing enables provisioning computational-based services through the internet. It is a form of shared resource interfaced through a service. Advantages cloud technologies provide include Cost Speed and agility Security Maintenance Scalability and elasticity Reliability Multitenancy Easy access to resources]]></summary></entry><entry><title type="html">Category Theory</title><link href="https://t-wilkinson.github.io/blog/2024/category-theory/" rel="alternate" type="text/html" title="Category Theory"/><published>2024-07-23T16:40:24+00:00</published><updated>2024-07-23T16:40:24+00:00</updated><id>https://t-wilkinson.github.io/blog/2024/category-theory</id><content type="html" xml:base="https://t-wilkinson.github.io/blog/2024/category-theory/"><![CDATA[<p>Category theory concerns itself with representing constructions with internal cohesion (how objects relate to each other through morphisms). Functors relate these contructions by embedding the domain construction into the codomain construction. Natural transformations allow comparing construction embeddings. This gives rise to topos theory, where a topos is a collection of constructions in $D$ made from $C$. It is insightful to view functors as objects existing in a category, and natural transformations as special morphisms preserving the internal structure of these functor objects. Morphisms preserve the structure of objects in context, natural transformation additionally preserve the structure of domains</p> <p>Category theory can be viewed as an algebraic object, like a groupoid without requirement of invertibility, or monoid without requirement of closed operations. In this lense, categories express some internal cohesion of elements expressed through morphisms. What makes categories useful is that they express necessary requirements for internal cohesion with an identity, which allows for proofs by algebra (or commuting diagrams) and identification of definitive objects. Sheaves on a category are like more general group actions. Continuing this perspective, functors are algebraic homomorphisms, maps between sets preserving the cohesive structure expressed by the domain category’s morphisms. In this sense, it is instructive to view categories as sets with some cohesivity, that is, a category should not in general be interpreted as a kind of set theory. Instead, we look at stacks valued in a topos (like set) which express category actions.</p> <p>Topos theory is a way to construct logic out of some type of construction. This logic is one that is conceptually useful and allows one to talk about various kinds of objects that exist from said construction. Universal properties like limits or kan extensions allow expressing for every relationship of a certain kind $F(A’) → X$, there is one construction its associated relationship $F(A) → X$ that it factors through $F(A’) → F(A)$. The functor is essentially projecting the category onto the construction $X$, isolating the objects of the domain of F that relate to $X$. Of the objects of the domain of F, these form a construction with X, and of those, F(A) is the best. Then we have exponential objects which represent all the ways A relates to B. The power object relates the function space and subobject classifier. Subobject classifier lets you talk about restrictions, smaller or larger object “spaces”, specify locations within an object.</p> <p>We like to look at objects as labels and the morphisms as expressing the elements and other relationships that objects have within a context of discorse. This is the value of representable functors and the yoneda embedding: morphisms express everything</p> <p>It is often instructive to look at functor spaces, a category of functors between two categories. We look at what kind of constructions functors represent, similar to looking at function spaces on topological spaces which represent embeddings. That is, we look at the construction or embedding and relationships, not as the extensional mapping between points or objects.</p> <p>Practicality: Category theory is a fundamental tool in math and physics for formalizing, organizing, comparing, concepts. It serves to abstract away irrelevant information to isolate the structure and semantics of concepts and provides ways to compare these structures, away from their nuances. It provides a single language to describe concepts found in disparate areas of mathematics and physics. Perhaps the most fundamental concept in category theory, universal properties, describes how an object relates to every construction in a context.</p> <p>A category is a context. Universal properties are different isolations of context. Take, for example, initial objects, which express the structure that every object/domain has, and terminal objects which express the underlying structure that every morphism/codomain must have.</p> <p>Category theory is an abstract species which enables lifting from the intricacies of various mathemtical fields to focus on the consistency (and inconsistency) between them. It enables sharing ideas, intuition, and proofs accross disciplines. One understands a category of a collection of objects of a certain type and relationships between those objects. The prevailing tool of category theory is that of a universal property which expresses the mathematical function of a object by how it relates to other objects within a categorical context. Functors recontextualize objects. The yoneda lemma expresses how any object can be charazterized purely by its universal properties, that is, its relationship with all other objects. Limits express how objects can be constructed out of existing ones.</p> <p>Mathematics is full of constructions which map objects from one category into another category. These constructions are functorial if they preserve the morphisms between objects as well. In other words, a functor is a mapping of categories which preserves the domain category structure.</p> <p>Categorification is the process of adding morphisms to existing sets to express the structure of set elements. Functorality of mathematical constructions can then be determined through categorical language.</p> <p>One can view all of mathematics as a site where objects represent axiosm with the covering representing which axioms imply(are implied by) other axioms. One sheaf maps axioms to collection of mathematical systems that satisfy those axioms. An index category maps existing mathematical subjects onto the site.</p> <p>What is it? Category theory is the abstract study of how things (manly mathematical objects) are related. A category defines 3 notions, categories: collection of objects, objects: elements of categories, and morphisms: arrows between objects. By studying how things relate, rather than what they are internally, we can ignore extraneous details and focus on the important differences.</p> <p>The most important notion in category theory is that of the universal property. A property which uniquely defines an object and an arrow to/from every other object in question. These properties are useful as they define how a singular object uniquely relates to all other objects.</p> <p>Category theory concerns itself less with how to find certain constructions, but instead what properties certain constructions would have, if they were found. It allows you convenient language to talk about properties and constructions.</p> <ul> <li>Basic definitions <ul> <li>A category consists of a collection of objects, arrows between the objects, and certain composition rules about the arrows.</li> <li>An arrow is between two objects. Two arrows that share a target/src can be composed to form a new arrow.</li> <li>For every object there is an identity arrow with same src and target, such that composition (in the natural way) with any other arrow is equal to that arrow.</li> </ul> </li> <li>Important concepts <ul> <li>Commuting Diagram</li> <li>Duality</li> <li>Isomorphism</li> </ul> </li> </ul> <h1 id="definition">Definition</h1> <p>A category consists of the following data:</p> <ul> <li>A class ob(C) of objects, for every object there is an identity morphism</li> <li>A class mor(C) of morphisms</li> <li>A domain and codomain class function: $dom: mor(C) → ob(C)$, $cod: mor(C) → ob(C)$</li> <li>A composition binary operator $hom(a,b) × hom(b,c) → hom(a,c)$, with associativity and respects the identity morphism</li> </ul> <p>Small and large categories:<br/> When the class of objects and morphisms is a set, the category is called small. If just the homsets are small, the category is called locally small.</p> <p>Small categories exist to avoid inconsistencies and paradoxes found with infinite sets. However, this avoids the creation of self-referential structures. For example the set of all sets. Universes in particular are a useful construction that utilizes self-referentiality. Locally small categories can include self-referential structures, or other proper classes of objects, but still restrict the homset between any two objects to be small, resulting in the best of both worlds.</p> <h1 id="important-constructions">Important constructions</h1> <h2 id="opposite-category">Opposite category</h2> <p>Mathematics is full of topological-like structures and other spaces. These spaces are often constructed through some notion of inclusion. The opposite category represents restricting these spaces to view a smaller, more local picture. Contravariant functors also capture this idea, further noting what happens when you assign information to these restrictions. Another view of opposite categories/contravariant functors is that they represent looking at various structures in the codomain or looking to trace morphisms from where they came instead.</p> <h2 id="morphisms">Morphisms</h2> <ul> <li>Monomorphism</li> <li>Bimorphism</li> <li>Epimorphism</li> <li>Isomorphism: are key in category theory as they allow defining to and referring to structures without describing their contents. Instead, one cares about how the structure behaves in a category. Isomorphic structures for all intents and purposes, are equal within a category. However, they could have different content which can be observed in other categories.</li> <li>Endomorphism</li> <li>Automorphism</li> <li>Retraction</li> <li>Section</li> </ul> <p>Category theory allows expressing structure through morphisms. Morphisms intrinsically represent the relationship of information. This is in contrast to the set theory perspective of understanding structure through expositional contents.</p> <p>Category theory treats objects as black boxes. Morphisms are the real object of study. The idea is to understand objects by studying how they relate to each other. In fact, an essential result of category theory (which I don’t quite understand) is the yoneda lemma. Which states that you can understand an object by understanding all of its morphisms. The formalization of tell me who your friends are and I’ll tell you who you are.</p> <h2 id="functor">Functor</h2> <ul> <li>Interpretations <ul> <li>When viewing functors as objects in a functor category, consider them like an embedding into a topological space. Each functor is like its own shape. In the case of topos theory, sheafs or presheafs are like embeddings from some base category into the category of set, where each embedding selects a shape in set theory.</li> <li>One can interpret functors as constructing a shape in their codomain using the domain as a template.</li> <li>View the functor as indexing objects in the codomain from the domain</li> <li>Recontextualization</li> <li>Categories have objects whose meanings are derived from morphisms. Functors carry that structure, and meaning of each object into a new category.</li> </ul> </li> <li>Full</li> <li>Faithful</li> <li>Covariant</li> <li>Contravariant</li> <li>Functors express how categories are related. They are analogies.</li> <li>Equivalence of categories</li> </ul> <p>A functor $F: A → B$, or generally structure preserving maps (morphisms), are ways to probe a space B from space A. Stacks allow probing a space B from the structure represented by a probe A.</p> <p>The yoneda lemma is like flattening a representable functor of an object onto a set, preserving the morphism structure in C, and saying there are as many ways to do that precisely as there are collection of elements in the presheaf on that object.</p> <h2 id="natural-transformation">Natural transformation</h2> <ul> <li>Relate constructions</li> <li>Naturally isomorphic</li> <li> <p>Natural transformations express how relations are related. They are comparisons of analogies.</p> </li> <li>An isomorphism of functors is stronger than the image of each object being isomorphic, as the latter does not imply there is a natural transformation. If there does exist a natural transformation, then the statements are equivalent.</li> </ul> <h2 id="universal-properties">Universal properties</h2> <p>Universal properties are all manifestations of the same underlying idea. They are special cases and generalizations over one another.</p> <p>Universal properties formalize the idea of looking at every possible construction, idea, or realization, and seeing which one is factored through/is factored through.</p> <p>Universal properties give rise to an adjoint pair only if every object has a construction.</p> <ul> <li>One fundamental concept in category theory is that of universal properties.</li> <li>These capture the notion of finding a unique object and arrow along with a unique relationship with every other object+arrow (in some class of objects).</li> <li>Finding the best solution to a question given a collection of candidate objects and the related morphisms</li> <li>end and coend</li> </ul> <h3 id="adjunction">Adjunction</h3> <ul> <li>Global/Local</li> <li>Integration/derivation</li> <li>Probability/inference</li> <li>Mass/density</li> <li>Type/Value of function</li> </ul> <p>Adjoint functor theorem: right adjoints preserve all limits and left adjoints preserve all colimits. The adjoint functor theorem states the converse, that under certain properties, a functor that preserves limits is a right adjoint and a functor that preserves colimits is a left adjoint.</p> <p>Something is efficient if it satisfies a universal property.</p> <p>Something is formulaic if it defines a formula, that is, it maps every object of a category to a new context.</p> <p>Pairs of adjoint functors can be used to describe optimal solutions to problems. Such as referring to the best universal construction in a category.</p> <h3 id="limits">Limits</h3> <p>Think about how limits behave in the opposite category.</p> <p>Limit and colimits are ways of gluing objects together. A category with all limits has objects which can be constructed to contain any structure of objects.</p> <p>Colimits primarily about how the object factors through other objects.<br/> Limits primarily about how objects factor through the limiting object</p> <p>A limit is a universal morphism from $\triangle$ to $F$. A colimit is a universal morphism from $F$ to $\triangle$. A cone is a natural transformation $ψ : X → F$ considered a morphism family indexed by objects in $C$.</p> <ul> <li>Limits <ul> <li>Terminal objects</li> <li>Products</li> <li>Equalizers</li> <li>Kernels</li> <li>Pullback (fibered product) <ul> <li>The categorical semantics of an equation. The pullback is the largest fiber bundle such that its projections can be made equal.</li> <li>The pullback can be a non-trivial product, that is, it can represent global information of fibers in relation to their projections. The morphisms $f$ and $g$ can be viewed as proofs that corresponding points in $A$ and $B$ are equal (can be mapped to the same point).</li> </ul> </li> <li>Inverse limit</li> </ul> </li> <li>Colimits <ul> <li>Initial objects</li> <li>Coproducts</li> <li>Coequalizers Cokernels and quotients</li> <li>Pushout</li> <li>Direct limit</li> </ul> </li> <li>Incredible abstraction/idea</li> <li>Type of universal property. Similar to the idea of limits in real analysis.</li> <li>Find the best object of the objects that relate to another category (considered as a sub category)</li> </ul> <h3 id="kan-extension">Kan extension</h3> <ul> <li>Extend a function defined on a part to a whole</li> <li>Restriction/extension of sheaves (extension of the site they are defined on)</li> <li>Extend a function defined on a partial set to a function on a whole</li> <li>Extend the expential $2^- : N -&gt; R_{&lt;0}$ to $2^- : R -&gt; R_{&lt;0}$</li> </ul> <h2 id="yoneda-lemma">Yoneda lemma</h2> <p>The Yoneda Lemma specifies that objects can be uniquely characterized by their hom-sets. This universal property applied to each object can be used as a definition for a construction, known as universal construction.</p> <p>States that the natural transformations between two presheafs a hom functor $Hom(-,A)$ and functor $F$, $Nat(Hom(-,A),F)$, are in one to one correspondance, dependent on the number of representations of the identity morphisms in $F(A)$. There is a one-to-one correspondance between the number of natural transformations taking $idₐ$ to $F(A)$, $idₐ$ to each element of $F(A)$. The elements of $F(X)$ are completely determined by wherever natural transformations take $idₐ$.</p> <p>For example, the Hom set Hom(-,X) in the category of topological spaces gives all continuous maps onto X, including structures like all embeddings and points of X. One can associate this Hom functor with any other functor and a natural bjiection between the two. Further, in the special case that the other functor is Hom functor on another object Hom(-,Y), one can recover all relevant information in the category.</p> <p>The Yoneda embedding is useful as it induces a fully faithful embedding of a category into the category of presheafs on that category, preserving all limits. Further, if this category is small, the category of presheafs has all finite colimits (it is the free cocompletion).</p> <h2 id="exponential-objects">Exponential objects</h2> <p>Can represent a function space. Function space is a set of functions between two sets (objects) where functions take on structure from the domain/codomain. Take the collection of morphisms between the interval and euclidean space. The function space can be interpreted as embeddings of each function in euclidean space. Or maps from a set to vector space, which assigns each element to a vector. Each function then can be viewed as a collection of vectors.</p> <h1 id="overview">Overview</h1> <p>Morphisms and functors at the base level express a kind of binary relationship, either themselves and their objects are equal or they are not. But higher mappings between these structures exposes a higher category, now interpreting those morphisms and functors as objects, with this binary relationship on the mappings between these new objects.</p> <ul> <li>Higher category theory <ul> <li>Universal constructions <ul> <li>Limits</li> <li>Adjunctions</li> <li>Kan extension</li> <li>End</li> <li>Representable functors</li> </ul> </li> <li>View morphisms in higher categories as expressing the higher-dimensional relationships between these higher objects.</li> <li>n-category</li> <li>(n,r)-category<br/> (2,1)-category := 2-Category<br/> (1,1)-category := Category<br/> (1,0)-category := 1-groupoid<br/> (0,0)-category := Set<br/> (-1,0)-category := Truth value<br/> (-2,0)-category := Trivial</li> </ul> </li> <li> <p>Category theory should be thought of a semantic view of various properties, and categories with certain structure are additional properties. In the semantic view of logic, these properties correspond to axioms.</p> </li> <li>Model category <ul> <li>A model category is a category with 3 classes of morphisms, with each class closed under composition. <ul> <li>Weak equivalences <ul> <li>Context for doing homotopy theory</li> <li>In Top, these correspond to fundamental group/groupoid</li> </ul> </li> <li>Fibrations (nice surjections) <ul> <li>In the category Top, these correspond to fiber bundles</li> </ul> </li> <li>Cofibrations (nice injections) <ul> <li>In the category Top, express how spaces can include smaller spaces</li> </ul> </li> </ul> </li> <li>An (∞,1)-category is a category with 1-morphisms and additional which tracks which 1-morphisms are equivalences.</li> </ul> </li> <li>Interpretations <ol> <li>Algebra of paths <ul> <li>Category is an algebra of paths</li> <li>Functors are recontextualization of the algebra of paths</li> </ul> </li> <li>Contexts <ul> <li>Categories are contexts</li> <li>Functors are constructions</li> <li>Natural Transformations are recontextualizations</li> </ul> </li> </ol> </li> </ul> <h1 id="categories">Categories</h1> <ul> <li>Evolving system <ul> <li>Slice category of graphs, mapping a graph to graphs it can evolve to</li> <li>Time-indexed graphs in category of graphs</li> <li>Graphs where morphisms are time dependent transformations</li> </ul> </li> <li>Category with morphisms or functors as objects. One can view morphisms or functors as labelled processes between two objects or constructions. Morphisms on these objects are modifications of the process, alterations of the composition, or changing of the domain/codomain.</li> <li>Topology <ul> <li>Morphisms are closeness-preserving maps</li> <li>Homeomorphisms can be seen as structure preserving maps (functors)</li> <li>Open sets can be seen as the structure of the category of topological spaces</li> </ul> </li> <li>Power</li> <li>Physics</li> <li>Business</li> <li>Computer science <ul> <li>Internet</li> <li>Program</li> <li>Category Hask <ul> <li>Objects are types, types of types, etc.</li> <li>Morphisms are functions</li> <li>n-cells (n&gt;1) are functions that take other functions</li> <li>Function taking types and returning types are type constructors</li> <li>Provides an interesting context in which one can run computationson</li> <li>There is a certain order required of the natural transformations (maps to sequential operations)</li> <li>Provides necessary tooling for working with contexts</li> <li>Endofunctor with coherent natural transformations</li> <li>Monoid in the category of endofunctors</li> <li>Monoids are associative and have an identity, so order matters. So application of unit/join have an order in which to be applied. So it represents sequential computation.</li> <li>Functors in haskell are endofunctors</li> <li>Functors take objects to a new context</li> <li>Ff defines a way to apply the function f in the resulting context</li> <li>Examples <ul> <li>Maybe := Context with failure</li> <li>Either := Context with either/or</li> <li>List := Context with multiple values</li> <li>IO := Context with actions</li> </ul> </li> <li>Monad <ul> <li>A category is a particular context which defines potential structure on objects and their relationships. A functor is a change of contexts, preserving the structure, which converts the structure of objects and morphisms to satisfactory corresponding items in the codomain category. A natural transformation is a morphism of functors which modifies how objects and morphisms are recontextualized. Monads are a certain context which have a notion of sequential action. This is captured through the natural transformations join and unit, which have identity, are associative but not commutative. This is, in some sense, the nicest algebraic properties one can have while still requiring order of application. A monad is then a certain context in which one can do sequential actions.</li> <li>Yet another crappy monad explanation. A monad is a context in which one can do sequential computation. And, given a value in the monad context, one can wrap, or unwrap the context. We can sequentially apply these wrap and unwrap functions in addition to applying normal functions to values in the monad context. For example, imagine a context in which computations can fail, or succeed with a value. We can sequentially apply functions (like 1/). To do this apply the function (1/) to a value x in the context. If the value x already represents a failed computation, we simply fail again. If x represents a success, then we take its value and apply the function, returning success (1/x) or fail if, in this case, x happened to be 0. Alternatively we can image a context in which we want to have computations with multiple values and where we apply functions over each of these values.</li> </ul> </li> </ul> </li> </ul> </li> <li>Neuroscience <ul> <li>Brain <ul> <li>Objects are open sets representing cells (mainly neurons) that are connected</li> <li>One can create sub categories representing the vascular system, neurons, glial cells, etc.</li> <li>Objects <ul> <li>Cortical columns</li> <li>Functional area</li> <li>Boccas area</li> <li>Neocortex</li> </ul> </li> <li>Sheafs <ul> <li>Maps open sets to probability that they are a certain area. Appears as a distribution. Each function represents a different brain function. Represents this as a 3d(or higher with time, etc.) image, with higher density representing the area. (x,y,z,t,density){}</li> </ul> </li> <li>Forms a covering</li> </ul> </li> </ul> </li> <li>Math <ul> <li>Simplex</li> <li>Set</li> <li>Binary operations <ul> <li>Poset</li> <li>Preorder</li> <li>Equality</li> </ul> </li> <li>Numbers</li> <li>Topological spaces</li> <li>(Smooth) manifolds</li> <li>Linear algebra</li> <li>Homotopy type theory</li> </ul> </li> <li>Sets</li> <li>Relations</li> <li>Algebraic <ul> <li>Groups <ul> <li>Magmas</li> <li>Abelian groups</li> <li>Group actions</li> <li>Monoid</li> <li>Groupoid</li> </ul> </li> <li>Rings <ul> <li>Fields</li> </ul> </li> <li>Modules <ul> <li>Vector spaces</li> </ul> </li> </ul> </li> <li>Constructions on categories <ul> <li>Free category</li> <li>Functor category</li> <li>Kleisli category</li> <li>Opposite category</li> <li>Quotient category</li> <li>Product category</li> <li>Comma category</li> <li>Subcategory <ul> <li>Take a subset of elements of a category, and fill it freely with necessary data to satisfy the structure of the original category.</li> <li>Examples <ul> <li>Subtopology</li> <li>Subgroup</li> <li>Subfield</li> <li>Subspace</li> </ul> </li> </ul> </li> </ul> </li> </ul> <h1 id="gpt">GPT</h1> <p>“Why are universal morphisms so useful to represent a diverse collection of ideas and concepts? Can you give a list of examples and ideas in mathematics formulated using universal properties?”</p> <p>Universal morphisms are powerful conceptual tools in mathematics because they allow for the abstract definition of a wide variety of constructions in terms of unique mappings that satisfy certain properties. The elegance of universal properties is that they can often define objects up to unique isomorphism, which means that the definition captures the essence of the structure without reference to any particular construction or representation. This level of abstraction makes it easier to prove general results and to transfer intuition and techniques across different areas of mathematics.</p> <p>Here is a list of examples and ideas in mathematics that are commonly formulated using universal properties:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Products and Coproducts: In category theory, the product of a collection of objects is defined by a universal property involving a unique morphism from the product object to each factor. Dually, the coproduct is defined by a universal property involving a unique morphism from each factor to the coproduct object.

Initial and Terminal Objects: An initial object in a category is one for which there is a unique morphism from it to any other object. Dually, a terminal object has a unique morphism from every object to it.

Free Objects: A free group (or free object in another category) is defined by a universal property related to the way elements of a set map into the group. For example, a free group on a set S is a group along with a function from S to the group such that any function from S to another group factors uniquely through this function.

Limits and Colimits: Limits in category theory generalize products, pullbacks, and other constructions. They are defined by a universal property that involves cones and a unique factorization property. Colimits are the dual notion and include coproducts, pushouts, and more.

Adjoint Functors: Given two categories and a pair of functors between them, an adjoint functor is defined by a universal property that specifies how the functors relate to each other via natural isomorphisms between certain Hom-sets.

Tensor Products: In the context of modules over a ring, the tensor product is defined by a universal property involving bilinear maps. The tensor product of two modules is the most general module that allows for bilinear maps from the product of the two modules to be factored uniquely through it.

Direct and Inverse Limits: In the context of algebra, direct limits (also known as inductive limits) and inverse limits (projective limits) of systems of objects are defined by universal properties that describe how elements of the systems map to or from the limit object.

Homotopy Limits and Colimits: In algebraic topology, homotopy limits and colimits generalize the notion of limits and colimits to accommodate homotopy considerations.

Representable Functors: A functor is said to be representable if it is naturally isomorphic to the Hom-functor for some object. This is a universal property that captures the essence of representability.

Universal Coefficient Theorem and Universal Covering Spaces: In algebraic topology, the universal coefficient theorem relates the homology of a space with the cohomology, and universal covering spaces are unique up to isomorphism with a certain universal property related to covering maps.
</code></pre></div></div> <p>“How can exponential objects be represented in a category with universal objects. What is the category and what is the functor valued in? Can other ideas like limits or kan extensions be used to define exponential objects and if so, how?”</p> <p>Exponential objects in category theory can be understood as a generalization of the function space concept from set theory, but within the context of an arbitrary category. They are a way to represent “functions” or “morphisms” within a category in a way that captures the notion of one object being raised to the power of another.</p> <p>For a category ( \mathcal{C} ), an exponential object is usually denoted by ( B^A ), where ( A ) and ( B ) are objects in ( \mathcal{C} ). The exponential object ( B^A ) represents the object of all morphisms from ( A ) to ( B ) in a way that is suitably “categorical.”</p> <p>To define exponential objects using universal properties, we use the concept of an adjunction. Specifically, in a category ( \mathcal{C} ) with binary products, an object ( B^A ) is the exponential object of ( B ) and ( A ) if there exists an isomorphism that is natural in ( X ):<br/> [ \mathcal{C}(X \times A, B) \cong \mathcal{C}(X, B^A) ]<br/> This isomorphism gives us a bijection between the set of morphisms ( X \times A \to B ) and the set of morphisms ( X \to B^A ). The object ( B^A ) together with a morphism ( \text{eval} : B^A \times A \to B ) (called the evaluation morphism) must satisfy this property universally.</p> <p>In a more formal sense, the functor valued in is the functor:<br/> [ - \times A : \mathcal{C} \to \mathcal{C} ]<br/> This functor takes an object ( X ) and maps it to the product ( X \times A ). The right adjoint to this functor, if it exists, will take an object ( B ) and map it to the exponential object ( B^A ), which represents the object of morphisms from ( A ) to ( B ).</p> <p>Other categorical concepts like limits and Kan extensions can also be related to exponential objects. For example:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Limits: Exponential objects can be seen as a specific form of limit. Specifically, if you have a diagram of shape ( J ) in your category ( \mathcal{C} ), a limit would be the universal object representing a cone over that diagram. Similarly, the exponential object ( B^A ) is the universal object representing all ways of mapping ( A ) into ( B ) when viewed through the adjunction lens.

Kan Extensions: A Kan extension is a way of extending or restricting functors between categories. If you have a functor ( F : \mathcal{C} \to \mathcal{D} ) and a functor ( G : \mathcal{C} \to \mathcal{E} ), then a right Kan extension of ( F ) along ( G ) is a functor ( \text{Ran}_G F : \mathcal{E} \to \mathcal{D} ) together with a natural transformation satisfying a universal property. The relationship between Kan extensions and exponential objects is more abstract, but one can think of the exponential object as a kind of solution to a universal problem, much like how Kan extensions solve universal problems relating to the extension of functors.
</code></pre></div></div> <p>In general, categorical concepts like exponential objects, limits, and Kan extensions are deeply interwoven and can be used to describe and solve problems in a very general and abstract way, which is one of the strengths of category theory.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Category theory concerns itself with representing constructions with internal cohesion (how objects relate to each other through morphisms). Functors relate these contructions by embedding the domain construction into the codomain construction. Natural transformations allow comparing construction embeddings. This gives rise to topos theory, where a topos is a collection of constructions in $D$ made from $C$. It is insightful to view functors as objects existing in a category, and natural transformations as special morphisms preserving the internal structure of these functor objects. Morphisms preserve the structure of objects in context, natural transformation additionally preserve the structure of domains]]></summary></entry></feed>