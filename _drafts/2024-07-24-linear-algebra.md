---
layout: post
title: Linear Algebra
date: 2024-07-24 15:08:44
description:
tags: math
categories:
---

- [Matrix Cookbook]{ref=./resources/reading/matrixcookbook.pdf}

- [Linear Algebra Done Right - Sheldon Axler](./resources/reading/Linear-Algebra-Done-Right-by-Axler,Sheldon-Jay.pdf)
- [A new way to start linear algebra - Gilbert Strang](https://www.youtube.com/watch?v=YrHlHbtiSM0&list=PLUl4u3cNGP61iQEFiWLE21EJCxwmWvvek)
- [Elementary Linear Algebra, Applications - Howard Anton, Chris Rorres](./resources/reading/Elementary-Linear-Algebra,Applications-Version-by-Howard-Anton,Chris-Rorres.pdf)
- [Linear Algebra and Its Applications  4ed  - Gilbert Strang](./resources/reading/Linear-Algebra-and-Its-Applications--4ed--by-Gilbert-Strang.pdf)
- [Introduction to Linear Algebra - Serge Lang](./resources/reading/Introduction-to-Linear-Algebra-by-Serge-Lang--auth.-.pdf)
- [Linear Algebra - Stephen H. Friedberg,Arnold J. Insel,Lawrence E. Spence](./resources/reading/Linear-Algebra-by-Stephen-H.-Friedberg,Arnold-J.-Insel,Lawrence-E.-Spence.pdf)
- [Linear algebra - Jim Hefferon](./resources/reading/linearalgebra-Jim-Hefferon.pdf)
- [Linear algebra - fcla](./resources/reading/linear-algebra-fcla.pdf)
- [Linear algebra solutions - fcla](./resources/reading/linear-algebra-solutions-fcla.pdf)
- [Linear algebra - Serge Lang](./resources/reading/Linear-algebra-by-Lang,Serge.pdf)

- Linear map
    - A linear map can be described as a transformation of basis vectors and viewing the rest of the space as a linear combination of these basis vectors, that is, a scalar for each basis vector.

- https://en.wikipedia.org/wiki/Linear_algebra
- All the math you missed but need to know
- Generalizing the reals to any field allows finding solutions to linear equations of that field
- Change of basis highlight the symmetries of a linear transformation under different basis

- Key theorem (the following are equivalent) of a nxn matrix A
    - Transpose(A) is Invertible
    - A is Invertible
    - For any b, there is a unique solution x to Ax=b
    - Det(A) != 0
    - Ker(A) == 0
    - Eigenvalues(A) != 0
    - Columns(A) == Linearly independent
    - Rows(A) == Linearly independent

- Linear algebra serves to generalize the finding of solutions to systems of linear equations
    - For a linear transformation T: m->n
        - m > n => There are generally no solutions
        - m < n => There are generally infinite solutions
- As systems of linear equations
    - We view vectors as a collection of variables
    - Matrices are corresponding coefficients
    - Solutions are found by finding the solution b for Av=b
    - Most problems can be reduced to problems in linear algebra. Problems in linear algebra reduce to solving systems of linear equations
    - The underlying field F are coefficients
- Study of objects that encode a notion of coordinate space and operations which preserve the linear structure of that space
- A coordinate space is a collection of (field) coordinate points which specify a linear combination of basis vectors of a vector space
- Philosophy
    - Simple axioms which enable simple solutions
    - Complex mathematical like manifolds can be approximated by a linear system
        - Can map complex non-linear problems to linear problems
- Dual space
    - Linear functional
        - Specifies where each basis vector should land when squishing all basis vectors onto its own span
        - Linear functional matmul with a vector is a inner product
- Inner product
    - Dot product
- Cross product == Outer product
- Formalization of mathematics of understanding and transforming space
- Linearity
- Normal vector
- Pseudovector
- Invariant space
    - Where the image of a subset of the space is a subset of itself
    - Where Av=λv
    - Eigenvalues
        - How much a linear map scales space along a direction
    - Eigenvector
        - Vector space along which a linear map is invariant
- Determinant
    - Multilinear map of every column/row of a square matrix
    - The product of eigen values whose sign is determined by how it flips space
    - How much a linear map scales space
- Linear map
    - Uniquely identified by where it takes each basis vector
    - Unique transformation of basis vectors (and entire space generated by basis vectors)
    - Specification of coordinates in units of previous coordinate system
    - Every linear map can be represented by a matrix
    - Preserves the linear structure of a vector space
    - The image of the basis vectors can be seen as a new basis for the vector space (or new unit system for coordinates)
- Matrix
    - Represent linear maps by mapping each basis vector to a linear combination of basis vectors in the new space
    - One can view a matrix as a collection of vectors
        - Matrix multiplication is then the dot product of every combination of vector
        - Matrix multiplication is then a computation to see how much the collection of vectors are similar
    - One can view a matrix as a collection of rows respecifying basis vectors
        - Multiplication with a vector specifies each row to express the similarity between vector and each axis
- Decomposition
    - Eigen
- PCA
- SVD
- Eigendecomposition

- Multilinear algebra
    - Exterior algebra
        - Algebra which uses the exterior product or wedge product as its multiplication. The wedge product generalizes multipling n vectors to form n-dimensional shapes, in a way that preserves orientation.

Norm:
- Norm forms a measure from the components of a vector; it establishes a metric on the components
- $L^p_{norm} = ||x||_p = (\sum_i |x_i|^p)^{1/p}$
- Measures size of a vector
- Norm f satisfies some properties
    - $f(x) = 0 => x = 0$
    - $f(x+y) <= f(x)+f(y)$
    - $\forall \alpha \in \mathbb{R}, f(\alpha x)= |\alpha|f(x)$

$L^1$ is linear distance
$L^2$ is euclidean distance. $L^2(x) == 1 <=> x is a unit vector$
$L^\infty$ is max element

- $||A||_f = \sqrt{\sum A^2_{ij}}$
- Orthogonal matrix := $A^T=A^{-1}$

# Key concepts
- Vector space
    - A vector space over a field F is a set V together with two binary operations that satsify some axioms.
    - Vector space is a formulation for field of reference or context in which objects exist within the space. The existence of fields and linear combinations has some notion of size in space. Matrices are changes of field of reference. One can view a matrix of a collection of vectors, each of which specify where a basis vector lands.
- Vector
    - Represents a linear transformation to one dimension
    - Any vector is uniquely defined by positve scalar and basis vector
    - Normal vector
        - Dot product with itself is 1
        - Magnitude is 1
- Scalar
    - Measurements of space
    - Assigns magnitudes to each vector, which has a field structure.
    - Operations
        - Determinant
        - Cross product
            - anticommutative
            - distributive over addition
            - interacts well with scalar multiplication
            - not associative
            - produces pseudovector
        - Dot product
            - Algebraically
                - not associative
                - commutative
                - distributive
                - interacts well with scalar multiplication
            - Project basis of x onto basis of y. Then multiply the unique scalar of x and y.
            - Compares how similar two vectors are
- Basis
    - Decomposition of vector space
    - The number of basis vectors is the dimension of a vector space
    - Change of basis
        - A change of basis is a reinterpretation of coordinates in a new basis
        - If A is a change of basis, then applying A⁻¹MA is applying the meaning of M in a different basis
- Linear combination
    - Allows extracting vectors into basis vectors, and lists of magnitudes of each base
    - Linear combination
        - See what space can be filled with a collection of vectors
        - Forming a linear combination == scaling each by a corresponding field coefficient adding the results
        - Precisely what a vector does to basis vectors
    - Linear dependence := Vector is on the span of other columns
    - Span := Set of all points obtainably by linear combination of the original vectors
    - Linear independence := Only one way to linearly combine to reach 0
    - Column space
    - Singular := Square matrix with linear dependent columns

# Functional analogs
Vectors can be viewed as functions with a continuous label. There is then a continuous analog to squared norm, dot product, and matrix multiplication.

$$∫_{-∞}^{∞}dt' W(t,t')v(t')$$
$$∫_{-∞}^{∞}dt W(j,t)V(t,k)$$

Vector:
- One can view a function $f(t)$ as a continous analog of a vector. Sums over a vector are then integrals $∑fₜ$ → $∫dt\,f(t)$.
- When considering each index as a dimension in space, the continuous analog can be a continuous movement from one dimension to the next, through both dimensions.
- A continuously accessable collection of values which continuously change. A manifold.

Linearity:
$$a\,(v(t)+u(t)) = a\,v(t) + a\,u(t)$$

Dot product:
$$∫^∞_{-∞}dt\,v(t)u(t)$$

Matrix:
- A function $M(t,t')$ which continuously transforms functions from $v(t)$ to $v'(t')$. That is, it rescales functions input and output.
- A continuous matrix can be interpreted as a 2D manifold, where each point has a neighborhood that is homeomorphic to an open subset of a 2D euclidean space. In other words, points with similar indices (inputs) are near each other.

Matrix multiplication:
$$g(t) = ∫^∞_{-∞}dt'\,W(t,t')f(t')$$
- The identity matrix is then $δ(t-t')$.

Töplitz matrix:
- A function $K(t-t')$ that is offset by an index $t'$.

Linear filter:
$$∫^∞_{-∞}dt'\,K(t-t')v(t') = ∫^∞_{-∞}dτ\,K(τ)v(t-τ) \\ where:τ = t-t'$$
- Process time varying functions subject to linearity. In most cases, these filters functions are time invariant.
- Useful for processing time-varying signals.
- Linear time invariant (LTI) filters can be entirely characterized by how they operate on sinuisoids of different frequencies.
