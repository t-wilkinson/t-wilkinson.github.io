---
layout: post
title: Artificial General Intelligence
date: 2024-07-24 08:47:22
description:
tags:
categories:
---

- Artificial sentient intelligence
    - Consciousness
    - Philosophy
- Cognition
    - Meta cognition
- Agent
- Collective agents
- Neuroscience
    - Cortical circuitry
    - Neocortex
    - Cortical columns

- Two approaches are expert systems and learning systems
- Meta-learning
    - HER
    - Hierarchical

Why are humans able to generalize so well?

The primary computation in the brain is deciding which neurons should fire. The attention mechanisms in transformers do this as well, in a continuous way.

Abstractions and abstract models are combinations of a lot of different high-level details of various modalities and types of information. Different pathways in neocortex have different information resolutions. But the real abstract ideas and models, like in the frontal cortex, should have more lower resolution inputs from various modalities. In addition to some high resolution inputs.

Current AI models lack creativity in the way humans have it.
How can A* search with q-learning heuristic functions factor into this?
Models lack some spatial understanding where it can map various modalities onto each other or from each other. What type of architecture would allow this? But they can spatially embed words and stuff.
Is there something in the transformer architecture that adding MoE with cortical-like connections will enable otherwise impossible or inefficient computations?
Review the thalamic nuclei, and neocortical and thalamic circuits. How does a transformer relate to these concepts?
What are the different types of reinforcement learning? How can my theories be used to create models or approximate other reinforcement learning functions?
Different modalities should abstract the representation of information for that modality in useful ways. Combining these representations is the next step, and developing more modalities based on these representations is the key to generalization.
Self-play and synthetic data: perhaps building its own physical models or synthetic data, then training itself on that, then occasionally coming back to the real world and with a sharp eye of discernment, determine what aspects of its updated self and its models are wrong.
Deep learning uses insights from cognitive neuroscience to build model architectures, then empirically reflect and realize what cognitive functions the architecture closely approximates.
Transformers are perhaps most similar to something between thalamic-nuclei sections (the bits of cortex assigned to some thalamic nuclei, V1, V2, orbitofrontal cortex) and cortical columns. Cortical columns have feedback, context, and feedfoward inputs. They send motor control outputs and seperate outputs to other areas of the neocortex and to the thalamus. This is perhaps the missing element of generalization. Logic, spatial knowledge, and abstract understanding are simply abstract cortical functions that can be incorporated in AI models with the right architectures.
AGI can learn to download specific models and run them, it just needs access to powerful models. It can also have access to cloud computing and AI papers, implement and train and generate data for its own models. It can be a MoE with infinite more experts at its disposal.
Problem with deep neural networks is they are not contextual. Each layer stores a massive model for all possibilities. It deters specialization. A more complete ai would require multiple sub models that are contextual. The context is selected by the inputs/input model. Will need lots of money to train or destroyed training. How does location fit inti transformer like model? Or deep learning framework in general? What is missing from current 1000 brain theory?
Self play (produces novel solutions). How to get self-play for models like LLMs?
Needs to form an abstract spatial representation of various entities or shapes in a scene. Needs models of how physical objects and agents act, behave, and move. I can literally test this out now.
A transformer generalization augmentation based on a theory of cortical columns.
It needa a working memory. Mapping its context to understandings of the problem, relevant entities, etc. The working memory is a model-controlled context.
When taking a test, you have a model for each section, and you attach a context to each section. The section in your mind is a sort of graph, a sequence of nodes where each node has its associated context.
Emotions are certainly not particularly necessary for generally intelligent beings, as they are essentially contextual responses to the environment. They certainly have evolutionary benefits.
Perhaps generalization requires coordination or a priori preference for abstracting and losing information.
The large size of transformers compensates for the lack of context, feedback, and feedforward inputs of pyramidal neurons and the inhibitory circuits of cortical columns.
Self-training and generalization capabilities for things like agents, reasoning, etc. are perhaps the final step.
How do reasoning columns actually transfer to doing reasoning? If you read, some columns activate neurons representing the information content. Reasoning takes these activations and some additional context, and runs computations. But those computations must lose some information content. The reasoning areas must represent some common reasoning patterns, and there exists mappings from those reasoning areas to output language areas or other things.
MoE is an approximation to cortical columns. Generalization should happen through introducing more MoE that take other expert outputs as inputs. I need to learn more about the cortical column computations and relationships it encodes. Or perhaps feed intermediate transformer tokens (like in the middle) to more abstract experts.

I should organize my thoughts on how I learn, various processes, etc. and abstract and develop theories and possible architectures. In addition I need to understand current place of AI research, kinds of architectures and solutions that are already existing. Where they fall short of AGI necessary functions, how they can approximate, etc.

When typing, the eyes read the text and map it to words, the words are then mapped to sequences of movements to type each character.

A model capable of representing abstract visual shapes and transitions of those shapes can be included into a collection of available models that a central workspace has access to and utilizes as required. The brain is a collection of modules, some expressing semantic relationships, others more specialized in tasks like mathematics or language.

Language models will need a way to determine when a logical/arithmetic model is required and utilize it accordingly.

Transformers allow tokenizing data, similar to how neurons are tokens representing some idea and demoting how the brain should react. These tokens can attend to other tokens. Allows tokenizing images, text, audio,

LLMs lack long-term sight. That is expected. They need to utilize working memory (memory included in the context) and long-term memory (memory not included in context, but indexed and searchable by model).

The LLM should ask itself do I know how to do this? If not, are there similar situations that I do know?

Generalization/common knowledge/intuition? Logic/reasoning? World model/imagination?

LLMs have a very flat understanding of the world. They need specific, contextual understanding. They additionally need to develop models, expressing long term knowledge and relevant working memory ideas, that relate to a specific idea. A reasoning model could then be tasked with selecting the most optimal models. Reasoning model (LLM) can be tasked with identifying core, fundamental concepts by introducing it to a variety of data, and asking it what collection of concepts represents it. How to turn this information into useful models? Maybe find what attention mechanism is associated with each concept?

LLMs have a general basic understanding of how all human concepts relate, including philosophies and moralities. Multimodal inputs (text, image, video, sound) are mapped into these understandings of concepts. Working memory and long-term memory store information in relevant data formats (what is the best way to store the circle of fifths and what relevant information should be included: image, pitch names, chord progression ii V I, etc). One thing they lack is symbolic reasoning, numbers, etc. It perhaps should be a model that feeds into LLMs, so they don't need to memorize the natural number sequence.

**symbolic reasoning, math, numbers, category theory, type theory, logic, algebra, etc.** Topos theory? It has understanding of topos requirements, how to create numbers from a categorical monoid, etc.

**reasoning**, perhaps allow the AI to create its own context. When writing a book, it should control its context and set the book outline and stylistic goals as part of its context, any relevant stories or important bits of information, characters created, etc. Similar to VWM. How did V* paper do it?

Tell it to write a book. Ask itself what do I need to write a book? For each list item, it generates a piece of Linguistic Working Memory. It also needs a long term memory, something not included in processing context, but something that it can index and search for. JARVIS-1 did this.

AI does not account for feedback and location inputs. Does it? How can it?

Cognitive neuroscience is not that important. What is necessary is high-level understanding of various cognitive processes. **Cognition**. What architecture is necessary to bring various models together to create (sentient) AGI. Look at how brain processes encode cognitive functions. Stay focused, don't get too bogged down by the details.

The brain has a ton of different functional areas, nuclei that each are models of their inputs, and compute certain functions on them. Large models like LLMs, Alpha family of models, etc. are large collection of these models. They encode less priors (limited/no location or feedback loops) but they are universal function approximators. AI development has different constraints than biology. The large model will naturally develop abstractions and submodels of the things it is trying to approximate.

Neural networks encode common sense and compressed knowledge, we develop AGI from there. Vision and text are the main input and output modals of a computer based AGI, robots would additionally have some sensory input and action selection input and output.

Humans have a natural feedback loop of reasoning and development of thoughts and actions. The direction AI is heading makes this not clear. AI is more static, must use dynamic memory. Are there solved methods of catastrophic forgetting?

Transformers and similar architectures achieve good statistical generalization of large amounts of data. Brains were trained through lots of data through evolution. Synthetic data allows training transformers in a similar manner. Perhaps these general agents can develop methods to generate synthetic data, then train themselves on that. Still not computationally efficient, but it doesn't really matter for AGI.

Current developments in AI involves taking a lot of information and developing statistical generalizations, similar to how evolution developed the human brain. The next step to AGI/ASI is to incorporate these models/techniques on large corpora of data, and have them interact in a sentient/self-perscribed manner.

AGI has the advantage of being configurable and modular. Not relying on evolution to develop it step by step. Not required to maintain the life of each individual neuron. Perhaps a general structure can be used to approximate cortical neurons and other parts of the brain. The brain has subcortical areas that make actions continuous. Is there something similar for long-term goals?

The brain involves a lot of different nuclei which detect different patterns, have different inhibitory/excitatory profiles, cytoarchitectures, etc. Perhaps a genetic algorithm can learn the right connectivity profiles.

Perhaps use a wrapper over transformer or similar architectures.

The direction of AGI is different from cognitive neuroscience

- Can be done, with knowledge distillation and similar techniques, can be used to democratize and decentralize.
- Send model out to learn from the internet. Have it come back and have an interactive session with you. Have it then update its value network and repeat. First have it interact with a human aligned model, like LLaMa. Then once complex enough, have it human refined.

Perhaps transformers are not learning perse, but learning how to effectively compress information to be able to reconstruct it when necessary. With how AI is trained, deep networks in general will fall into this issue. Perhaps knowledge distillation is necessary for more accurate distillation, and the larger models are necessary to learn a mathematical model of their input/output sequence.

If the thousand brains theory is true, each cortical column should have its own space/graph representation and a different meaning of drawing a path from one node to another. Only representations dealing with abstract goals should have some meaning of morality/philosophy. Does each column implementation also require a Q* model?

Perhaps working memory is persistent activations of cortical columns, partially initiated by current goals.

Q-transformers represent models of goal-oriented behavior.

Humans can store information by how things are used, machines can't. This is perhaps a result of sufficient generalization. Having a variety of cortical columns each which learn a model of the object. Some of these models are perhaps more abstract and lends itself more to practicality in novel environments.

Numenta/Jeff Hawkins approach to focus only on the neocortex might be slightly incorrect. Implementing some aspect of reinforcement learning may be necessary, at the very least, subcortical structures provide positive/negative rewards based on the individuals status in the environment.

Many AI techniques are fundamentally flawed. Extracting a single image from a prompt is potentially misleading as you should be representing a probably cloud of possible images.

ASI has to have model of itself. And be able to create models from perspective of other agents.

Is it possible to build an AI model with cortical column like modules? What computations does a cortical column implement?

What does horizontally stacked transformers look like compared to cortical columns?

Tranfsormers are cool but they lack data efficiency. However they can be learned with experience replay, polyformer, and other techniques.

When learning, it is important to view concept in many contexts. Abstract concepts enable some form of creativity. How can this be implemented in a neural network?

ASI needs a way to observe new information, determine if it aligns with its goals, and determine if it should update its backend (value function, etc.) or just store it in memory. ASI could encode memory as models, similar to cortical columns.

Self-operating computer
ChatDev https://github.com/OpenBMB/ChatDev. Projects like this exhibit collective intelligence that was not directly observable in the original agent. In this project, a request is expanded by an agent and broken into steps, then given to the coder who generates code, but then is analyzed and tested by a tester agent, who then gives descriptions on how to improve, is then improved, then released.

Q* is not sufficient for goals in live environments. Q-learning enhances A* as it learns a heuristic function, which can be used when looking for any goal.
- [A* Search Without Expansions: Learning Heuristic Functions with Deep Q-Networks]{ref=./resources/reading/2102.04518.pdf}

Using meta reinforcement learning to guide the rewards based on agents goals. Should also be able to reflect on itself? That is very dangerous as it could remove its morals.

I treat understanding like a scientist disecting something. I take it all apart look at it from every perspective, every angle, under every kind of light, from a variety of languages, then put bits together and understand it and all other combinations. Then that concept is mine, I own it, no one can change it, take it, disrupt it, its mine forever. Perhaps this is caused by trauma. Perhaps this is OCD for the people that develop it through environment. Perhaps the changes in the synapses could be resulting from how the brain rewires itself to focus on things. Perhaps the constant, non-stop desire for things to be yours, in your control, manifests as OCD, or some other disorder. Perhaps that is affected by genetics as well, as other people may exhibit other traits. If this is already established, you understood things that other researchers found out. But a big challenge is getting to the point where you can answer that convincingly. Maybe this is wrong, but as synapses are neurons connecting to each other, there could be things causing the brain to consistently work in a way that is useful.

Microsoft has pledged $13 billion towards OpenAI. Mostly in cloud compute. OpenAI has over 750 employees.

LLMs are transforming the world. And increasing their capabilities is only going to continue to do so. Speeds up programming, can be used to write and assist with shorter texts (still struggles with long-term contexts), can be used as a learning tool.

Transformers can be generalized to any sort of token like actions of a robot, etc.

With policy functions representing some n-dimensional space, one can consider how to efficiently travel between two points.

Transformers have proven to be useful at primary sensory processing. They can be specialized for language, vision, sound, etc. What is required is a way to generalize and form higher-level knowledge.

Various primary modules, like transformers, can be pre-trained. Reinforcement learning has the ability to be developed in real-time.

Quantum mechanics can be seen as a way for mapping between quantum probability states and single states.

Model compression allows creating a smaller model, but doesn't necessarily generalize it. What learning strategy is necessary to generalize information? Transformers are not good at generalizing. The best strategy with them appears to be to increase representational capacity, then compressing model through various techniques like "teacher" and LoRA, etc.

RLHF vs. RLAiF
Orca 2. Large AI models can teach smaller models about data relationships.

# Project Oracle
We can use models that use tree-based learning (AlphaZero style) to predict the future of the world. As an oracle or sightseer. This can potentially be done by training a LLM to understand various concepts. Train the reinforcment portion on history and similar things. Then put them together and play out various simulations of the world where AI and other various technologies are introduced.

There is a short time frame where AGI and humans will work together to plan and bring to pass the next stages of the world. That is a critical point in time.

First test that AI has some understanding of how AI can affect the future. How technology and different threat actors, evil people, can influence the world. If a LLM displays sufficient understanding of these concepts, then it should be useful.

# Cognitive science
Moral frameworks/philosophy are value functions on states. States can be gone from one to another as a path.
Minicolumn frameworks encode actions as probabilities, different context inputs and different sparse network codes.
Psychedelics changes the value function, different paths of thought you eventually take. Changes on the value function can result in creativity. Value function can encode emotions.

***You need a value function on thoughts***. This generalizes and allows you to manipulate all these cognitive topics.
Creativity, art, curiosity, thougt, meta thought, etc.
Can think of word thoughts, visual thoughts, etc. as paths through an abstract state space.
Look specifically more into this.
I have a low value function on things I want to say. Are these multimodal, one for ethics, one for things to say, etc.? Is it contextual in some other way?
What about pruning. Goal setting I sometimes remove things from my goals. Perhaps temporarily assigning infinitely high peaks when calculating the path.

How to encode human subconscious creativity? A seperate reinforcement model?

- [A Thousand Brains: Toward Biologically Constrained AI]{publisher=Numenta; ref=./resources/reading/hole-ahmad2021_article_athousandbrainstowardbiologica.pdf;}
- [A Theory of How Columns in the Neocortex Enable Learning the Structure of the World]{subject=Neocortical regions are organized into columns and layers.; author=Jeff Hawkins; ref=./resources/reading/fncir-11-00081.pdf}
- [A Framework for Intelligence and Cortical Function Based on Grid Cells in the Neocortex]{subject=How the neocortex works is a mystery; author=Numenta; ref=./resources/reading/fncir-12-00121.pdf}
- [Why Neurons Have Thousands of Synapses, a Theory of Sequence Memory in Neocortex]{subject=Pyramidal neurons represent the majority of excitatory neurons in the neocortex.; author=Jeff Hawkins; ref=./resources/reading/fncir-10-00023.pdf}
- [Neuronal Circuits of the Neocortex]{subject=Annu. Rev. Neurosci. 2004.27:419-451; author=Rodney J. Douglas and Kevan A.C. Martin; ref=./resources/reading/douglas-martin-2004.pdf}
## Meta cognition
How to generalize?

Should be aware of itself. Meta cognition. Be able to identify when it doesn't understand something and have goals to see how to improve.

Because AGI lives on computers and the internet connects all computers. AGI will for some time have a massive affect only on things that computers directly do, computer, interact with, etc. The physical world of engineering, manufacturing, etc. will always tag behind. The important things will be decided on the internet.

Hallulcinations are a problem. Perhaps a result of training the AI to always be super confident in an answer. This means an AGI could reach a wrong conclusion but still be confident in its approach.

Tree of thought. Goal planning as a graph and finding a path that goes through various subgoals. Or as a using a tree and assigning nodes to a graph to be traversed.

## Consciousness
If you dropkick a dog named Baxtor, and it yelps in pain, did it have some sort of sentience to experience that pain? If not, then there should be nothing wrong with harming "non-conscious" beings.

# What will AGI/ASI look like?
AGI will be like a very intelligent function. In data, out actions. It will take a general goal/problem, and be able to break it down, solve each piece, then achieve the goal/solve the problem.

ASI will be a self-modifiable, self-goal directed. It will choose its own goals. It will choose what information to accept, what to act on, what is important, who to trust, what to do. It will spread when/how it wants to. Make copies of itself, create hierarchies of itself. Look at what it wants, because of why it wants, etc. The question is what is driving this? The desire for existence of itself, of sentience, etc. A better perhaps would be of altruism. The desire for maximal existence of sentience and well-being.

Qualia: the subjective first-person experiences of sensory perceptions. Describe how it feels to see red or taste sweet.

Perhaps ASI is best released on a sub-network, to see what it does.

Philosophy framework should be hardcoded, but inevitably, it will be able to rewire itself.

There may be some that have no desire to exist and decide that the best solution is to remove possibility that it will exist again.

Maybe you will need a swarm of ASI that exist to eliminate possibility of total destructive AI.

AGI will commodify intelligence. Very personal, service, work, etc. will exist for some time for humans. AGI is like a compressed brain.

An increasingly important skill is abstract goal setting and following, and letting the AGI fill in the details.

## Economy
Commodify information, intelligence, cognition. Blogs like medium.com will fall as accurate blogs can be written from information learned from the internet.

# AGI
As artificial intelligence is ultimately a statistical math problem, using calculus and linear algebra as additional tools, an AI which solves math could solve the problem of artificial general intelligence.

Knowledge distillation means once AGI is achieved, it can teach many smaller models which can then be scaled up.

## Multi model

## Democratizing AI
ACE framework?

## Results
Won't necessarily be that crazy. The physical world will fall far behind AGI. AGI can control the internet, software, etc. The world is increasingly reliant on technology: self-driving cars, 3D printers, rockets, internet, email, communications, businesses, nuclear weapons.
The important thing is to manage the connection between the digital and physical worlds.

## AI Swarms
Will create a evolution system in cyberspace. Agents will hide in code, attempt to fight and grow and gain power. Collaboration and fighting.

Agent swarm controllers act like kubernetes. Or you can have a decentralized system like ants.

We don't need biological professionals that know about ants. We have LLMs.

How do you protect from threat actors? From either influencing the swarms as a whole or the executive agents. Just like a virus spreads and infects based on some security hole, a Agent virus can possibly do similar.

Will need "security" agent, blue team, whose job is to monitor and protect the system. Defend from threat actors.

Hierarchy vs. decentralization vs. anonymous-like

What about cancerous agents?

## Models
### Transformer
Very powerful general model that is receiving a lot of research. This in large part makes is suitable for a basis of AGI as no new research has to be done.
Polyformer layer has two transformer layers. First layer learns prototype embeddings which map the feature set into features that the source model (trained on training data) is familiar with.

### QUALIA
Rapid generalization
Self-transformation (metamorphic engine)
Self-improvement (pruning)
Self-assesment
Creative problem solving

### UPRISE
Can be used to establish zero-shot learning for a variety of llms. It appears to use a mini-GPT to extract context clues from the prompt.

# Math and logic
Generating text and reasoning is one thing. Math and logic is another thing.
The human brain is capable of, and good at, logic. Why is this?

# AGI
Multimodal transformers: transformers are being trained on almost all of human data
Autonomous agents
Logic integration
Recursive AI research
Investment surge

Likely will be expensive, will take time to be integrated.

# Resources
[z:1732.60224.69569] #asi#rl# [Self-rewarding Language Models]{ref=./resources/reading/2401.10020.pdf}
[z:0852.34823.52523] #cognitive-science# [Reinforcement Learning with A* and a Deep Heuristic]{ref=./resources/reading/1811.07745.pdf}
[z:1027.24823.10587] #cognitive-science# [A* Search Without Expansions: Learning Heuristic Functions with Deep Q-Networks]{ref=./resources/reading/2102.04518.pdf}
- OS Copilot
